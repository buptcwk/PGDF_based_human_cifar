| Building net
epoch:0
epoch:1
epoch:2
epoch:3
epoch:4
epoch:5
epoch:6
epoch:7
epoch:8
epoch:9
epoch:10
epoch:11
epoch:12
epoch:13
epoch:14
epoch:15
epoch:16
epoch:17
epoch:18
epoch:19
epoch:20
epoch:21
epoch:22
epoch:23
epoch:24
epoch:25
epoch:26
epoch:27
epoch:28
epoch:29
epoch:30
epoch:31
epoch:32
epoch:33
epoch:34
epoch:35
epoch:36
epoch:37
epoch:38
epoch:39
epoch:40
epoch:41
epoch:42
epoch:43
epoch:44
epoch:45
epoch:46
epoch:47
epoch:48
epoch:49
epoch:50
epoch:51
epoch:52
epoch:53
epoch:54
epoch:55
epoch:56
epoch:57
epoch:58
epoch:59
epoch:60
epoch:61
epoch:62
epoch:63
epoch:64
epoch:65
epoch:66
epoch:67
epoch:68
epoch:69
epoch:70
epoch:71
epoch:72
epoch:73
epoch:74
epoch:75
epoch:76
epoch:77
epoch:78
epoch:79
epoch:80
epoch:81
epoch:82
epoch:83
epoch:84
epoch:85
epoch:86
epoch:87
epoch:88
epoch:89
epoch:90
epoch:91
epoch:92
epoch:93
epoch:94
epoch:95
epoch:96
epoch:97
epoch:98
epoch:99
epoch:100
epoch:101
epoch:102
epoch:103
epoch:104
epoch:105
epoch:106
epoch:107
epoch:108
epoch:109
epoch:110
epoch:111
epoch:112
epoch:113
epoch:114
epoch:115
epoch:116
epoch:117
epoch:118
epoch:119
epoch:120
epoch:121
epoch:122
epoch:123
epoch:124
epoch:125
epoch:126
epoch:127
epoch:128
epoch:129
epoch:130
epoch:131
epoch:132
epoch:133
epoch:134
epoch:135
epoch:136
epoch:137
epoch:138
epoch:139
epoch:140
epoch:141
epoch:142
epoch:143
epoch:144
epoch:145
epoch:146
epoch:147
epoch:148
epoch:149
epoch:150
epoch:151
epoch:152
epoch:153
epoch:154
epoch:155
epoch:156
epoch:157
epoch:158
epoch:159
epoch:160
epoch:161
epoch:162
epoch:163
epoch:164
epoch:165
epoch:166
epoch:167
epoch:168
epoch:169
epoch:170
epoch:171
epoch:172
epoch:173
epoch:174
epoch:175
epoch:176
epoch:177
epoch:178
epoch:179
epoch:180
epoch:181
epoch:182
epoch:183
epoch:184
epoch:185
epoch:186
epoch:187
epoch:188
epoch:189
epoch:190
epoch:191
epoch:192
epoch:193
epoch:194
epoch:195
epoch:196
epoch:197
epoch:198
epoch:199
epoch:200
epoch:201
epoch:202
epoch:203
epoch:204
epoch:205
epoch:206
epoch:207
epoch:208
epoch:209
epoch:210
epoch:211
epoch:212
epoch:213
epoch:214
epoch:215
epoch:216
epoch:217
epoch:218
epoch:219
epoch:220
epoch:221
epoch:222
epoch:223
epoch:224
epoch:225
epoch:226
epoch:227
epoch:228
epoch:229
epoch:230
epoch:231
epoch:232
epoch:233
epoch:234
epoch:235
epoch:236
epoch:237
epoch:238
epoch:239
epoch:240
epoch:241
epoch:242
epoch:243
epoch:244
epoch:245
epoch:246
epoch:247
epoch:248
epoch:249
epoch:250
epoch:251
epoch:252
epoch:253
epoch:254
epoch:255
epoch:256
epoch:257
epoch:258
epoch:259
epoch:260
epoch:261
epoch:262
epoch:263
epoch:264
epoch:265
epoch:266
epoch:267
epoch:268
epoch:269
epoch:270
epoch:271
epoch:272
epoch:273
epoch:274
epoch:275
epoch:276
epoch:277
epoch:278
epoch:279
epoch:280
epoch:281
epoch:282
epoch:283
epoch:284
epoch:285
epoch:286
epoch:287
epoch:288
epoch:289
epoch:290
epoch:291
epoch:292
epoch:293
epoch:294
epoch:295
epoch:296
epoch:297
epoch:298
epoch:299
| Building net
0.2
4001
saving noisy labels to ./checkpoints/c10/20sym/worst/saved/easy_labels.p...
epoch:0
epoch:1
epoch:2
epoch:3
epoch:4
epoch:5
epoch:6
epoch:7
epoch:8
epoch:9
epoch:10
epoch:11
epoch:12
epoch:13
epoch:14
epoch:15
epoch:16
epoch:17
epoch:18
epoch:19
epoch:20
epoch:21
epoch:22
epoch:23
epoch:24
epoch:25
epoch:26
epoch:27
epoch:28
epoch:29
epoch:30
epoch:31
epoch:32
epoch:33
epoch:34
epoch:35
epoch:36
epoch:37
epoch:38
epoch:39
epoch:40
epoch:41
epoch:42
epoch:43
epoch:44
epoch:45
epoch:46
epoch:47
epoch:48
epoch:49
epoch:50
epoch:51
epoch:52
epoch:53
epoch:54
epoch:55
epoch:56
epoch:57
epoch:58
epoch:59
epoch:60
epoch:61
epoch:62
epoch:63
epoch:64
epoch:65
epoch:66
epoch:67
epoch:68
epoch:69
epoch:70
epoch:71
epoch:72
epoch:73
epoch:74
epoch:75
epoch:76
epoch:77
epoch:78
epoch:79
epoch:80
epoch:81
epoch:82
epoch:83
epoch:84
epoch:85
epoch:86
epoch:87
epoch:88
epoch:89
epoch:90
epoch:91
epoch:92
epoch:93
epoch:94
epoch:95
epoch:96
epoch:97
epoch:98
epoch:99
epoch:100
epoch:101
epoch:102
epoch:103
epoch:104
epoch:105
epoch:106
epoch:107
epoch:108
epoch:109
epoch:110
epoch:111
epoch:112
epoch:113
epoch:114
epoch:115
epoch:116
epoch:117
epoch:118
epoch:119
epoch:120
epoch:121
epoch:122
epoch:123
epoch:124
epoch:125
epoch:126
epoch:127
epoch:128
epoch:129
epoch:130
epoch:131
epoch:132
epoch:133
epoch:134
epoch:135
epoch:136
epoch:137
epoch:138
epoch:139
epoch:140
epoch:141
epoch:142
epoch:143
epoch:144
epoch:145
epoch:146
epoch:147
epoch:148
epoch:149
epoch:150
epoch:151
epoch:152
epoch:153
epoch:154
epoch:155
epoch:156
epoch:157
epoch:158
epoch:159
epoch:160
epoch:161
epoch:162
epoch:163
epoch:164
epoch:165
epoch:166
epoch:167
epoch:168
epoch:169
epoch:170
epoch:171
epoch:172
epoch:173
epoch:174
epoch:175
epoch:176
epoch:177
epoch:178
epoch:179
epoch:180
epoch:181
epoch:182
epoch:183
epoch:184
epoch:185
epoch:186
epoch:187
epoch:188
epoch:189
epoch:190
epoch:191
epoch:192
epoch:193
epoch:194
epoch:195
epoch:196
epoch:197
epoch:198
epoch:199
epoch:200
epoch:201
epoch:202
epoch:203
epoch:204
epoch:205
epoch:206
epoch:207
epoch:208
epoch:209
epoch:210
epoch:211
epoch:212
epoch:213
epoch:214
epoch:215
epoch:216
epoch:217
epoch:218
epoch:219
epoch:220
epoch:221
epoch:222
epoch:223
epoch:224
epoch:225
epoch:226
epoch:227
epoch:228
epoch:229
epoch:230
epoch:231
epoch:232
epoch:233
epoch:234
epoch:235
epoch:236
epoch:237
epoch:238
epoch:239
epoch:240
epoch:241
epoch:242
epoch:243
epoch:244
epoch:245
epoch:246
epoch:247
epoch:248
epoch:249
epoch:250
epoch:251
epoch:252
epoch:253
epoch:254
epoch:255
epoch:256
epoch:257
epoch:258
epoch:259
epoch:260
epoch:261
epoch:262
epoch:263
epoch:264
epoch:265
epoch:266
epoch:267
epoch:268
epoch:269
epoch:270
epoch:271
epoch:272
epoch:273
epoch:274
epoch:275
epoch:276
epoch:277
epoch:278
epoch:279
epoch:280
epoch:281
epoch:282
epoch:283
epoch:284
epoch:285
epoch:286
epoch:287
epoch:288
epoch:289
epoch:290
epoch:291
epoch:292
epoch:293
epoch:294
epoch:295
epoch:296
epoch:297
epoch:298
epoch:299
training 1D-CNN classifier for class0
Training: Epoch[001/050]  Loss: 3.0902 Acc:62.85%
[[   0. 1518.]
 [   0. 1137.]]
0.42824858757062145
Training: Epoch[002/050]  Loss: 2.6963 Acc:85.92%
Training: Epoch[003/050]  Loss: 2.4645 Acc:85.92%
[[   0. 1518.]
 [   0. 1137.]]
0.42824858757062145
Training: Epoch[004/050]  Loss: 2.3098 Acc:85.92%
Training: Epoch[005/050]  Loss: 2.1776 Acc:85.92%
[[   0. 1518.]
 [   0. 1137.]]
0.42824858757062145
Training: Epoch[006/050]  Loss: 2.0271 Acc:85.92%
Training: Epoch[007/050]  Loss: 1.8484 Acc:85.92%
[[   0. 1518.]
 [   0. 1137.]]
0.42824858757062145
Training: Epoch[008/050]  Loss: 1.6214 Acc:85.92%
Training: Epoch[009/050]  Loss: 1.3694 Acc:85.92%
[[ 403. 1115.]
 [   0. 1137.]]
0.5800376647834274
Training: Epoch[010/050]  Loss: 1.1031 Acc:85.92%
Training: Epoch[011/050]  Loss: 0.8304 Acc:91.78%
[[1281.  237.]
 [  60. 1077.]]
0.888135593220339
Training: Epoch[012/050]  Loss: 0.5949 Acc:95.84%
Training: Epoch[013/050]  Loss: 0.4243 Acc:97.92%
[[1378.  140.]
 [ 134. 1003.]]
0.8967984934086629
Training: Epoch[014/050]  Loss: 0.3098 Acc:98.49%
Training: Epoch[015/050]  Loss: 0.2394 Acc:98.68%
[[1412.  106.]
 [ 188.  949.]]
0.8892655367231639
Training: Epoch[016/050]  Loss: 0.1968 Acc:98.87%
Training: Epoch[017/050]  Loss: 0.1702 Acc:99.05%
[[1427.   91.]
 [ 220.  917.]]
0.8828625235404897
Training: Epoch[018/050]  Loss: 0.1527 Acc:98.87%
Training: Epoch[019/050]  Loss: 0.1407 Acc:98.96%
[[1440.   78.]
 [ 241.  896.]]
0.87984934086629
Training: Epoch[020/050]  Loss: 0.1322 Acc:98.96%
Training: Epoch[021/050]  Loss: 0.1258 Acc:98.96%
[[1445.   73.]
 [ 254.  883.]]
0.8768361581920904
Training: Epoch[022/050]  Loss: 0.1209 Acc:98.96%
Training: Epoch[023/050]  Loss: 0.1170 Acc:98.87%
[[1450.   68.]
 [ 263.  874.]]
0.8753295668549906
Training: Epoch[024/050]  Loss: 0.1138 Acc:98.87%
Training: Epoch[025/050]  Loss: 0.1113 Acc:98.87%
[[1457.   61.]
 [ 280.  857.]]
0.8715630885122411
Training: Epoch[026/050]  Loss: 0.1142 Acc:98.87%
Training: Epoch[027/050]  Loss: 0.1147 Acc:98.87%
[[1454.   64.]
 [ 270.  867.]]
0.8741996233521657
Training: Epoch[028/050]  Loss: 0.1123 Acc:98.87%
Training: Epoch[029/050]  Loss: 0.1146 Acc:98.87%
[[1448.   70.]
 [ 262.  875.]]
0.8749529190207156
Training: Epoch[030/050]  Loss: 0.1090 Acc:98.87%
Training: Epoch[031/050]  Loss: 0.1156 Acc:98.87%
[[1444.   74.]
 [ 251.  886.]]
0.8775894538606404
Training: Epoch[032/050]  Loss: 0.1045 Acc:98.87%
Training: Epoch[033/050]  Loss: 0.1093 Acc:98.87%
[[1451.   67.]
 [ 266.  871.]]
0.8745762711864407
Training: Epoch[034/050]  Loss: 0.1159 Acc:98.87%
Training: Epoch[035/050]  Loss: 0.1022 Acc:98.87%
[[1427.   91.]
 [ 229.  908.]]
0.879472693032015
Training: Epoch[036/050]  Loss: 0.1130 Acc:98.77%
Training: Epoch[037/050]  Loss: 0.1838 Acc:98.49%
[[1384.  134.]
 [ 150.  987.]]
0.8930320150659133
Training: Epoch[038/050]  Loss: 0.1806 Acc:98.58%
Training: Epoch[039/050]  Loss: 0.1176 Acc:98.96%
[[1457.   61.]
 [ 278.  859.]]
0.8723163841807909
Training: Epoch[040/050]  Loss: 0.1093 Acc:98.96%
Training: Epoch[041/050]  Loss: 0.1002 Acc:98.87%
[[1458.   60.]
 [ 275.  862.]]
0.8738229755178908
Training: Epoch[042/050]  Loss: 0.1102 Acc:98.96%
Training: Epoch[043/050]  Loss: 0.0997 Acc:98.87%
[[1458.   60.]
 [ 274.  863.]]
0.8741996233521657
Training: Epoch[044/050]  Loss: 0.1100 Acc:98.96%
Training: Epoch[045/050]  Loss: 0.0990 Acc:98.87%
[[1453.   65.]
 [ 271.  866.]]
0.8734463276836159
Training: Epoch[046/050]  Loss: 0.1081 Acc:98.96%
Training: Epoch[047/050]  Loss: 0.0975 Acc:98.87%
[[1451.   67.]
 [ 270.  867.]]
0.8730696798493409
Training: Epoch[048/050]  Loss: 0.1079 Acc:98.96%
Training: Epoch[049/050]  Loss: 0.0966 Acc:98.87%
[[1450.   68.]
 [ 266.  871.]]
0.8741996233521657
Training: Epoch[050/050]  Loss: 0.1063 Acc:98.96%
Training: Epoch[001/050]  Loss: 4.3279 Acc:65.69%
[[   0. 1518.]
 [   0. 1137.]]
0.42824858757062145
Training: Epoch[002/050]  Loss: 2.7130 Acc:85.92%
Training: Epoch[003/050]  Loss: 2.3885 Acc:85.92%
[[   0. 1518.]
 [   0. 1137.]]
0.42824858757062145
Training: Epoch[004/050]  Loss: 2.2116 Acc:85.92%
Training: Epoch[005/050]  Loss: 2.3315 Acc:85.92%
[[   0. 1518.]
 [   0. 1137.]]
0.42824858757062145
Training: Epoch[006/050]  Loss: 1.9531 Acc:85.92%
Training: Epoch[007/050]  Loss: 1.7607 Acc:85.92%
[[   0. 1518.]
 [   0. 1137.]]
0.42824858757062145
Training: Epoch[008/050]  Loss: 1.5836 Acc:85.92%
Training: Epoch[009/050]  Loss: 1.3928 Acc:85.92%
[[ 590.  928.]
 [   0. 1137.]]
0.6504708097928437
Training: Epoch[010/050]  Loss: 1.1501 Acc:85.82%
Training: Epoch[011/050]  Loss: 1.0265 Acc:87.81%
[[1232.  286.]
 [  42. 1095.]]
0.8764595103578154
Training: Epoch[012/050]  Loss: 0.7304 Acc:94.05%
Training: Epoch[013/050]  Loss: 0.4621 Acc:96.69%
[[1379.  139.]
 [ 143.  994.]]
0.8937853107344633
Training: Epoch[014/050]  Loss: 0.3179 Acc:98.20%
Training: Epoch[015/050]  Loss: 0.2401 Acc:98.58%
[[1408.  110.]
 [ 200.  937.]]
0.8832391713747646
Training: Epoch[016/050]  Loss: 0.1947 Acc:98.58%
Training: Epoch[017/050]  Loss: 0.1680 Acc:98.77%
[[1435.   83.]
 [ 238.  899.]]
0.8790960451977401
Training: Epoch[018/050]  Loss: 0.1524 Acc:98.96%
Training: Epoch[019/050]  Loss: 0.1449 Acc:98.87%
[[1438.   80.]
 [ 240.  897.]]
0.879472693032015
Training: Epoch[020/050]  Loss: 0.1467 Acc:98.77%
Training: Epoch[021/050]  Loss: 0.1695 Acc:98.77%
[[1400.  118.]
 [ 181.  956.]]
0.8873822975517891
Training: Epoch[022/050]  Loss: 0.2298 Acc:98.39%
Training: Epoch[023/050]  Loss: 0.2403 Acc:98.39%
[[1427.   91.]
 [ 221.  916.]]
0.8824858757062147
Training: Epoch[024/050]  Loss: 0.1417 Acc:98.77%
Training: Epoch[025/050]  Loss: 0.1103 Acc:98.87%
[[1467.   51.]
 [ 321.  816.]]
0.8598870056497175
Training: Epoch[026/050]  Loss: 0.1090 Acc:98.87%
Training: Epoch[027/050]  Loss: 0.1089 Acc:98.87%
[[1467.   51.]
 [ 319.  818.]]
0.8606403013182674
Training: Epoch[028/050]  Loss: 0.1079 Acc:98.87%
Training: Epoch[029/050]  Loss: 0.1083 Acc:98.87%
[[1466.   52.]
 [ 321.  816.]]
0.8595103578154426
Training: Epoch[030/050]  Loss: 0.1075 Acc:98.87%
Training: Epoch[031/050]  Loss: 0.1077 Acc:98.87%
[[1462.   56.]
 [ 314.  823.]]
0.8606403013182674
Training: Epoch[032/050]  Loss: 0.1072 Acc:98.87%
Training: Epoch[033/050]  Loss: 0.1071 Acc:98.87%
[[1460.   58.]
 [ 305.  832.]]
0.8632768361581921
Training: Epoch[034/050]  Loss: 0.1071 Acc:98.87%
Training: Epoch[035/050]  Loss: 0.1064 Acc:98.87%
[[1458.   60.]
 [ 300.  837.]]
0.864406779661017
Training: Epoch[036/050]  Loss: 0.1062 Acc:98.87%
Training: Epoch[037/050]  Loss: 0.1060 Acc:98.87%
[[1458.   60.]
 [ 293.  844.]]
0.8670433145009416
Training: Epoch[038/050]  Loss: 0.1051 Acc:98.87%
Training: Epoch[039/050]  Loss: 0.1056 Acc:98.87%
[[1458.   60.]
 [ 288.  849.]]
0.8689265536723164
Training: Epoch[040/050]  Loss: 0.1046 Acc:98.87%
Training: Epoch[041/050]  Loss: 0.1047 Acc:98.87%
[[1456.   62.]
 [ 285.  852.]]
0.8693032015065913
Training: Epoch[042/050]  Loss: 0.1030 Acc:98.87%
Training: Epoch[043/050]  Loss: 0.1036 Acc:98.87%
[[1456.   62.]
 [ 285.  852.]]
0.8693032015065913
Training: Epoch[044/050]  Loss: 0.1021 Acc:98.87%
Training: Epoch[045/050]  Loss: 0.1032 Acc:98.96%
[[1456.   62.]
 [ 284.  853.]]
0.8696798493408663
Training: Epoch[046/050]  Loss: 0.1006 Acc:98.96%
Training: Epoch[047/050]  Loss: 0.1024 Acc:98.96%
[[1455.   63.]
 [ 282.  855.]]
0.8700564971751412
Training: Epoch[048/050]  Loss: 0.0991 Acc:98.96%
Training: Epoch[049/050]  Loss: 0.1024 Acc:98.96%
[[1454.   64.]
 [ 280.  857.]]
0.8704331450094162
Training: Epoch[050/050]  Loss: 0.0968 Acc:98.96%
training 1D-CNN classifier for class1
Training: Epoch[001/050]  Loss: 0.7527 Acc:90.13%
[[   0. 2292.]
 [   0.  734.]]
0.24256444150693984
Training: Epoch[002/050]  Loss: 4.1410 Acc:68.33%
Training: Epoch[003/050]  Loss: 2.5937 Acc:90.13%
[[   0. 2292.]
 [   0.  734.]]
0.24256444150693984
Training: Epoch[004/050]  Loss: 2.2288 Acc:90.13%
Training: Epoch[005/050]  Loss: 2.1164 Acc:90.13%
[[   0. 2292.]
 [   0.  734.]]
0.24256444150693984
Training: Epoch[006/050]  Loss: 2.0391 Acc:90.13%
Training: Epoch[007/050]  Loss: 1.9892 Acc:90.13%
[[   0. 2292.]
 [   0.  734.]]
0.24256444150693984
Training: Epoch[008/050]  Loss: 2.0498 Acc:90.13%
Training: Epoch[009/050]  Loss: 1.8097 Acc:90.13%
[[   0. 2292.]
 [   0.  734.]]
0.24256444150693984
Training: Epoch[010/050]  Loss: 2.2083 Acc:90.13%
Training: Epoch[011/050]  Loss: 1.8087 Acc:90.13%
[[   0. 2292.]
 [   0.  734.]]
0.24256444150693984
Training: Epoch[012/050]  Loss: 1.6189 Acc:90.13%
Training: Epoch[013/050]  Loss: 1.5018 Acc:90.13%
[[   0. 2292.]
 [   0.  734.]]
0.24256444150693984
Training: Epoch[014/050]  Loss: 1.2857 Acc:90.13%
Training: Epoch[015/050]  Loss: 1.1323 Acc:90.13%
[[ 374. 1918.]
 [   0.  734.]]
0.3661599471249174
Training: Epoch[016/050]  Loss: 1.0169 Acc:90.30%
Training: Epoch[017/050]  Loss: 0.7559 Acc:94.33%
[[1046. 1246.]
 [  10.  724.]]
0.5849306014540647
Training: Epoch[018/050]  Loss: 0.6162 Acc:95.97%
Training: Epoch[019/050]  Loss: 0.4795 Acc:97.08%
[[1406.  886.]
 [  31.  703.]]
0.6969596827495043
Training: Epoch[020/050]  Loss: 0.3840 Acc:97.60%
Training: Epoch[021/050]  Loss: 0.3300 Acc:97.77%
[[1597.  695.]
 [  44.  690.]]
0.755783212161269
Training: Epoch[022/050]  Loss: 0.2986 Acc:97.94%
Training: Epoch[023/050]  Loss: 0.2827 Acc:97.94%
[[1659.  633.]
 [  51.  683.]]
0.7739590218109715
Training: Epoch[024/050]  Loss: 0.2743 Acc:97.94%
Training: Epoch[025/050]  Loss: 0.2697 Acc:97.94%
[[1675.  617.]
 [  53.  681.]]
0.7785855915399867
Training: Epoch[026/050]  Loss: 0.2666 Acc:97.94%
Training: Epoch[027/050]  Loss: 0.2640 Acc:98.03%
[[1691.  601.]
 [  55.  679.]]
0.783212161269002
Training: Epoch[028/050]  Loss: 0.2622 Acc:98.03%
Training: Epoch[029/050]  Loss: 0.2610 Acc:98.03%
[[1692.  600.]
 [  54.  680.]]
0.7838730998017185
Training: Epoch[030/050]  Loss: 0.2627 Acc:98.03%
Training: Epoch[031/050]  Loss: 0.3248 Acc:98.03%
[[1316.  976.]
 [  24.  710.]]
0.6695307336417713
Training: Epoch[032/050]  Loss: 0.4969 Acc:96.82%
Training: Epoch[033/050]  Loss: 0.3239 Acc:97.85%
[[1855.  437.]
 [  82.  652.]]
0.8284864507600793
Training: Epoch[034/050]  Loss: 0.2700 Acc:97.77%
Training: Epoch[035/050]  Loss: 0.3478 Acc:97.77%
[[1878.  414.]
 [  84.  650.]]
0.8354263053536021
Training: Epoch[036/050]  Loss: 0.2725 Acc:97.77%
Training: Epoch[037/050]  Loss: 0.3468 Acc:97.77%
[[1907.  385.]
 [  86.  648.]]
0.8443489755452743
Training: Epoch[038/050]  Loss: 0.2783 Acc:97.68%
Training: Epoch[039/050]  Loss: 0.3526 Acc:97.60%
[[1930.  362.]
 [  93.  641.]]
0.849636483807006
Training: Epoch[040/050]  Loss: 0.2822 Acc:97.60%
Training: Epoch[041/050]  Loss: 0.3438 Acc:97.77%
[[1932.  360.]
 [  93.  641.]]
0.8502974223397224
Training: Epoch[042/050]  Loss: 0.2843 Acc:97.60%
Training: Epoch[043/050]  Loss: 0.3354 Acc:97.77%
[[1936.  356.]
 [  94.  640.]]
0.8512888301387971
Training: Epoch[044/050]  Loss: 0.2812 Acc:97.60%
Training: Epoch[045/050]  Loss: 0.3212 Acc:97.77%
[[1929.  363.]
 [  93.  641.]]
0.8493060145406477
Training: Epoch[046/050]  Loss: 0.2788 Acc:97.60%
Training: Epoch[047/050]  Loss: 0.3133 Acc:97.85%
[[1925.  367.]
 [  91.  643.]]
0.8486450760079313
Training: Epoch[048/050]  Loss: 0.2739 Acc:97.68%
Training: Epoch[049/050]  Loss: 0.3038 Acc:97.85%
[[1923.  369.]
 [  90.  644.]]
0.848314606741573
Training: Epoch[050/050]  Loss: 0.2719 Acc:97.77%
Training: Epoch[001/050]  Loss: 0.9294 Acc:90.13%
[[   0. 2292.]
 [   0.  734.]]
0.24256444150693984
Training: Epoch[002/050]  Loss: 2.9459 Acc:90.13%
Training: Epoch[003/050]  Loss: 2.2878 Acc:90.13%
[[   0. 2292.]
 [   0.  734.]]
0.24256444150693984
Training: Epoch[004/050]  Loss: 2.3481 Acc:90.13%
Training: Epoch[005/050]  Loss: 2.0479 Acc:90.13%
[[   0. 2292.]
 [   0.  734.]]
0.24256444150693984
Training: Epoch[006/050]  Loss: 1.9362 Acc:90.13%
Training: Epoch[007/050]  Loss: 1.8475 Acc:90.13%
[[   0. 2292.]
 [   0.  734.]]
0.24256444150693984
Training: Epoch[008/050]  Loss: 1.7815 Acc:90.13%
Training: Epoch[009/050]  Loss: 1.6885 Acc:90.13%
[[   0. 2292.]
 [   0.  734.]]
0.24256444150693984
Training: Epoch[010/050]  Loss: 1.6052 Acc:90.13%
Training: Epoch[011/050]  Loss: 1.4866 Acc:90.13%
[[   0. 2292.]
 [   0.  734.]]
0.24256444150693984
Training: Epoch[012/050]  Loss: 1.4911 Acc:90.13%
Training: Epoch[013/050]  Loss: 1.3175 Acc:90.13%
[[   0. 2292.]
 [   0.  734.]]
0.24256444150693984
Training: Epoch[014/050]  Loss: 1.1858 Acc:90.13%
Training: Epoch[015/050]  Loss: 1.0556 Acc:90.13%
[[   0. 2292.]
 [   0.  734.]]
0.24256444150693984
Training: Epoch[016/050]  Loss: 1.0062 Acc:90.13%
Training: Epoch[017/050]  Loss: 0.7910 Acc:90.39%
[[1246. 1046.]
 [  13.  721.]]
0.6500330469266358
Training: Epoch[018/050]  Loss: 0.6579 Acc:94.68%
Training: Epoch[019/050]  Loss: 0.5482 Acc:96.22%
[[1592.  700.]
 [  37.  697.]]
0.7564441506939854
Training: Epoch[020/050]  Loss: 0.4420 Acc:97.60%
Training: Epoch[021/050]  Loss: 0.3628 Acc:97.77%
[[1782.  510.]
 [  63.  671.]]
0.8106411103767349
Training: Epoch[022/050]  Loss: 0.3105 Acc:97.77%
Training: Epoch[023/050]  Loss: 0.2868 Acc:97.60%
[[1830.  462.]
 [  74.  660.]]
0.8228684732319894
Training: Epoch[024/050]  Loss: 0.2803 Acc:97.51%
Training: Epoch[025/050]  Loss: 0.2754 Acc:97.68%
[[1827.  465.]
 [  71.  663.]]
0.8228684732319894
Training: Epoch[026/050]  Loss: 0.2737 Acc:97.77%
Training: Epoch[027/050]  Loss: 0.2714 Acc:97.85%
[[1796.  496.]
 [  66.  668.]]
0.8142762723066754
Training: Epoch[028/050]  Loss: 0.2755 Acc:97.77%
Training: Epoch[029/050]  Loss: 0.2687 Acc:97.85%
[[1823.  469.]
 [  71.  663.]]
0.8215465961665566
Training: Epoch[030/050]  Loss: 0.2690 Acc:97.85%
Training: Epoch[031/050]  Loss: 0.3340 Acc:97.68%
[[1542.  750.]
 [  35.  699.]]
0.7405816259087905
Training: Epoch[032/050]  Loss: 0.4552 Acc:97.17%
Training: Epoch[033/050]  Loss: 0.3714 Acc:97.77%
[[1830.  462.]
 [  72.  662.]]
0.8235294117647058
Training: Epoch[034/050]  Loss: 0.2654 Acc:97.85%
Training: Epoch[035/050]  Loss: 0.2650 Acc:97.85%
[[1851.  441.]
 [  79.  655.]]
0.828155981493721
Training: Epoch[036/050]  Loss: 0.2666 Acc:97.85%
Training: Epoch[037/050]  Loss: 0.2877 Acc:98.03%
[[1880.  412.]
 [  83.  651.]]
0.8364177131526768
Training: Epoch[038/050]  Loss: 0.2727 Acc:97.77%
Training: Epoch[039/050]  Loss: 0.3332 Acc:97.85%
[[1856.  436.]
 [  82.  652.]]
0.8288169200264376
Training: Epoch[040/050]  Loss: 0.2652 Acc:97.85%
Training: Epoch[041/050]  Loss: 0.2894 Acc:98.03%
[[1884.  408.]
 [  83.  651.]]
0.8377395902181097
Training: Epoch[042/050]  Loss: 0.2704 Acc:97.77%
Training: Epoch[043/050]  Loss: 0.3245 Acc:97.85%
[[1877.  415.]
 [  82.  652.]]
0.8357567746199603
Training: Epoch[044/050]  Loss: 0.2658 Acc:97.77%
Training: Epoch[045/050]  Loss: 0.2997 Acc:97.85%
[[1894.  398.]
 [  83.  651.]]
0.841044282881692
Training: Epoch[046/050]  Loss: 0.2685 Acc:97.85%
Training: Epoch[047/050]  Loss: 0.3134 Acc:97.77%
[[1897.  395.]
 [  83.  651.]]
0.8420356906807667
Training: Epoch[048/050]  Loss: 0.2674 Acc:97.85%
Training: Epoch[049/050]  Loss: 0.3068 Acc:97.77%
[[1904.  388.]
 [  83.  651.]]
0.8443489755452743
Training: Epoch[050/050]  Loss: 0.2685 Acc:97.85%
training 1D-CNN classifier for class2
Training: Epoch[001/050]  Loss: 0.9959 Acc:87.03%
[[   0. 1625.]
 [   0. 1046.]]
0.3916136278547361
Training: Epoch[002/050]  Loss: 3.6804 Acc:88.00%
Training: Epoch[003/050]  Loss: 2.4554 Acc:88.00%
[[   0. 1625.]
 [   0. 1046.]]
0.3916136278547361
Training: Epoch[004/050]  Loss: 2.2115 Acc:88.00%
Training: Epoch[005/050]  Loss: 2.0951 Acc:88.00%
[[   0. 1625.]
 [   0. 1046.]]
0.3916136278547361
Training: Epoch[006/050]  Loss: 2.0046 Acc:88.00%
Training: Epoch[007/050]  Loss: 1.9145 Acc:88.00%
[[   0. 1625.]
 [   0. 1046.]]
0.3916136278547361
Training: Epoch[008/050]  Loss: 2.1459 Acc:88.00%
Training: Epoch[009/050]  Loss: 1.8830 Acc:88.00%
[[   0. 1625.]
 [   0. 1046.]]
0.3916136278547361
Training: Epoch[010/050]  Loss: 1.7301 Acc:88.00%
Training: Epoch[011/050]  Loss: 1.6244 Acc:88.00%
[[   0. 1625.]
 [   0. 1046.]]
0.3916136278547361
Training: Epoch[012/050]  Loss: 1.5255 Acc:88.00%
Training: Epoch[013/050]  Loss: 1.4281 Acc:88.00%
[[   0. 1625.]
 [   0. 1046.]]
0.3916136278547361
Training: Epoch[014/050]  Loss: 1.3170 Acc:88.00%
Training: Epoch[015/050]  Loss: 1.1956 Acc:88.00%
[[   0. 1625.]
 [   0. 1046.]]
0.3916136278547361
Training: Epoch[016/050]  Loss: 1.0635 Acc:87.90%
Training: Epoch[017/050]  Loss: 0.9663 Acc:89.35%
[[ 807.  818.]
 [  46. 1000.]]
0.6765256458255335
Training: Epoch[018/050]  Loss: 0.7959 Acc:92.35%
Training: Epoch[019/050]  Loss: 0.6552 Acc:94.87%
[[1214.  411.]
 [ 159.  887.]]
0.7865967802321228
Training: Epoch[020/050]  Loss: 0.5391 Acc:95.93%
Training: Epoch[021/050]  Loss: 0.4494 Acc:96.71%
[[1349.  276.]
 [ 257.  789.]]
0.8004492699363535
Training: Epoch[022/050]  Loss: 0.3868 Acc:96.71%
Training: Epoch[023/050]  Loss: 0.3466 Acc:97.00%
[[1409.  216.]
 [ 322.  724.]]
0.7985773118682141
Training: Epoch[024/050]  Loss: 0.3228 Acc:97.00%
Training: Epoch[025/050]  Loss: 0.3100 Acc:96.90%
[[1419.  206.]
 [ 339.  707.]]
0.7959565705728192
Training: Epoch[026/050]  Loss: 0.3067 Acc:96.81%
Training: Epoch[027/050]  Loss: 0.3125 Acc:96.81%
[[1397.  228.]
 [ 305.  741.]]
0.8004492699363535
Training: Epoch[028/050]  Loss: 0.3386 Acc:96.90%
Training: Epoch[029/050]  Loss: 0.3874 Acc:96.90%
[[1316.  309.]
 [ 239.  807.]]
0.7948333957319356
Training: Epoch[030/050]  Loss: 0.4023 Acc:96.81%
Training: Epoch[031/050]  Loss: 0.3434 Acc:96.90%
[[1433.  192.]
 [ 361.  685.]]
0.7929614376637963
Training: Epoch[032/050]  Loss: 0.2749 Acc:97.68%
Training: Epoch[033/050]  Loss: 0.2670 Acc:97.87%
[[1419.  206.]
 [ 340.  706.]]
0.7955821789591914
Training: Epoch[034/050]  Loss: 0.2809 Acc:97.48%
Training: Epoch[035/050]  Loss: 0.2647 Acc:97.87%
[[1421.  204.]
 [ 340.  706.]]
0.7963309621864471
Training: Epoch[036/050]  Loss: 0.2788 Acc:97.48%
Training: Epoch[037/050]  Loss: 0.2807 Acc:97.39%
[[1400.  225.]
 [ 309.  737.]]
0.8000748783227256
Training: Epoch[038/050]  Loss: 0.3028 Acc:97.39%
Training: Epoch[039/050]  Loss: 0.3600 Acc:96.90%
[[1315.  310.]
 [ 233.  813.]]
0.7967053538000749
Training: Epoch[040/050]  Loss: 0.3846 Acc:96.71%
Training: Epoch[041/050]  Loss: 0.3161 Acc:97.00%
[[1441.  184.]
 [ 377.  669.]]
0.7899663047547735
Training: Epoch[042/050]  Loss: 0.2596 Acc:97.77%
Training: Epoch[043/050]  Loss: 0.2652 Acc:97.58%
[[1446.  179.]
 [ 385.  661.]]
0.7888431299138899
Training: Epoch[044/050]  Loss: 0.2554 Acc:97.68%
Training: Epoch[045/050]  Loss: 0.2920 Acc:97.29%
[[1419.  206.]
 [ 346.  700.]]
0.7933358292774242
Training: Epoch[046/050]  Loss: 0.2563 Acc:97.77%
Training: Epoch[047/050]  Loss: 0.2512 Acc:97.87%
[[1399.  226.]
 [ 320.  726.]]
0.7955821789591914
Training: Epoch[048/050]  Loss: 0.2598 Acc:97.68%
Training: Epoch[049/050]  Loss: 0.2605 Acc:97.58%
[[1382.  243.]
 [ 299.  747.]]
0.7970797454137027
Training: Epoch[050/050]  Loss: 0.2754 Acc:97.58%
Training: Epoch[001/050]  Loss: 4.3460 Acc:63.41%
[[   0. 1625.]
 [   0. 1046.]]
0.3916136278547361
Training: Epoch[002/050]  Loss: 2.0640 Acc:88.00%
Training: Epoch[003/050]  Loss: 1.9988 Acc:88.00%
[[   0. 1625.]
 [   0. 1046.]]
0.3916136278547361
Training: Epoch[004/050]  Loss: 1.9373 Acc:88.00%
Training: Epoch[005/050]  Loss: 1.8686 Acc:88.00%
[[   0. 1625.]
 [   0. 1046.]]
0.3916136278547361
Training: Epoch[006/050]  Loss: 1.8037 Acc:88.00%
Training: Epoch[007/050]  Loss: 1.6863 Acc:88.00%
[[   0. 1625.]
 [   0. 1046.]]
0.3916136278547361
Training: Epoch[008/050]  Loss: 1.5851 Acc:88.00%
Training: Epoch[009/050]  Loss: 1.4792 Acc:88.00%
[[   0. 1625.]
 [   0. 1046.]]
0.3916136278547361
Training: Epoch[010/050]  Loss: 1.3649 Acc:88.00%
Training: Epoch[011/050]  Loss: 1.2568 Acc:88.00%
[[ 140. 1485.]
 [   2. 1044.]]
0.44327967053538003
Training: Epoch[012/050]  Loss: 1.1075 Acc:87.90%
Training: Epoch[013/050]  Loss: 0.9714 Acc:89.25%
[[ 759.  866.]
 [  42. 1004.]]
0.6600524148259079
Training: Epoch[014/050]  Loss: 0.8468 Acc:91.97%
Training: Epoch[015/050]  Loss: 0.7015 Acc:94.29%
[[1068.  557.]
 [ 107.  939.]]
0.7514039685511045
Training: Epoch[016/050]  Loss: 0.6009 Acc:95.74%
Training: Epoch[017/050]  Loss: 0.4945 Acc:96.22%
[[1226.  399.]
 [ 170.  876.]]
0.7869711718457506
Training: Epoch[018/050]  Loss: 0.4241 Acc:96.81%
Training: Epoch[019/050]  Loss: 0.3786 Acc:96.81%
[[1290.  335.]
 [ 217.  829.]]
0.7933358292774242
Training: Epoch[020/050]  Loss: 0.3494 Acc:97.00%
Training: Epoch[021/050]  Loss: 0.3353 Acc:96.90%
[[1328.  297.]
 [ 240.  806.]]
0.798951703481842
Training: Epoch[022/050]  Loss: 0.3289 Acc:97.00%
Training: Epoch[023/050]  Loss: 0.3358 Acc:97.00%
[[1305.  320.]
 [ 226.  820.]]
0.7955821789591914
Training: Epoch[024/050]  Loss: 0.3651 Acc:97.00%
Training: Epoch[025/050]  Loss: 0.4112 Acc:96.71%
[[1250.  375.]
 [ 184.  862.]]
0.7907150879820292
Training: Epoch[026/050]  Loss: 0.4281 Acc:96.81%
Training: Epoch[027/050]  Loss: 0.3681 Acc:97.00%
[[1354.  271.]
 [ 267.  779.]]
0.7985773118682141
Training: Epoch[028/050]  Loss: 0.2886 Acc:97.48%
Training: Epoch[029/050]  Loss: 0.2647 Acc:97.68%
[[1374.  251.]
 [ 289.  757.]]
0.7978285286409584
Training: Epoch[030/050]  Loss: 0.2767 Acc:97.68%
Training: Epoch[031/050]  Loss: 0.2668 Acc:97.87%
[[1383.  242.]
 [ 300.  746.]]
0.7970797454137027
Training: Epoch[032/050]  Loss: 0.2726 Acc:97.68%
Training: Epoch[033/050]  Loss: 0.2952 Acc:97.48%
[[1315.  310.]
 [ 229.  817.]]
0.7982029202545863
Training: Epoch[034/050]  Loss: 0.3724 Acc:96.90%
Training: Epoch[035/050]  Loss: 0.4319 Acc:96.52%
[[1278.  347.]
 [ 206.  840.]]
0.7929614376637963
Training: Epoch[036/050]  Loss: 0.3724 Acc:97.10%
Training: Epoch[037/050]  Loss: 0.2777 Acc:97.68%
[[1411.  214.]
 [ 342.  704.]]
0.7918382628229128
Training: Epoch[038/050]  Loss: 0.2590 Acc:97.77%
Training: Epoch[039/050]  Loss: 0.2789 Acc:97.48%
[[1398.  227.]
 [ 320.  726.]]
0.7952077873455634
Training: Epoch[040/050]  Loss: 0.2565 Acc:97.68%
Training: Epoch[041/050]  Loss: 0.2636 Acc:97.77%
[[1388.  237.]
 [ 305.  741.]]
0.7970797454137027
Training: Epoch[042/050]  Loss: 0.2608 Acc:97.87%
Training: Epoch[043/050]  Loss: 0.2703 Acc:97.68%
[[1351.  274.]
 [ 263.  783.]]
0.798951703481842
Training: Epoch[044/050]  Loss: 0.3069 Acc:97.48%
Training: Epoch[045/050]  Loss: 0.3548 Acc:97.29%
[[1295.  330.]
 [ 218.  828.]]
0.7948333957319356
Training: Epoch[046/050]  Loss: 0.3499 Acc:97.10%
Training: Epoch[047/050]  Loss: 0.2847 Acc:97.48%
[[1407.  218.]
 [ 333.  713.]]
0.793710220891052
Training: Epoch[048/050]  Loss: 0.2502 Acc:97.77%
Training: Epoch[049/050]  Loss: 0.2601 Acc:97.68%
[[1403.  222.]
 [ 329.  717.]]
0.793710220891052
Training: Epoch[050/050]  Loss: 0.2454 Acc:97.77%
training 1D-CNN classifier for class3
Training: Epoch[001/050]  Loss: 3.3144 Acc:64.32%
[[   0. 1577.]
 [   0.  759.]]
0.3249143835616438
Training: Epoch[002/050]  Loss: 2.4503 Acc:82.27%
Training: Epoch[003/050]  Loss: 2.3134 Acc:82.27%
[[   0. 1577.]
 [   0.  759.]]
0.3249143835616438
Training: Epoch[004/050]  Loss: 2.2775 Acc:82.27%
Training: Epoch[005/050]  Loss: 2.1606 Acc:82.27%
[[   0. 1577.]
 [   0.  759.]]
0.3249143835616438
Training: Epoch[006/050]  Loss: 2.1028 Acc:82.27%
Training: Epoch[007/050]  Loss: 2.0660 Acc:82.27%
[[   0. 1577.]
 [   0.  759.]]
0.3249143835616438
Training: Epoch[008/050]  Loss: 2.0374 Acc:82.27%
Training: Epoch[009/050]  Loss: 2.0015 Acc:82.27%
[[   0. 1577.]
 [   0.  759.]]
0.3249143835616438
Training: Epoch[010/050]  Loss: 1.9162 Acc:82.27%
Training: Epoch[011/050]  Loss: 2.6008 Acc:82.27%
[[   0. 1577.]
 [   0.  759.]]
0.3249143835616438
Training: Epoch[012/050]  Loss: 2.1109 Acc:82.27%
Training: Epoch[013/050]  Loss: 2.0297 Acc:82.27%
[[   0. 1577.]
 [   0.  759.]]
0.3249143835616438
Training: Epoch[014/050]  Loss: 1.8253 Acc:82.27%
Training: Epoch[015/050]  Loss: 1.9132 Acc:82.27%
[[   0. 1577.]
 [   0.  759.]]
0.3249143835616438
Training: Epoch[016/050]  Loss: 1.7107 Acc:82.27%
Training: Epoch[017/050]  Loss: 1.7278 Acc:82.27%
[[   0. 1577.]
 [   0.  759.]]
0.3249143835616438
Training: Epoch[018/050]  Loss: 1.5871 Acc:82.27%
Training: Epoch[019/050]  Loss: 1.6519 Acc:82.27%
[[   0. 1577.]
 [   0.  759.]]
0.3249143835616438
Training: Epoch[020/050]  Loss: 1.4870 Acc:82.27%
Training: Epoch[021/050]  Loss: 1.4822 Acc:82.27%
[[   2. 1575.]
 [   0.  759.]]
0.3257705479452055
Training: Epoch[022/050]  Loss: 1.3708 Acc:82.38%
Training: Epoch[023/050]  Loss: 1.3271 Acc:82.49%
[[ 379. 1198.]
 [  13.  746.]]
0.4815924657534247
Training: Epoch[024/050]  Loss: 1.2366 Acc:82.70%
Training: Epoch[025/050]  Loss: 1.1708 Acc:85.62%
[[597. 980.]
 [ 41. 718.]]
0.5629280821917808
Training: Epoch[026/050]  Loss: 1.0970 Acc:86.81%
Training: Epoch[027/050]  Loss: 1.0282 Acc:88.86%
[[758. 819.]
 [ 79. 680.]]
0.615582191780822
Training: Epoch[028/050]  Loss: 0.9547 Acc:90.49%
Training: Epoch[029/050]  Loss: 0.8620 Acc:91.46%
[[937. 640.]
 [136. 623.]]
0.6678082191780822
Training: Epoch[030/050]  Loss: 0.7883 Acc:92.11%
Training: Epoch[031/050]  Loss: 0.7230 Acc:93.08%
[[1045.  532.]
 [ 199.  560.]]
0.6870719178082192
Training: Epoch[032/050]  Loss: 0.6586 Acc:93.41%
Training: Epoch[033/050]  Loss: 0.5947 Acc:94.05%
[[1174.  403.]
 [ 263.  496.]]
0.7148972602739726
Training: Epoch[034/050]  Loss: 0.5293 Acc:94.70%
Training: Epoch[035/050]  Loss: 0.4810 Acc:94.92%
[[1367.  210.]
 [ 399.  360.]]
0.7392979452054794
Training: Epoch[036/050]  Loss: 0.5053 Acc:95.14%
Training: Epoch[037/050]  Loss: 0.4967 Acc:95.14%
[[1351.  226.]
 [ 389.  370.]]
0.7367294520547946
Training: Epoch[038/050]  Loss: 0.5022 Acc:95.24%
Training: Epoch[039/050]  Loss: 0.4980 Acc:95.03%
[[1338.  239.]
 [ 379.  380.]]
0.735445205479452
Training: Epoch[040/050]  Loss: 0.5028 Acc:95.03%
Training: Epoch[041/050]  Loss: 0.5009 Acc:95.03%
[[1327.  250.]
 [ 374.  385.]]
0.7328767123287672
Training: Epoch[042/050]  Loss: 0.5044 Acc:95.03%
Training: Epoch[043/050]  Loss: 0.5037 Acc:95.03%
[[1318.  259.]
 [ 369.  390.]]
0.7311643835616438
Training: Epoch[044/050]  Loss: 0.5056 Acc:95.03%
Training: Epoch[045/050]  Loss: 0.5048 Acc:95.14%
[[1318.  259.]
 [ 365.  394.]]
0.7328767123287672
Training: Epoch[046/050]  Loss: 0.5050 Acc:95.14%
Training: Epoch[047/050]  Loss: 0.5053 Acc:95.14%
[[1316.  261.]
 [ 362.  397.]]
0.733304794520548
Training: Epoch[048/050]  Loss: 0.5048 Acc:95.24%
Training: Epoch[049/050]  Loss: 0.5052 Acc:95.24%
[[1316.  261.]
 [ 360.  399.]]
0.7341609589041096
Training: Epoch[050/050]  Loss: 0.5056 Acc:95.24%
Training: Epoch[001/050]  Loss: 4.4863 Acc:62.16%
[[   0. 1577.]
 [   0.  759.]]
0.3249143835616438
Training: Epoch[002/050]  Loss: 2.7824 Acc:82.27%
Training: Epoch[003/050]  Loss: 2.4647 Acc:82.27%
[[   0. 1577.]
 [   0.  759.]]
0.3249143835616438
Training: Epoch[004/050]  Loss: 2.3270 Acc:82.27%
Training: Epoch[005/050]  Loss: 2.2667 Acc:82.27%
[[   0. 1577.]
 [   0.  759.]]
0.3249143835616438
Training: Epoch[006/050]  Loss: 2.2159 Acc:82.27%
Training: Epoch[007/050]  Loss: 2.1564 Acc:82.27%
[[   0. 1577.]
 [   0.  759.]]
0.3249143835616438
Training: Epoch[008/050]  Loss: 2.1287 Acc:82.27%
Training: Epoch[009/050]  Loss: 2.0543 Acc:82.27%
[[   0. 1577.]
 [   0.  759.]]
0.3249143835616438
Training: Epoch[010/050]  Loss: 2.0447 Acc:82.27%
Training: Epoch[011/050]  Loss: 1.9599 Acc:82.27%
[[   0. 1577.]
 [   0.  759.]]
0.3249143835616438
Training: Epoch[012/050]  Loss: 1.8792 Acc:82.27%
Training: Epoch[013/050]  Loss: 1.8411 Acc:82.27%
[[   0. 1577.]
 [   0.  759.]]
0.3249143835616438
Training: Epoch[014/050]  Loss: 1.7313 Acc:82.27%
Training: Epoch[015/050]  Loss: 1.7757 Acc:82.38%
[[   0. 1577.]
 [   0.  759.]]
0.3249143835616438
Training: Epoch[016/050]  Loss: 1.5093 Acc:82.38%
Training: Epoch[017/050]  Loss: 1.4046 Acc:82.27%
[[ 400. 1177.]
 [  15.  744.]]
0.4897260273972603
Training: Epoch[018/050]  Loss: 1.2735 Acc:82.38%
Training: Epoch[019/050]  Loss: 1.1369 Acc:86.16%
[[739. 838.]
 [ 67. 692.]]
0.6125856164383562
Training: Epoch[020/050]  Loss: 1.0465 Acc:88.76%
Training: Epoch[021/050]  Loss: 0.9290 Acc:91.03%
[[955. 622.]
 [139. 620.]]
0.6742294520547946
Training: Epoch[022/050]  Loss: 0.8134 Acc:92.43%
Training: Epoch[023/050]  Loss: 0.7202 Acc:92.97%
[[1073.  504.]
 [ 212.  547.]]
0.6934931506849316
Training: Epoch[024/050]  Loss: 0.6588 Acc:93.30%
Training: Epoch[025/050]  Loss: 0.5811 Acc:94.05%
[[1200.  377.]
 [ 275.  484.]]
0.7208904109589042
Training: Epoch[026/050]  Loss: 0.5108 Acc:95.24%
Training: Epoch[027/050]  Loss: 0.4776 Acc:95.14%
[[1331.  246.]
 [ 374.  385.]]
0.7345890410958904
Training: Epoch[028/050]  Loss: 0.5024 Acc:95.03%
Training: Epoch[029/050]  Loss: 0.4938 Acc:95.14%
[[1325.  252.]
 [ 366.  393.]]
0.735445205479452
Training: Epoch[030/050]  Loss: 0.4981 Acc:95.14%
Training: Epoch[031/050]  Loss: 0.4964 Acc:95.24%
[[1324.  253.]
 [ 364.  395.]]
0.7358732876712328
Training: Epoch[032/050]  Loss: 0.4994 Acc:95.24%
Training: Epoch[033/050]  Loss: 0.4995 Acc:95.24%
[[1322.  255.]
 [ 362.  397.]]
0.7358732876712328
Training: Epoch[034/050]  Loss: 0.5004 Acc:95.24%
Training: Epoch[035/050]  Loss: 0.5007 Acc:95.24%
[[1321.  256.]
 [ 360.  399.]]
0.7363013698630136
Training: Epoch[036/050]  Loss: 0.5009 Acc:95.24%
Training: Epoch[037/050]  Loss: 0.5011 Acc:95.35%
[[1323.  254.]
 [ 360.  399.]]
0.7371575342465754
Training: Epoch[038/050]  Loss: 0.5007 Acc:95.35%
Training: Epoch[039/050]  Loss: 0.4996 Acc:95.46%
[[1324.  253.]
 [ 365.  394.]]
0.735445205479452
Training: Epoch[040/050]  Loss: 0.5014 Acc:95.46%
Training: Epoch[041/050]  Loss: 0.4984 Acc:95.46%
[[1336.  241.]
 [ 370.  389.]]
0.7384417808219178
Training: Epoch[042/050]  Loss: 0.5062 Acc:95.24%
Training: Epoch[043/050]  Loss: 0.4963 Acc:95.35%
[[1345.  232.]
 [ 375.  384.]]
0.740154109589041
Training: Epoch[044/050]  Loss: 0.5042 Acc:95.24%
Training: Epoch[045/050]  Loss: 0.4905 Acc:95.46%
[[1346.  231.]
 [ 377.  382.]]
0.7397260273972602
Training: Epoch[046/050]  Loss: 0.5002 Acc:95.24%
Training: Epoch[047/050]  Loss: 0.4889 Acc:95.46%
[[1347.  230.]
 [ 376.  383.]]
0.740582191780822
Training: Epoch[048/050]  Loss: 0.4936 Acc:95.57%
Training: Epoch[049/050]  Loss: 0.4889 Acc:95.57%
[[1344.  233.]
 [ 371.  388.]]
0.7414383561643836
Training: Epoch[050/050]  Loss: 0.4834 Acc:95.57%
training 1D-CNN classifier for class4
Training: Epoch[001/050]  Loss: 4.0510 Acc:46.70%
[[   0. 1168.]
 [   0.  852.]]
0.42178217821782177
Training: Epoch[002/050]  Loss: 2.8204 Acc:74.74%
Training: Epoch[003/050]  Loss: 2.8441 Acc:74.74%
[[   0. 1168.]
 [   0.  852.]]
0.42178217821782177
Training: Epoch[004/050]  Loss: 2.6784 Acc:74.74%
Training: Epoch[005/050]  Loss: 2.6405 Acc:74.74%
[[   0. 1168.]
 [   0.  852.]]
0.42178217821782177
Training: Epoch[006/050]  Loss: 2.5067 Acc:74.74%
Training: Epoch[007/050]  Loss: 2.4730 Acc:74.74%
[[   0. 1168.]
 [   0.  852.]]
0.42178217821782177
Training: Epoch[008/050]  Loss: 2.3000 Acc:74.74%
Training: Epoch[009/050]  Loss: 2.2548 Acc:74.74%
[[   0. 1168.]
 [   0.  852.]]
0.42178217821782177
Training: Epoch[010/050]  Loss: 2.2616 Acc:74.74%
Training: Epoch[011/050]  Loss: 1.9358 Acc:74.74%
[[   0. 1168.]
 [   0.  852.]]
0.42178217821782177
Training: Epoch[012/050]  Loss: 1.7641 Acc:74.86%
Training: Epoch[013/050]  Loss: 1.5935 Acc:74.62%
[[755. 413.]
 [ 31. 821.]]
0.7801980198019802
Training: Epoch[014/050]  Loss: 1.4140 Acc:75.67%
Training: Epoch[015/050]  Loss: 1.2223 Acc:84.59%
[[1007.  161.]
 [ 147.  705.]]
0.8475247524752475
Training: Epoch[016/050]  Loss: 1.0521 Acc:88.18%
Training: Epoch[017/050]  Loss: 0.8976 Acc:90.96%
[[1077.   91.]
 [ 245.  607.]]
0.8336633663366336
Training: Epoch[018/050]  Loss: 0.7368 Acc:93.28%
Training: Epoch[019/050]  Loss: 0.6213 Acc:94.55%
[[1110.   58.]
 [ 323.  529.]]
0.8113861386138614
Training: Epoch[020/050]  Loss: 0.5296 Acc:95.37%
Training: Epoch[021/050]  Loss: 0.4489 Acc:96.41%
[[1129.   39.]
 [ 394.  458.]]
0.7856435643564357
Training: Epoch[022/050]  Loss: 0.3916 Acc:96.76%
Training: Epoch[023/050]  Loss: 0.3529 Acc:96.87%
[[1135.   33.]
 [ 440.  412.]]
0.7658415841584159
Training: Epoch[024/050]  Loss: 0.3266 Acc:97.57%
Training: Epoch[025/050]  Loss: 0.3080 Acc:97.57%
[[1138.   30.]
 [ 460.  392.]]
0.7574257425742574
Training: Epoch[026/050]  Loss: 0.2951 Acc:97.68%
Training: Epoch[027/050]  Loss: 0.2867 Acc:97.68%
[[1139.   29.]
 [ 471.  381.]]
0.7524752475247525
Training: Epoch[028/050]  Loss: 0.2804 Acc:97.80%
Training: Epoch[029/050]  Loss: 0.2763 Acc:97.80%
[[1139.   29.]
 [ 471.  381.]]
0.7524752475247525
Training: Epoch[030/050]  Loss: 0.2751 Acc:97.80%
Training: Epoch[031/050]  Loss: 0.2788 Acc:97.80%
[[1136.   32.]
 [ 448.  404.]]
0.7623762376237624
Training: Epoch[032/050]  Loss: 0.2915 Acc:97.68%
Training: Epoch[033/050]  Loss: 0.3188 Acc:96.87%
[[1129.   39.]
 [ 380.  472.]]
0.7925742574257426
Training: Epoch[034/050]  Loss: 0.3470 Acc:96.06%
Training: Epoch[035/050]  Loss: 0.3402 Acc:96.18%
[[1133.   35.]
 [ 406.  446.]]
0.7816831683168317
Training: Epoch[036/050]  Loss: 0.2926 Acc:97.68%
Training: Epoch[037/050]  Loss: 0.2562 Acc:97.57%
[[1154.   14.]
 [ 548.  304.]]
0.7217821782178218
Training: Epoch[038/050]  Loss: 0.2640 Acc:97.57%
Training: Epoch[039/050]  Loss: 0.2580 Acc:97.57%
[[1151.   17.]
 [ 529.  323.]]
0.7297029702970297
Training: Epoch[040/050]  Loss: 0.2552 Acc:97.57%
Training: Epoch[041/050]  Loss: 0.2534 Acc:97.57%
[[1149.   19.]
 [ 522.  330.]]
0.7321782178217822
Training: Epoch[042/050]  Loss: 0.2521 Acc:97.57%
Training: Epoch[043/050]  Loss: 0.2511 Acc:97.57%
[[1148.   20.]
 [ 519.  333.]]
0.7331683168316832
Training: Epoch[044/050]  Loss: 0.2503 Acc:97.57%
Training: Epoch[045/050]  Loss: 0.2503 Acc:97.57%
[[1149.   19.]
 [ 521.  331.]]
0.7326732673267327
Training: Epoch[046/050]  Loss: 0.2523 Acc:97.57%
Training: Epoch[047/050]  Loss: 0.2571 Acc:97.57%
[[1150.   18.]
 [ 524.  328.]]
0.7316831683168317
Training: Epoch[048/050]  Loss: 0.2572 Acc:97.57%
Training: Epoch[049/050]  Loss: 0.2529 Acc:97.57%
[[1148.   20.]
 [ 514.  338.]]
0.7356435643564356
Training: Epoch[050/050]  Loss: 0.2494 Acc:97.57%
Training: Epoch[001/050]  Loss: 5.6389 Acc:66.98%
[[   0. 1168.]
 [   0.  852.]]
0.42178217821782177
Training: Epoch[002/050]  Loss: 3.2325 Acc:47.05%
Training: Epoch[003/050]  Loss: 2.7734 Acc:74.74%
[[   0. 1168.]
 [   0.  852.]]
0.42178217821782177
Training: Epoch[004/050]  Loss: 2.5445 Acc:74.74%
Training: Epoch[005/050]  Loss: 2.4231 Acc:74.74%
[[   0. 1168.]
 [   0.  852.]]
0.42178217821782177
Training: Epoch[006/050]  Loss: 2.2947 Acc:74.74%
Training: Epoch[007/050]  Loss: 2.1368 Acc:74.28%
[[   0. 1168.]
 [   0.  852.]]
0.42178217821782177
Training: Epoch[008/050]  Loss: 1.9527 Acc:73.58%
Training: Epoch[009/050]  Loss: 1.7737 Acc:72.89%
[[559. 609.]
 [  8. 844.]]
0.6945544554455445
Training: Epoch[010/050]  Loss: 1.5553 Acc:76.48%
Training: Epoch[011/050]  Loss: 1.3545 Acc:82.04%
[[861. 307.]
 [ 49. 803.]]
0.8237623762376237
Training: Epoch[012/050]  Loss: 1.1496 Acc:85.86%
Training: Epoch[013/050]  Loss: 0.9208 Acc:90.61%
[[985. 183.]
 [120. 732.]]
0.85
Training: Epoch[014/050]  Loss: 0.7837 Acc:91.77%
Training: Epoch[015/050]  Loss: 0.6425 Acc:93.28%
[[1049.  119.]
 [ 178.  674.]]
0.852970297029703
Training: Epoch[016/050]  Loss: 0.5223 Acc:95.13%
Training: Epoch[017/050]  Loss: 0.4449 Acc:96.18%
[[1090.   78.]
 [ 252.  600.]]
0.8366336633663366
Training: Epoch[018/050]  Loss: 0.3973 Acc:96.64%
Training: Epoch[019/050]  Loss: 0.3637 Acc:96.76%
[[1108.   60.]
 [ 297.  555.]]
0.8232673267326732
Training: Epoch[020/050]  Loss: 0.3423 Acc:96.76%
Training: Epoch[021/050]  Loss: 0.3311 Acc:96.76%
[[1113.   55.]
 [ 322.  530.]]
0.8133663366336633
Training: Epoch[022/050]  Loss: 0.3273 Acc:96.64%
Training: Epoch[023/050]  Loss: 0.3362 Acc:96.29%
[[1103.   65.]
 [ 291.  561.]]
0.8237623762376237
Training: Epoch[024/050]  Loss: 0.3531 Acc:96.29%
Training: Epoch[025/050]  Loss: 0.3687 Acc:96.06%
[[1097.   71.]
 [ 264.  588.]]
0.8341584158415841
Training: Epoch[026/050]  Loss: 0.3774 Acc:96.06%
Training: Epoch[027/050]  Loss: 0.3570 Acc:96.29%
[[1113.   55.]
 [ 321.  531.]]
0.8138613861386138
Training: Epoch[028/050]  Loss: 0.3156 Acc:96.76%
Training: Epoch[029/050]  Loss: 0.2758 Acc:97.57%
[[1134.   34.]
 [ 429.  423.]]
0.7707920792079208
Training: Epoch[030/050]  Loss: 0.2577 Acc:97.68%
Training: Epoch[031/050]  Loss: 0.2563 Acc:97.68%
[[1140.   28.]
 [ 480.  372.]]
0.7485148514851485
Training: Epoch[032/050]  Loss: 0.2562 Acc:97.57%
Training: Epoch[033/050]  Loss: 0.2544 Acc:97.68%
[[1139.   29.]
 [ 477.  375.]]
0.7495049504950495
Training: Epoch[034/050]  Loss: 0.2523 Acc:97.68%
Training: Epoch[035/050]  Loss: 0.2497 Acc:97.80%
[[1136.   32.]
 [ 452.  400.]]
0.7603960396039604
Training: Epoch[036/050]  Loss: 0.2548 Acc:97.80%
Training: Epoch[037/050]  Loss: 0.3981 Acc:95.37%
[[1060.  108.]
 [ 205.  647.]]
0.845049504950495
Training: Epoch[038/050]  Loss: 0.4904 Acc:94.44%
Training: Epoch[039/050]  Loss: 0.3588 Acc:96.41%
[[1131.   37.]
 [ 396.  456.]]
0.7856435643564357
Training: Epoch[040/050]  Loss: 0.2610 Acc:97.68%
Training: Epoch[041/050]  Loss: 0.2526 Acc:97.68%
[[1140.   28.]
 [ 482.  370.]]
0.7475247524752475
Training: Epoch[042/050]  Loss: 0.2515 Acc:97.68%
Training: Epoch[043/050]  Loss: 0.2498 Acc:97.68%
[[1140.   28.]
 [ 479.  373.]]
0.749009900990099
Training: Epoch[044/050]  Loss: 0.2485 Acc:97.68%
Training: Epoch[045/050]  Loss: 0.2472 Acc:97.80%
[[1140.   28.]
 [ 476.  376.]]
0.7504950495049505
Training: Epoch[046/050]  Loss: 0.2459 Acc:97.80%
Training: Epoch[047/050]  Loss: 0.2444 Acc:97.80%
[[1139.   29.]
 [ 472.  380.]]
0.751980198019802
Training: Epoch[048/050]  Loss: 0.2429 Acc:97.91%
Training: Epoch[049/050]  Loss: 0.2629 Acc:97.45%
[[1108.   60.]
 [ 302.  550.]]
0.8207920792079207
Training: Epoch[050/050]  Loss: 0.3715 Acc:95.71%
training 1D-CNN classifier for class5
Training: Epoch[001/050]  Loss: 3.3921 Acc:63.42%
[[   0. 1974.]
 [   0.  860.]]
0.30345800988002825
Training: Epoch[002/050]  Loss: 3.3843 Acc:85.06%
Training: Epoch[003/050]  Loss: 2.8775 Acc:85.06%
[[   0. 1974.]
 [   0.  860.]]
0.30345800988002825
Training: Epoch[004/050]  Loss: 2.6612 Acc:85.06%
Training: Epoch[005/050]  Loss: 2.5349 Acc:85.06%
[[   0. 1974.]
 [   0.  860.]]
0.30345800988002825
Training: Epoch[006/050]  Loss: 2.5004 Acc:85.06%
Training: Epoch[007/050]  Loss: 2.3669 Acc:85.06%
[[   0. 1974.]
 [   0.  860.]]
0.30345800988002825
Training: Epoch[008/050]  Loss: 2.2904 Acc:85.06%
Training: Epoch[009/050]  Loss: 2.2183 Acc:85.06%
[[   0. 1974.]
 [   0.  860.]]
0.30345800988002825
Training: Epoch[010/050]  Loss: 2.1749 Acc:85.06%
Training: Epoch[011/050]  Loss: 2.0539 Acc:85.06%
[[   0. 1974.]
 [   0.  860.]]
0.30345800988002825
Training: Epoch[012/050]  Loss: 2.0156 Acc:85.06%
Training: Epoch[013/050]  Loss: 1.8572 Acc:85.06%
[[   0. 1974.]
 [   0.  860.]]
0.30345800988002825
Training: Epoch[014/050]  Loss: 1.7895 Acc:85.06%
Training: Epoch[015/050]  Loss: 1.6257 Acc:85.06%
[[   0. 1974.]
 [   0.  860.]]
0.30345800988002825
Training: Epoch[016/050]  Loss: 1.7959 Acc:84.97%
Training: Epoch[017/050]  Loss: 1.4184 Acc:84.88%
[[ 247. 1727.]
 [   0.  860.]]
0.3906139731827805
Training: Epoch[018/050]  Loss: 1.3628 Acc:84.44%
Training: Epoch[019/050]  Loss: 1.1653 Acc:87.92%
[[ 953. 1021.]
 [  46.  814.]]
0.623500352858151
Training: Epoch[020/050]  Loss: 1.0562 Acc:91.14%
Training: Epoch[021/050]  Loss: 0.9676 Acc:91.86%
[[1193.  781.]
 [  83.  777.]]
0.6951305575158786
Training: Epoch[022/050]  Loss: 0.8723 Acc:93.29%
Training: Epoch[023/050]  Loss: 0.8022 Acc:93.65%
[[1328.  646.]
 [ 123.  737.]]
0.728652081863091
Training: Epoch[024/050]  Loss: 0.7436 Acc:94.10%
Training: Epoch[025/050]  Loss: 0.6945 Acc:94.36%
[[1438.  536.]
 [ 169.  691.]]
0.7512350035285815
Training: Epoch[026/050]  Loss: 0.6462 Acc:94.63%
Training: Epoch[027/050]  Loss: 0.6124 Acc:94.90%
[[1491.  483.]
 [ 200.  660.]]
0.7589978828510938
Training: Epoch[028/050]  Loss: 0.5895 Acc:95.17%
Training: Epoch[029/050]  Loss: 0.5732 Acc:95.08%
[[1519.  455.]
 [ 223.  637.]]
0.7607621736062103
Training: Epoch[030/050]  Loss: 0.5623 Acc:94.99%
Training: Epoch[031/050]  Loss: 0.5619 Acc:95.17%
[[1519.  455.]
 [ 223.  637.]]
0.7607621736062103
Training: Epoch[032/050]  Loss: 0.5668 Acc:95.17%
Training: Epoch[033/050]  Loss: 0.5830 Acc:94.90%
[[1486.  488.]
 [ 200.  660.]]
0.7572335920959774
Training: Epoch[034/050]  Loss: 0.6087 Acc:94.99%
Training: Epoch[035/050]  Loss: 0.6187 Acc:94.99%
[[1473.  501.]
 [ 194.  666.]]
0.7547635850388144
Training: Epoch[036/050]  Loss: 0.6060 Acc:95.08%
Training: Epoch[037/050]  Loss: 0.5611 Acc:95.17%
[[1552.  422.]
 [ 237.  623.]]
0.7674664784756527
Training: Epoch[038/050]  Loss: 0.5207 Acc:95.26%
Training: Epoch[039/050]  Loss: 0.4936 Acc:95.89%
[[1634.  340.]
 [ 282.  578.]]
0.7805222300635145
Training: Epoch[040/050]  Loss: 0.4852 Acc:95.89%
Training: Epoch[041/050]  Loss: 0.4830 Acc:95.97%
[[1601.  373.]
 [ 264.  596.]]
0.7752293577981652
Training: Epoch[042/050]  Loss: 0.4855 Acc:95.97%
Training: Epoch[043/050]  Loss: 0.5222 Acc:95.71%
[[1479.  495.]
 [ 197.  663.]]
0.7558221594918842
Training: Epoch[044/050]  Loss: 0.6002 Acc:95.08%
Training: Epoch[045/050]  Loss: 0.5960 Acc:95.08%
[[1502.  472.]
 [ 208.  652.]]
0.7600564573041637
Training: Epoch[046/050]  Loss: 0.5506 Acc:95.26%
Training: Epoch[047/050]  Loss: 0.4938 Acc:95.89%
[[1629.  345.]
 [ 281.  579.]]
0.7791107974594214
Training: Epoch[048/050]  Loss: 0.4735 Acc:95.89%
Training: Epoch[049/050]  Loss: 0.4700 Acc:95.89%
[[1604.  370.]
 [ 269.  591.]]
0.7745236414961185
Training: Epoch[050/050]  Loss: 0.4670 Acc:95.97%
Training: Epoch[001/050]  Loss: 4.4865 Acc:63.15%
[[   0. 1974.]
 [   0.  860.]]
0.30345800988002825
Training: Epoch[002/050]  Loss: 2.7918 Acc:85.06%
Training: Epoch[003/050]  Loss: 2.6786 Acc:85.06%
[[   0. 1974.]
 [   0.  860.]]
0.30345800988002825
Training: Epoch[004/050]  Loss: 2.6159 Acc:85.06%
Training: Epoch[005/050]  Loss: 2.5498 Acc:85.06%
[[   0. 1974.]
 [   0.  860.]]
0.30345800988002825
Training: Epoch[006/050]  Loss: 2.5175 Acc:85.06%
Training: Epoch[007/050]  Loss: 2.4658 Acc:85.06%
[[   0. 1974.]
 [   0.  860.]]
0.30345800988002825
Training: Epoch[008/050]  Loss: 2.4841 Acc:85.06%
Training: Epoch[009/050]  Loss: 2.3727 Acc:85.06%
[[   0. 1974.]
 [   0.  860.]]
0.30345800988002825
Training: Epoch[010/050]  Loss: 2.3067 Acc:85.06%
Training: Epoch[011/050]  Loss: 2.1523 Acc:85.06%
[[   0. 1974.]
 [   0.  860.]]
0.30345800988002825
Training: Epoch[012/050]  Loss: 2.9195 Acc:85.06%
Training: Epoch[013/050]  Loss: 2.3027 Acc:85.06%
[[   0. 1974.]
 [   0.  860.]]
0.30345800988002825
Training: Epoch[014/050]  Loss: 2.0995 Acc:85.06%
Training: Epoch[015/050]  Loss: 2.0622 Acc:85.06%
[[   0. 1974.]
 [   0.  860.]]
0.30345800988002825
Training: Epoch[016/050]  Loss: 1.7507 Acc:85.06%
Training: Epoch[017/050]  Loss: 1.7087 Acc:84.70%
[[   0. 1974.]
 [   0.  860.]]
0.30345800988002825
Training: Epoch[018/050]  Loss: 1.4849 Acc:84.70%
Training: Epoch[019/050]  Loss: 1.3790 Acc:85.42%
[[ 325. 1649.]
 [   3.  857.]]
0.4170783345095272
Training: Epoch[020/050]  Loss: 1.1913 Acc:89.27%
Training: Epoch[021/050]  Loss: 1.0911 Acc:91.14%
[[ 784. 1190.]
 [  28.  832.]]
0.5702187720536345
Training: Epoch[022/050]  Loss: 0.9695 Acc:92.31%
Training: Epoch[023/050]  Loss: 0.8790 Acc:92.93%
[[1007.  967.]
 [  54.  806.]]
0.6397318278052223
Training: Epoch[024/050]  Loss: 0.8048 Acc:93.83%
Training: Epoch[025/050]  Loss: 0.7430 Acc:94.28%
[[1161.  813.]
 [  85.  775.]]
0.6831333803810868
Training: Epoch[026/050]  Loss: 0.6946 Acc:94.45%
Training: Epoch[027/050]  Loss: 0.6585 Acc:94.81%
[[1269.  705.]
 [ 105.  755.]]
0.7141848976711362
Training: Epoch[028/050]  Loss: 0.6312 Acc:94.90%
Training: Epoch[029/050]  Loss: 0.6109 Acc:95.17%
[[1336.  638.]
 [ 126.  734.]]
0.7304163726182075
Training: Epoch[030/050]  Loss: 0.5944 Acc:95.17%
Training: Epoch[031/050]  Loss: 0.5861 Acc:95.08%
[[1366.  608.]
 [ 144.  716.]]
0.7346506704304869
Training: Epoch[032/050]  Loss: 0.5782 Acc:95.17%
Training: Epoch[033/050]  Loss: 0.5717 Acc:95.17%
[[1388.  586.]
 [ 151.  709.]]
0.7399435426958363
Training: Epoch[034/050]  Loss: 0.5661 Acc:95.08%
Training: Epoch[035/050]  Loss: 0.5610 Acc:95.08%
[[1412.  562.]
 [ 163.  697.]]
0.7441778405081158
Training: Epoch[036/050]  Loss: 0.5576 Acc:95.17%
Training: Epoch[037/050]  Loss: 0.5544 Acc:95.17%
[[1422.  552.]
 [ 168.  692.]]
0.7459421312632322
Training: Epoch[038/050]  Loss: 0.5521 Acc:95.26%
Training: Epoch[039/050]  Loss: 0.5495 Acc:95.26%
[[1427.  547.]
 [ 169.  691.]]
0.7473535638673253
Training: Epoch[040/050]  Loss: 0.5507 Acc:95.26%
Training: Epoch[041/050]  Loss: 0.5453 Acc:95.44%
[[1443.  531.]
 [ 179.  681.]]
0.7494707127734651
Training: Epoch[042/050]  Loss: 0.5335 Acc:95.44%
Training: Epoch[043/050]  Loss: 0.5235 Acc:95.53%
[[1471.  503.]
 [ 191.  669.]]
0.7551164431898377
Training: Epoch[044/050]  Loss: 0.5138 Acc:95.53%
Training: Epoch[045/050]  Loss: 0.5069 Acc:95.71%
[[1485.  489.]
 [ 200.  660.]]
0.7568807339449541
Training: Epoch[046/050]  Loss: 0.5034 Acc:95.71%
Training: Epoch[047/050]  Loss: 0.5063 Acc:95.62%
[[1475.  499.]
 [ 191.  669.]]
0.7565278757939309
Training: Epoch[048/050]  Loss: 0.5164 Acc:95.71%
Training: Epoch[049/050]  Loss: 0.5361 Acc:95.44%
[[1432.  542.]
 [ 169.  691.]]
0.7491178546224417
Training: Epoch[050/050]  Loss: 0.5498 Acc:95.53%
training 1D-CNN classifier for class6
Training: Epoch[001/050]  Loss: 4.3540 Acc:49.60%
[[   0.  812.]
 [   0. 1289.]]
0.6135173726796763
Training: Epoch[002/050]  Loss: 2.8815 Acc:78.53%
Training: Epoch[003/050]  Loss: 2.3971 Acc:78.53%
[[   0.  812.]
 [   0. 1289.]]
0.6135173726796763
Training: Epoch[004/050]  Loss: 2.5636 Acc:78.53%
Training: Epoch[005/050]  Loss: 2.2678 Acc:78.53%
[[   0.  812.]
 [   0. 1289.]]
0.6135173726796763
Training: Epoch[006/050]  Loss: 2.1078 Acc:78.53%
Training: Epoch[007/050]  Loss: 1.9827 Acc:78.53%
[[   0.  812.]
 [   0. 1289.]]
0.6135173726796763
Training: Epoch[008/050]  Loss: 1.8440 Acc:78.53%
Training: Epoch[009/050]  Loss: 1.6749 Acc:78.53%
[[   2.  810.]
 [   0. 1289.]]
0.6144693003331747
Training: Epoch[010/050]  Loss: 1.4940 Acc:78.42%
Training: Epoch[011/050]  Loss: 1.2813 Acc:79.91%
[[ 537.  275.]
 [  47. 1242.]]
0.8467396477867682
Training: Epoch[012/050]  Loss: 1.0583 Acc:88.52%
Training: Epoch[013/050]  Loss: 0.8460 Acc:92.42%
[[ 663.  149.]
 [ 133. 1156.]]
0.8657782008567348
Training: Epoch[014/050]  Loss: 0.6423 Acc:95.18%
Training: Epoch[015/050]  Loss: 0.4922 Acc:96.33%
[[ 721.   91.]
 [ 205. 1084.]]
0.8591147072822466
Training: Epoch[016/050]  Loss: 0.3884 Acc:96.90%
Training: Epoch[017/050]  Loss: 0.3230 Acc:97.24%
[[ 745.   67.]
 [ 264. 1025.]]
0.8424559733460257
Training: Epoch[018/050]  Loss: 0.2833 Acc:97.36%
Training: Epoch[019/050]  Loss: 0.2592 Acc:97.36%
[[760.  52.]
 [300. 989.]]
0.8324607329842932
Training: Epoch[020/050]  Loss: 0.2444 Acc:97.59%
Training: Epoch[021/050]  Loss: 0.2359 Acc:97.59%
[[764.  48.]
 [311. 978.]]
0.829128986197049
Training: Epoch[022/050]  Loss: 0.2324 Acc:97.59%
Training: Epoch[023/050]  Loss: 0.2331 Acc:97.59%
[[764.  48.]
 [307. 982.]]
0.8310328415040457
Training: Epoch[024/050]  Loss: 0.2410 Acc:97.59%
Training: Epoch[025/050]  Loss: 0.2612 Acc:97.13%
[[ 745.   67.]
 [ 257. 1032.]]
0.8457877201332699
Training: Epoch[026/050]  Loss: 0.2971 Acc:96.79%
Training: Epoch[027/050]  Loss: 0.3193 Acc:96.56%
[[ 742.   70.]
 [ 244. 1045.]]
0.8505473584007616
Training: Epoch[028/050]  Loss: 0.2719 Acc:97.24%
Training: Epoch[029/050]  Loss: 0.2052 Acc:97.70%
[[777.  35.]
 [377. 912.]]
0.8039029033793431
Training: Epoch[030/050]  Loss: 0.1945 Acc:98.05%
Training: Epoch[031/050]  Loss: 0.1995 Acc:97.93%
[[777.  35.]
 [375. 914.]]
0.8048548310328415
Training: Epoch[032/050]  Loss: 0.1937 Acc:98.05%
Training: Epoch[033/050]  Loss: 0.1967 Acc:97.93%
[[777.  35.]
 [374. 915.]]
0.8053307948595907
Training: Epoch[034/050]  Loss: 0.1951 Acc:98.16%
Training: Epoch[035/050]  Loss: 0.1977 Acc:98.05%
[[775.  37.]
 [356. 933.]]
0.8129462160875773
Training: Epoch[036/050]  Loss: 0.2094 Acc:97.82%
Training: Epoch[037/050]  Loss: 0.2652 Acc:96.90%
[[ 739.   73.]
 [ 232. 1057.]]
0.8548310328415041
Training: Epoch[038/050]  Loss: 0.3493 Acc:96.10%
Training: Epoch[039/050]  Loss: 0.3018 Acc:96.56%
[[765.  47.]
 [330. 959.]]
0.820561637315564
Training: Epoch[040/050]  Loss: 0.1985 Acc:97.82%
Training: Epoch[041/050]  Loss: 0.1912 Acc:98.05%
[[768.  44.]
 [337. 952.]]
0.8186577820085673
Training: Epoch[042/050]  Loss: 0.1963 Acc:97.82%
Training: Epoch[043/050]  Loss: 0.1885 Acc:98.05%
[[773.  39.]
 [346. 943.]]
0.8167539267015707
Training: Epoch[044/050]  Loss: 0.1946 Acc:97.82%
Training: Epoch[045/050]  Loss: 0.1864 Acc:98.05%
[[776.  36.]
 [366. 923.]]
0.8086625416468348
Training: Epoch[046/050]  Loss: 0.1922 Acc:97.93%
Training: Epoch[047/050]  Loss: 0.1858 Acc:98.05%
[[780.  32.]
 [394. 895.]]
0.7972394098048549
Training: Epoch[048/050]  Loss: 0.1874 Acc:98.05%
Training: Epoch[049/050]  Loss: 0.1894 Acc:98.05%
[[778.  34.]
 [377. 912.]]
0.8043788672060923
Training: Epoch[050/050]  Loss: 0.2002 Acc:98.05%
Training: Epoch[001/050]  Loss: 3.6148 Acc:60.39%
[[   0.  812.]
 [   0. 1289.]]
0.6135173726796763
Training: Epoch[002/050]  Loss: 2.5405 Acc:78.53%
Training: Epoch[003/050]  Loss: 2.5396 Acc:78.53%
[[   0.  812.]
 [   0. 1289.]]
0.6135173726796763
Training: Epoch[004/050]  Loss: 2.4356 Acc:78.53%
Training: Epoch[005/050]  Loss: 2.3192 Acc:78.53%
[[   0.  812.]
 [   0. 1289.]]
0.6135173726796763
Training: Epoch[006/050]  Loss: 2.1952 Acc:78.53%
Training: Epoch[007/050]  Loss: 2.0378 Acc:78.53%
[[   0.  812.]
 [   0. 1289.]]
0.6135173726796763
Training: Epoch[008/050]  Loss: 1.8573 Acc:78.53%
Training: Epoch[009/050]  Loss: 1.6297 Acc:78.53%
[[ 235.  577.]
 [   6. 1283.]]
0.7225130890052356
Training: Epoch[010/050]  Loss: 1.3731 Acc:79.68%
Training: Epoch[011/050]  Loss: 1.1207 Acc:88.29%
[[ 553.  259.]
 [  53. 1236.]]
0.8514992860542598
Training: Epoch[012/050]  Loss: 0.8440 Acc:92.88%
Training: Epoch[013/050]  Loss: 0.6475 Acc:95.41%
[[ 654.  158.]
 [ 115. 1174.]]
0.8700618752974774
Training: Epoch[014/050]  Loss: 0.5106 Acc:96.21%
Training: Epoch[015/050]  Loss: 0.4122 Acc:96.90%
[[ 694.  118.]
 [ 178. 1111.]]
0.8591147072822466
Training: Epoch[016/050]  Loss: 0.3469 Acc:97.01%
Training: Epoch[017/050]  Loss: 0.3044 Acc:97.13%
[[ 719.   93.]
 [ 204. 1085.]]
0.8586387434554974
Training: Epoch[018/050]  Loss: 0.2760 Acc:97.36%
Training: Epoch[019/050]  Loss: 0.2566 Acc:97.36%
[[ 727.   85.]
 [ 224. 1065.]]
0.8529271775345074
Training: Epoch[020/050]  Loss: 0.2432 Acc:97.59%
Training: Epoch[021/050]  Loss: 0.2343 Acc:97.82%
[[ 732.   80.]
 [ 232. 1057.]]
0.8514992860542598
Training: Epoch[022/050]  Loss: 0.2287 Acc:97.82%
Training: Epoch[023/050]  Loss: 0.2257 Acc:97.82%
[[ 734.   78.]
 [ 235. 1054.]]
0.8510233222275108
Training: Epoch[024/050]  Loss: 0.2249 Acc:97.82%
Training: Epoch[025/050]  Loss: 0.2261 Acc:97.82%
[[ 732.   80.]
 [ 229. 1060.]]
0.8529271775345074
Training: Epoch[026/050]  Loss: 0.2294 Acc:97.82%
Training: Epoch[027/050]  Loss: 0.2336 Acc:97.82%
[[ 729.   83.]
 [ 226. 1063.]]
0.8529271775345074
Training: Epoch[028/050]  Loss: 0.2367 Acc:97.59%
Training: Epoch[029/050]  Loss: 0.2339 Acc:97.70%
[[ 732.   80.]
 [ 230. 1059.]]
0.8524512137077582
Training: Epoch[030/050]  Loss: 0.2215 Acc:97.82%
Training: Epoch[031/050]  Loss: 0.2023 Acc:98.05%
[[ 757.   55.]
 [ 289. 1000.]]
0.8362684435982866
Training: Epoch[032/050]  Loss: 0.1907 Acc:97.93%
Training: Epoch[033/050]  Loss: 0.1907 Acc:97.93%
[[ 758.   54.]
 [ 288. 1001.]]
0.8372203712517848
Training: Epoch[034/050]  Loss: 0.1902 Acc:97.93%
Training: Epoch[035/050]  Loss: 0.1926 Acc:98.05%
[[ 751.   61.]
 [ 263. 1026.]]
0.8457877201332699
Training: Epoch[036/050]  Loss: 0.2088 Acc:98.05%
Training: Epoch[037/050]  Loss: 0.2797 Acc:96.79%
[[ 699.  113.]
 [ 183. 1106.]]
0.8591147072822466
Training: Epoch[038/050]  Loss: 0.3357 Acc:96.10%
Training: Epoch[039/050]  Loss: 0.2535 Acc:97.36%
[[764.  48.]
 [304. 985.]]
0.8324607329842932
Training: Epoch[040/050]  Loss: 0.1886 Acc:97.93%
Training: Epoch[041/050]  Loss: 0.1893 Acc:98.05%
[[765.  47.]
 [305. 984.]]
0.8324607329842932
Training: Epoch[042/050]  Loss: 0.1857 Acc:97.93%
Training: Epoch[043/050]  Loss: 0.1876 Acc:98.05%
[[765.  47.]
 [309. 980.]]
0.8305568776772966
Training: Epoch[044/050]  Loss: 0.1830 Acc:98.05%
Training: Epoch[045/050]  Loss: 0.1867 Acc:98.05%
[[764.  48.]
 [308. 981.]]
0.8305568776772966
Training: Epoch[046/050]  Loss: 0.1807 Acc:98.05%
Training: Epoch[047/050]  Loss: 0.1840 Acc:98.05%
[[ 761.   51.]
 [ 287. 1002.]]
0.8391242265587815
Training: Epoch[048/050]  Loss: 0.1824 Acc:98.16%
Training: Epoch[049/050]  Loss: 0.1865 Acc:98.05%
[[ 752.   60.]
 [ 262. 1027.]]
0.8467396477867682
Training: Epoch[050/050]  Loss: 0.2065 Acc:98.16%
training 1D-CNN classifier for class7
Training: Epoch[001/050]  Loss: 4.0107 Acc:67.65%
[[   0. 1119.]
 [   0. 1517.]]
0.5754931714719271
Training: Epoch[002/050]  Loss: 2.5179 Acc:85.29%
Training: Epoch[003/050]  Loss: 2.4156 Acc:85.29%
[[   0. 1119.]
 [   0. 1517.]]
0.5754931714719271
Training: Epoch[004/050]  Loss: 2.3263 Acc:85.29%
Training: Epoch[005/050]  Loss: 2.2526 Acc:85.29%
[[   0. 1119.]
 [   0. 1517.]]
0.5754931714719271
Training: Epoch[006/050]  Loss: 2.1622 Acc:85.29%
Training: Epoch[007/050]  Loss: 2.0664 Acc:85.29%
[[   0. 1119.]
 [   0. 1517.]]
0.5754931714719271
Training: Epoch[008/050]  Loss: 1.9488 Acc:85.29%
Training: Epoch[009/050]  Loss: 1.8106 Acc:85.29%
[[   0. 1119.]
 [   0. 1517.]]
0.5754931714719271
Training: Epoch[010/050]  Loss: 1.6142 Acc:85.29%
Training: Epoch[011/050]  Loss: 1.4085 Acc:85.29%
[[ 258.  861.]
 [   2. 1515.]]
0.6726100151745068
Training: Epoch[012/050]  Loss: 1.1769 Acc:85.29%
Training: Epoch[013/050]  Loss: 1.0337 Acc:89.56%
[[ 578.  541.]
 [  44. 1473.]]
0.7780728376327769
Training: Epoch[014/050]  Loss: 0.7279 Acc:94.69%
Training: Epoch[015/050]  Loss: 0.5932 Acc:96.02%
[[ 721.  398.]
 [  83. 1434.]]
0.81752655538695
Training: Epoch[016/050]  Loss: 0.4557 Acc:96.87%
Training: Epoch[017/050]  Loss: 0.3752 Acc:97.15%
[[ 792.  327.]
 [ 126. 1391.]]
0.8281487101669196
Training: Epoch[018/050]  Loss: 0.3274 Acc:97.82%
Training: Epoch[019/050]  Loss: 0.2956 Acc:97.91%
[[ 833.  286.]
 [ 159. 1358.]]
0.8311836115326252
Training: Epoch[020/050]  Loss: 0.2740 Acc:98.01%
Training: Epoch[021/050]  Loss: 0.2594 Acc:98.29%
[[ 853.  266.]
 [ 172. 1345.]]
0.8338391502276176
Training: Epoch[022/050]  Loss: 0.2496 Acc:98.29%
Training: Epoch[023/050]  Loss: 0.2439 Acc:98.39%
[[ 857.  262.]
 [ 177. 1340.]]
0.8334597875569044
Training: Epoch[024/050]  Loss: 0.2429 Acc:98.39%
Training: Epoch[025/050]  Loss: 0.2500 Acc:98.39%
[[ 839.  280.]
 [ 163. 1354.]]
0.8319423368740516
Training: Epoch[026/050]  Loss: 0.2851 Acc:98.39%
Training: Epoch[027/050]  Loss: 0.3546 Acc:97.72%
[[ 769.  350.]
 [ 115. 1402.]]
0.8235963581183612
Training: Epoch[028/050]  Loss: 0.3463 Acc:97.82%
Training: Epoch[029/050]  Loss: 0.2496 Acc:98.10%
[[ 891.  228.]
 [ 207. 1310.]]
0.8349772382397572
Training: Epoch[030/050]  Loss: 0.2206 Acc:98.67%
Training: Epoch[031/050]  Loss: 0.2175 Acc:98.67%
[[ 892.  227.]
 [ 208. 1309.]]
0.8349772382397572
Training: Epoch[032/050]  Loss: 0.2202 Acc:98.67%
Training: Epoch[033/050]  Loss: 0.2168 Acc:98.67%
[[ 893.  226.]
 [ 208. 1309.]]
0.8353566009104704
Training: Epoch[034/050]  Loss: 0.2204 Acc:98.67%
Training: Epoch[035/050]  Loss: 0.2167 Acc:98.67%
[[ 888.  231.]
 [ 206. 1311.]]
0.8342185128983308
Training: Epoch[036/050]  Loss: 0.2178 Acc:98.67%
Training: Epoch[037/050]  Loss: 0.2149 Acc:98.67%
[[ 888.  231.]
 [ 206. 1311.]]
0.8342185128983308
Training: Epoch[038/050]  Loss: 0.2175 Acc:98.67%
Training: Epoch[039/050]  Loss: 0.2123 Acc:98.67%
[[ 888.  231.]
 [ 206. 1311.]]
0.8342185128983308
Training: Epoch[040/050]  Loss: 0.2195 Acc:98.67%
Training: Epoch[041/050]  Loss: 0.2129 Acc:98.67%
[[ 877.  242.]
 [ 195. 1322.]]
0.8342185128983308
Training: Epoch[042/050]  Loss: 0.2095 Acc:98.67%
Training: Epoch[043/050]  Loss: 0.2279 Acc:98.77%
[[ 856.  263.]
 [ 180. 1337.]]
0.8319423368740516
Training: Epoch[044/050]  Loss: 0.2611 Acc:98.77%
Training: Epoch[045/050]  Loss: 0.4059 Acc:96.96%
[[ 758.  361.]
 [ 103. 1414.]]
0.8239757207890743
Training: Epoch[046/050]  Loss: 0.3381 Acc:97.82%
Training: Epoch[047/050]  Loss: 0.2149 Acc:98.58%
[[ 894.  225.]
 [ 210. 1307.]]
0.8349772382397572
Training: Epoch[048/050]  Loss: 0.2132 Acc:98.77%
Training: Epoch[049/050]  Loss: 0.2085 Acc:98.67%
[[ 890.  229.]
 [ 206. 1311.]]
0.8349772382397572
Training: Epoch[050/050]  Loss: 0.2106 Acc:98.77%
Training: Epoch[001/050]  Loss: 3.9066 Acc:61.39%
[[   0. 1119.]
 [   0. 1517.]]
0.5754931714719271
Training: Epoch[002/050]  Loss: 2.8902 Acc:85.29%
Training: Epoch[003/050]  Loss: 2.4919 Acc:85.29%
[[   0. 1119.]
 [   0. 1517.]]
0.5754931714719271
Training: Epoch[004/050]  Loss: 2.2740 Acc:85.29%
Training: Epoch[005/050]  Loss: 2.1046 Acc:85.29%
[[   0. 1119.]
 [   0. 1517.]]
0.5754931714719271
Training: Epoch[006/050]  Loss: 1.9348 Acc:85.29%
Training: Epoch[007/050]  Loss: 1.7221 Acc:85.29%
[[   0. 1119.]
 [   0. 1517.]]
0.5754931714719271
Training: Epoch[008/050]  Loss: 1.5632 Acc:85.29%
Training: Epoch[009/050]  Loss: 1.2066 Acc:85.29%
[[ 480.  639.]
 [  24. 1493.]]
0.7484825493171472
Training: Epoch[010/050]  Loss: 0.9190 Acc:91.37%
Training: Epoch[011/050]  Loss: 0.7406 Acc:94.88%
[[ 716.  403.]
 [  79. 1438.]]
0.8171471927162367
Training: Epoch[012/050]  Loss: 0.4728 Acc:96.77%
Training: Epoch[013/050]  Loss: 0.3245 Acc:97.91%
[[ 845.  274.]
 [ 165. 1352.]]
0.8334597875569044
Training: Epoch[014/050]  Loss: 0.2711 Acc:98.20%
Training: Epoch[015/050]  Loss: 0.2458 Acc:98.48%
[[ 872.  247.]
 [ 186. 1331.]]
0.8357359635811836
Training: Epoch[016/050]  Loss: 0.2324 Acc:98.67%
Training: Epoch[017/050]  Loss: 0.2247 Acc:98.67%
[[ 887.  232.]
 [ 201. 1316.]]
0.8357359635811836
Training: Epoch[018/050]  Loss: 0.2201 Acc:98.67%
Training: Epoch[019/050]  Loss: 0.2173 Acc:98.67%
[[ 898.  221.]
 [ 207. 1310.]]
0.8376327769347496
Training: Epoch[020/050]  Loss: 0.2161 Acc:98.67%
Training: Epoch[021/050]  Loss: 0.2162 Acc:98.67%
[[ 900.  219.]
 [ 211. 1306.]]
0.8368740515933232
Training: Epoch[022/050]  Loss: 0.2166 Acc:98.67%
Training: Epoch[023/050]  Loss: 0.2169 Acc:98.67%
[[ 898.  221.]
 [ 210. 1307.]]
0.83649468892261
Training: Epoch[024/050]  Loss: 0.2169 Acc:98.67%
Training: Epoch[025/050]  Loss: 0.2166 Acc:98.67%
[[ 897.  222.]
 [ 207. 1310.]]
0.8372534142640364
Training: Epoch[026/050]  Loss: 0.2171 Acc:98.67%
Training: Epoch[027/050]  Loss: 0.2133 Acc:98.67%
[[ 903.  216.]
 [ 210. 1307.]]
0.838391502276176
Training: Epoch[028/050]  Loss: 0.2266 Acc:98.77%
Training: Epoch[029/050]  Loss: 0.3592 Acc:97.63%
[[ 702.  417.]
 [  74. 1443.]]
0.813732928679818
Training: Epoch[030/050]  Loss: 0.4637 Acc:96.20%
Training: Epoch[031/050]  Loss: 0.2236 Acc:98.39%
[[ 899.  220.]
 [ 208. 1309.]]
0.8376327769347496
Training: Epoch[032/050]  Loss: 0.2126 Acc:98.67%
Training: Epoch[033/050]  Loss: 0.2109 Acc:98.67%
[[ 897.  222.]
 [ 208. 1309.]]
0.8368740515933232
Training: Epoch[034/050]  Loss: 0.2103 Acc:98.77%
Training: Epoch[035/050]  Loss: 0.2085 Acc:98.67%
[[ 899.  220.]
 [ 208. 1309.]]
0.8376327769347496
Training: Epoch[036/050]  Loss: 0.2105 Acc:98.77%
Training: Epoch[037/050]  Loss: 0.2093 Acc:98.67%
[[ 897.  222.]
 [ 208. 1309.]]
0.8368740515933232
Training: Epoch[038/050]  Loss: 0.2100 Acc:98.77%
Training: Epoch[039/050]  Loss: 0.2073 Acc:98.67%
[[ 886.  233.]
 [ 202. 1315.]]
0.8349772382397572
Training: Epoch[040/050]  Loss: 0.2064 Acc:98.77%
Training: Epoch[041/050]  Loss: 0.2072 Acc:98.77%
[[ 882.  237.]
 [ 196. 1321.]]
0.8357359635811836
Training: Epoch[042/050]  Loss: 0.2027 Acc:98.67%
Training: Epoch[043/050]  Loss: 0.2137 Acc:98.67%
[[ 860.  259.]
 [ 181. 1336.]]
0.8330804248861912
Training: Epoch[044/050]  Loss: 0.2403 Acc:98.77%
Training: Epoch[045/050]  Loss: 0.3264 Acc:97.44%
[[ 779.  340.]
 [ 119. 1398.]]
0.8258725341426404
Training: Epoch[046/050]  Loss: 0.2852 Acc:98.01%
Training: Epoch[047/050]  Loss: 0.2042 Acc:98.67%
[[ 902.  217.]
 [ 210. 1307.]]
0.8380121396054628
Training: Epoch[048/050]  Loss: 0.2080 Acc:98.67%
Training: Epoch[049/050]  Loss: 0.2045 Acc:98.67%
[[ 900.  219.]
 [ 210. 1307.]]
0.8372534142640364
Training: Epoch[050/050]  Loss: 0.2065 Acc:98.67%
training 1D-CNN classifier for class8
Training: Epoch[001/050]  Loss: 3.6893 Acc:52.48%
[[   0.  748.]
 [   0. 1567.]]
0.676889848812095
Training: Epoch[002/050]  Loss: 2.6492 Acc:79.51%
Training: Epoch[003/050]  Loss: 2.4956 Acc:79.51%
[[   0.  748.]
 [   0. 1567.]]
0.676889848812095
Training: Epoch[004/050]  Loss: 2.3965 Acc:79.51%
Training: Epoch[005/050]  Loss: 2.3859 Acc:79.51%
[[   0.  748.]
 [   0. 1567.]]
0.676889848812095
Training: Epoch[006/050]  Loss: 2.9502 Acc:79.51%
Training: Epoch[007/050]  Loss: 2.0382 Acc:79.51%
[[   0.  748.]
 [   0. 1567.]]
0.676889848812095
Training: Epoch[008/050]  Loss: 2.0681 Acc:79.51%
Training: Epoch[009/050]  Loss: 1.5737 Acc:79.09%
[[ 364.  384.]
 [   8. 1559.]]
0.8306695464362851
Training: Epoch[010/050]  Loss: 1.0539 Acc:89.76%
Training: Epoch[011/050]  Loss: 0.6689 Acc:96.41%
[[ 541.  207.]
 [  58. 1509.]]
0.8855291576673866
Training: Epoch[012/050]  Loss: 0.4324 Acc:97.78%
Training: Epoch[013/050]  Loss: 0.2857 Acc:99.47%
[[ 599.  149.]
 [  95. 1472.]]
0.8946004319654428
Training: Epoch[014/050]  Loss: 0.2033 Acc:99.58%
Training: Epoch[015/050]  Loss: 0.1577 Acc:99.58%
[[ 616.  132.]
 [ 121. 1446.]]
0.8907127429805616
Training: Epoch[016/050]  Loss: 0.1297 Acc:99.58%
Training: Epoch[017/050]  Loss: 0.1111 Acc:99.58%
[[ 629.  119.]
 [ 130. 1437.]]
0.8924406047516199
Training: Epoch[018/050]  Loss: 0.0979 Acc:99.58%
Training: Epoch[019/050]  Loss: 0.0883 Acc:99.58%
[[ 632.  116.]
 [ 142. 1425.]]
0.8885529157667387
Training: Epoch[020/050]  Loss: 0.0809 Acc:99.58%
Training: Epoch[021/050]  Loss: 0.0753 Acc:99.58%
[[ 634.  114.]
 [ 151. 1416.]]
0.8855291576673866
Training: Epoch[022/050]  Loss: 0.0709 Acc:99.58%
Training: Epoch[023/050]  Loss: 0.0676 Acc:99.58%
[[ 638.  110.]
 [ 152. 1415.]]
0.8868250539956803
Training: Epoch[024/050]  Loss: 0.0652 Acc:99.58%
Training: Epoch[025/050]  Loss: 0.0638 Acc:99.58%
[[ 636.  112.]
 [ 152. 1415.]]
0.8859611231101512
Training: Epoch[026/050]  Loss: 0.0636 Acc:99.58%
Training: Epoch[027/050]  Loss: 0.0653 Acc:99.58%
[[ 632.  116.]
 [ 138. 1429.]]
0.890280777537797
Training: Epoch[028/050]  Loss: 0.0700 Acc:99.58%
Training: Epoch[029/050]  Loss: 0.0795 Acc:99.58%
[[ 613.  135.]
 [ 117. 1450.]]
0.8911447084233262
Training: Epoch[030/050]  Loss: 0.0917 Acc:99.47%
Training: Epoch[031/050]  Loss: 0.0928 Acc:99.47%
[[ 621.  127.]
 [ 123. 1444.]]
0.8920086393088553
Training: Epoch[032/050]  Loss: 0.0669 Acc:99.58%
Training: Epoch[033/050]  Loss: 0.0429 Acc:99.68%
[[ 654.   94.]
 [ 173. 1394.]]
0.8846652267818574
Training: Epoch[034/050]  Loss: 0.0434 Acc:99.68%
Training: Epoch[035/050]  Loss: 0.0421 Acc:99.68%
[[ 652.   96.]
 [ 170. 1397.]]
0.885097192224622
Training: Epoch[036/050]  Loss: 0.0438 Acc:99.68%
Training: Epoch[037/050]  Loss: 0.0413 Acc:99.68%
[[ 648.  100.]
 [ 161. 1406.]]
0.8872570194384449
Training: Epoch[038/050]  Loss: 0.0475 Acc:99.68%
Training: Epoch[039/050]  Loss: 0.0409 Acc:99.68%
[[ 651.   97.]
 [ 167. 1400.]]
0.8859611231101512
Training: Epoch[040/050]  Loss: 0.0450 Acc:99.68%
Training: Epoch[041/050]  Loss: 0.0408 Acc:99.68%
[[ 657.   91.]
 [ 174. 1393.]]
0.8855291576673866
Training: Epoch[042/050]  Loss: 0.0420 Acc:99.68%
Training: Epoch[043/050]  Loss: 0.0403 Acc:99.68%
[[ 658.   90.]
 [ 175. 1392.]]
0.8855291576673866
Training: Epoch[044/050]  Loss: 0.0416 Acc:99.68%
Training: Epoch[045/050]  Loss: 0.0413 Acc:99.68%
[[ 658.   90.]
 [ 175. 1392.]]
0.8855291576673866
Training: Epoch[046/050]  Loss: 0.0416 Acc:99.68%
Training: Epoch[047/050]  Loss: 0.0438 Acc:99.68%
[[ 648.  100.]
 [ 160. 1407.]]
0.8876889848812095
Training: Epoch[048/050]  Loss: 0.0564 Acc:99.58%
Training: Epoch[049/050]  Loss: 0.0897 Acc:99.47%
[[ 598.  150.]
 [  95. 1472.]]
0.8941684665226782
Training: Epoch[050/050]  Loss: 0.1096 Acc:99.47%
Training: Epoch[001/050]  Loss: 2.9232 Acc:52.48%
[[   0.  748.]
 [   0. 1567.]]
0.676889848812095
Training: Epoch[002/050]  Loss: 2.5841 Acc:79.51%
Training: Epoch[003/050]  Loss: 2.6322 Acc:79.51%
[[   0.  748.]
 [   0. 1567.]]
0.676889848812095
Training: Epoch[004/050]  Loss: 2.5678 Acc:79.51%
Training: Epoch[005/050]  Loss: 2.4474 Acc:79.51%
[[   0.  748.]
 [   0. 1567.]]
0.676889848812095
Training: Epoch[006/050]  Loss: 2.3413 Acc:79.51%
Training: Epoch[007/050]  Loss: 2.2318 Acc:79.51%
[[   0.  748.]
 [   0. 1567.]]
0.676889848812095
Training: Epoch[008/050]  Loss: 2.1173 Acc:79.51%
Training: Epoch[009/050]  Loss: 2.0056 Acc:79.51%
[[   0.  748.]
 [   0. 1567.]]
0.676889848812095
Training: Epoch[010/050]  Loss: 1.7531 Acc:79.51%
Training: Epoch[011/050]  Loss: 1.5995 Acc:79.51%
[[   0.  748.]
 [   0. 1567.]]
0.676889848812095
Training: Epoch[012/050]  Loss: 1.2896 Acc:79.51%
Training: Epoch[013/050]  Loss: 1.2196 Acc:81.31%
[[ 426.  322.]
 [  18. 1549.]]
0.8531317494600432
Training: Epoch[014/050]  Loss: 0.6858 Acc:95.78%
Training: Epoch[015/050]  Loss: 0.4595 Acc:97.36%
[[ 522.  226.]
 [  55. 1512.]]
0.8786177105831533
Training: Epoch[016/050]  Loss: 0.2882 Acc:98.84%
Training: Epoch[017/050]  Loss: 0.1890 Acc:99.47%
[[ 569.  179.]
 [  76. 1491.]]
0.8898488120950324
Training: Epoch[018/050]  Loss: 0.1386 Acc:99.58%
Training: Epoch[019/050]  Loss: 0.1099 Acc:99.58%
[[ 592.  156.]
 [  93. 1474.]]
0.8924406047516199
Training: Epoch[020/050]  Loss: 0.0920 Acc:99.58%
Training: Epoch[021/050]  Loss: 0.0809 Acc:99.58%
[[ 603.  145.]
 [  99. 1468.]]
0.8946004319654428
Training: Epoch[022/050]  Loss: 0.0730 Acc:99.58%
Training: Epoch[023/050]  Loss: 0.0674 Acc:99.68%
[[ 607.  141.]
 [ 106. 1461.]]
0.8933045356371491
Training: Epoch[024/050]  Loss: 0.0632 Acc:99.68%
Training: Epoch[025/050]  Loss: 0.0600 Acc:99.68%
[[ 610.  138.]
 [ 109. 1458.]]
0.8933045356371491
Training: Epoch[026/050]  Loss: 0.0576 Acc:99.68%
Training: Epoch[027/050]  Loss: 0.0558 Acc:99.68%
[[ 615.  133.]
 [ 112. 1455.]]
0.8941684665226782
Training: Epoch[028/050]  Loss: 0.0545 Acc:99.68%
Training: Epoch[029/050]  Loss: 0.0535 Acc:99.68%
[[ 615.  133.]
 [ 114. 1453.]]
0.8933045356371491
Training: Epoch[030/050]  Loss: 0.0529 Acc:99.68%
Training: Epoch[031/050]  Loss: 0.0526 Acc:99.68%
[[ 615.  133.]
 [ 114. 1453.]]
0.8933045356371491
Training: Epoch[032/050]  Loss: 0.0527 Acc:99.68%
Training: Epoch[033/050]  Loss: 0.0530 Acc:99.68%
[[ 615.  133.]
 [ 114. 1453.]]
0.8933045356371491
Training: Epoch[034/050]  Loss: 0.0536 Acc:99.68%
Training: Epoch[035/050]  Loss: 0.0543 Acc:99.68%
[[ 615.  133.]
 [ 114. 1453.]]
0.8933045356371491
Training: Epoch[036/050]  Loss: 0.0547 Acc:99.68%
Training: Epoch[037/050]  Loss: 0.0539 Acc:99.68%
[[ 618.  130.]
 [ 116. 1451.]]
0.8937365010799136
Training: Epoch[038/050]  Loss: 0.0513 Acc:99.68%
Training: Epoch[039/050]  Loss: 0.0464 Acc:99.68%
[[ 629.  119.]
 [ 137. 1430.]]
0.8894168466522678
Training: Epoch[040/050]  Loss: 0.0424 Acc:99.68%
Training: Epoch[041/050]  Loss: 0.0432 Acc:99.68%
[[ 629.  119.]
 [ 137. 1430.]]
0.8894168466522678
Training: Epoch[042/050]  Loss: 0.0419 Acc:99.68%
Training: Epoch[043/050]  Loss: 0.0430 Acc:99.68%
[[ 629.  119.]
 [ 137. 1430.]]
0.8894168466522678
Training: Epoch[044/050]  Loss: 0.0418 Acc:99.68%
Training: Epoch[045/050]  Loss: 0.0424 Acc:99.68%
[[ 628.  120.]
 [ 136. 1431.]]
0.8894168466522678
Training: Epoch[046/050]  Loss: 0.0420 Acc:99.68%
Training: Epoch[047/050]  Loss: 0.0421 Acc:99.68%
[[ 626.  122.]
 [ 134. 1433.]]
0.8894168466522678
Training: Epoch[048/050]  Loss: 0.0423 Acc:99.68%
Training: Epoch[049/050]  Loss: 0.0425 Acc:99.68%
[[ 625.  123.]
 [ 132. 1435.]]
0.8898488120950324
Training: Epoch[050/050]  Loss: 0.0427 Acc:99.68%
training 1D-CNN classifier for class9
Training: Epoch[001/050]  Loss: 3.1010 Acc:56.77%
[[   0. 1540.]
 [   0.  865.]]
0.3596673596673597
Training: Epoch[002/050]  Loss: 2.3618 Acc:83.04%
Training: Epoch[003/050]  Loss: 2.2594 Acc:83.04%
[[   0. 1540.]
 [   0.  865.]]
0.3596673596673597
Training: Epoch[004/050]  Loss: 2.1303 Acc:83.04%
Training: Epoch[005/050]  Loss: 1.9869 Acc:83.04%
[[   0. 1540.]
 [   0.  865.]]
0.3596673596673597
Training: Epoch[006/050]  Loss: 1.9496 Acc:83.04%
Training: Epoch[007/050]  Loss: 1.6388 Acc:83.04%
[[   0. 1540.]
 [   0.  865.]]
0.3596673596673597
Training: Epoch[008/050]  Loss: 1.4664 Acc:83.04%
Training: Epoch[009/050]  Loss: 1.1955 Acc:83.04%
[[ 395. 1145.]
 [  12.  853.]]
0.518918918918919
Training: Epoch[010/050]  Loss: 0.9091 Acc:91.93%
Training: Epoch[011/050]  Loss: 0.6290 Acc:96.28%
[[1080.  460.]
 [  56.  809.]]
0.7854469854469854
Training: Epoch[012/050]  Loss: 0.4065 Acc:97.52%
Training: Epoch[013/050]  Loss: 0.2883 Acc:97.62%
[[1324.  216.]
 [ 114.  751.]]
0.8627858627858628
Training: Epoch[014/050]  Loss: 0.2345 Acc:98.24%
Training: Epoch[015/050]  Loss: 0.2078 Acc:98.35%
[[1368.  172.]
 [ 145.  720.]]
0.8681912681912682
Training: Epoch[016/050]  Loss: 0.1917 Acc:98.55%
Training: Epoch[017/050]  Loss: 0.1816 Acc:98.55%
[[1390.  150.]
 [ 163.  702.]]
0.8698544698544699
Training: Epoch[018/050]  Loss: 0.1752 Acc:98.55%
Training: Epoch[019/050]  Loss: 0.1702 Acc:98.66%
[[1400.  140.]
 [ 176.  689.]]
0.8686070686070686
Training: Epoch[020/050]  Loss: 0.1672 Acc:98.66%
Training: Epoch[021/050]  Loss: 0.1647 Acc:98.66%
[[1414.  126.]
 [ 180.  685.]]
0.8727650727650728
Training: Epoch[022/050]  Loss: 0.1628 Acc:98.66%
Training: Epoch[023/050]  Loss: 0.1614 Acc:98.66%
[[1422.  118.]
 [ 182.  683.]]
0.8752598752598753
Training: Epoch[024/050]  Loss: 0.1602 Acc:98.66%
Training: Epoch[025/050]  Loss: 0.1595 Acc:98.66%
[[1424.  116.]
 [ 183.  682.]]
0.8756756756756757
Training: Epoch[026/050]  Loss: 0.1592 Acc:98.66%
Training: Epoch[027/050]  Loss: 0.1609 Acc:98.66%
[[1393.  147.]
 [ 167.  698.]]
0.8694386694386694
Training: Epoch[028/050]  Loss: 0.1922 Acc:98.45%
Training: Epoch[029/050]  Loss: 0.3257 Acc:96.48%
[[1174.  366.]
 [  66.  799.]]
0.8203742203742204
Training: Epoch[030/050]  Loss: 0.3267 Acc:96.69%
Training: Epoch[031/050]  Loss: 0.1593 Acc:98.55%
[[1428.  112.]
 [ 191.  674.]]
0.8740124740124741
Training: Epoch[032/050]  Loss: 0.1614 Acc:98.86%
Training: Epoch[033/050]  Loss: 0.1717 Acc:98.45%
[[1463.   77.]
 [ 229.  636.]]
0.8727650727650728
Training: Epoch[034/050]  Loss: 0.1777 Acc:98.55%
Training: Epoch[035/050]  Loss: 0.2357 Acc:97.72%
[[1467.   73.]
 [ 238.  627.]]
0.8706860706860707
Training: Epoch[036/050]  Loss: 0.1834 Acc:98.45%
Training: Epoch[037/050]  Loss: 0.2318 Acc:97.72%
[[1466.   74.]
 [ 235.  630.]]
0.8715176715176716
Training: Epoch[038/050]  Loss: 0.1849 Acc:98.45%
Training: Epoch[039/050]  Loss: 0.2208 Acc:97.62%
[[1461.   79.]
 [ 228.  637.]]
0.8723492723492724
Training: Epoch[040/050]  Loss: 0.1827 Acc:98.45%
Training: Epoch[041/050]  Loss: 0.2095 Acc:97.72%
[[1453.   87.]
 [ 220.  645.]]
0.8723492723492724
Training: Epoch[042/050]  Loss: 0.1789 Acc:98.55%
Training: Epoch[043/050]  Loss: 0.2007 Acc:97.83%
[[1451.   89.]
 [ 214.  651.]]
0.8740124740124741
Training: Epoch[044/050]  Loss: 0.1758 Acc:98.55%
Training: Epoch[045/050]  Loss: 0.1955 Acc:97.83%
[[1444.   96.]
 [ 211.  654.]]
0.8723492723492724
Training: Epoch[046/050]  Loss: 0.1737 Acc:98.55%
Training: Epoch[047/050]  Loss: 0.1926 Acc:97.83%
[[1444.   96.]
 [ 210.  655.]]
0.8727650727650728
Training: Epoch[048/050]  Loss: 0.1722 Acc:98.76%
Training: Epoch[049/050]  Loss: 0.1904 Acc:97.83%
[[1444.   96.]
 [ 208.  657.]]
0.8735966735966736
Training: Epoch[050/050]  Loss: 0.1709 Acc:98.76%
Training: Epoch[001/050]  Loss: 4.1430 Acc:64.01%
[[   0. 1540.]
 [   0.  865.]]
0.3596673596673597
Training: Epoch[002/050]  Loss: 2.5331 Acc:83.04%
Training: Epoch[003/050]  Loss: 2.3464 Acc:83.04%
[[   0. 1540.]
 [   0.  865.]]
0.3596673596673597
Training: Epoch[004/050]  Loss: 2.2648 Acc:83.04%
Training: Epoch[005/050]  Loss: 2.2058 Acc:83.04%
[[   0. 1540.]
 [   0.  865.]]
0.3596673596673597
Training: Epoch[006/050]  Loss: 2.1794 Acc:83.04%
Training: Epoch[007/050]  Loss: 2.0544 Acc:83.04%
[[   0. 1540.]
 [   0.  865.]]
0.3596673596673597
Training: Epoch[008/050]  Loss: 1.9205 Acc:83.04%
Training: Epoch[009/050]  Loss: 1.8092 Acc:83.04%
[[   0. 1540.]
 [   0.  865.]]
0.3596673596673597
Training: Epoch[010/050]  Loss: 1.6376 Acc:83.04%
Training: Epoch[011/050]  Loss: 1.3677 Acc:82.94%
[[ 282. 1258.]
 [   3.  862.]]
0.4756756756756757
Training: Epoch[012/050]  Loss: 1.1220 Acc:82.94%
Training: Epoch[013/050]  Loss: 0.8081 Acc:94.00%
[[1152.  388.]
 [  60.  805.]]
0.8137214137214137
Training: Epoch[014/050]  Loss: 0.5117 Acc:96.79%
Training: Epoch[015/050]  Loss: 0.3212 Acc:97.52%
[[1391.  149.]
 [ 154.  711.]]
0.8740124740124741
Training: Epoch[016/050]  Loss: 0.2307 Acc:97.93%
Training: Epoch[017/050]  Loss: 0.1968 Acc:98.45%
[[1427.  113.]
 [ 190.  675.]]
0.8740124740124741
Training: Epoch[018/050]  Loss: 0.1823 Acc:98.55%
Training: Epoch[019/050]  Loss: 0.1738 Acc:98.55%
[[1442.   98.]
 [ 198.  667.]]
0.8769230769230769
Training: Epoch[020/050]  Loss: 0.1687 Acc:98.76%
Training: Epoch[021/050]  Loss: 0.1653 Acc:98.76%
[[1445.   95.]
 [ 199.  666.]]
0.8777546777546777
Training: Epoch[022/050]  Loss: 0.1629 Acc:98.76%
Training: Epoch[023/050]  Loss: 0.1613 Acc:98.76%
[[1444.   96.]
 [ 199.  666.]]
0.8773388773388774
Training: Epoch[024/050]  Loss: 0.1599 Acc:98.76%
Training: Epoch[025/050]  Loss: 0.1589 Acc:98.76%
[[1443.   97.]
 [ 199.  666.]]
0.8769230769230769
Training: Epoch[026/050]  Loss: 0.1579 Acc:98.76%
Training: Epoch[027/050]  Loss: 0.1570 Acc:98.76%
[[1442.   98.]
 [ 199.  666.]]
0.8765072765072766
Training: Epoch[028/050]  Loss: 0.1561 Acc:98.86%
Training: Epoch[029/050]  Loss: 0.1553 Acc:98.86%
[[1437.  103.]
 [ 197.  668.]]
0.8752598752598753
Training: Epoch[030/050]  Loss: 0.1543 Acc:98.86%
Training: Epoch[031/050]  Loss: 0.1534 Acc:98.86%
[[1419.  121.]
 [ 178.  687.]]
0.8756756756756757
Training: Epoch[032/050]  Loss: 0.1877 Acc:98.45%
Training: Epoch[033/050]  Loss: 0.4023 Acc:96.28%
[[1327.  213.]
 [ 109.  756.]]
0.8661122661122661
Training: Epoch[034/050]  Loss: 0.2237 Acc:97.62%
Training: Epoch[035/050]  Loss: 0.1747 Acc:98.55%
[[1305.  235.]
 [ 101.  764.]]
0.8602910602910603
Training: Epoch[036/050]  Loss: 0.2337 Acc:97.62%
Training: Epoch[037/050]  Loss: 0.1753 Acc:98.55%
[[1300.  240.]
 [ 100.  765.]]
0.8586278586278586
Training: Epoch[038/050]  Loss: 0.2280 Acc:97.72%
Training: Epoch[039/050]  Loss: 0.1776 Acc:98.55%
[[1291.  249.]
 [  94.  771.]]
0.8573804573804574
Training: Epoch[040/050]  Loss: 0.2268 Acc:97.62%
Training: Epoch[041/050]  Loss: 0.1767 Acc:98.55%
[[1289.  251.]
 [  92.  773.]]
0.8573804573804574
Training: Epoch[042/050]  Loss: 0.2212 Acc:97.72%
Training: Epoch[043/050]  Loss: 0.1772 Acc:98.45%
[[1295.  245.]
 [  93.  772.]]
0.8594594594594595
Training: Epoch[044/050]  Loss: 0.2126 Acc:97.83%
Training: Epoch[045/050]  Loss: 0.1740 Acc:98.55%
[[1301.  239.]
 [ 100.  765.]]
0.8590436590436591
Training: Epoch[046/050]  Loss: 0.2023 Acc:97.83%
Training: Epoch[047/050]  Loss: 0.1725 Acc:98.55%
[[1304.  236.]
 [ 102.  763.]]
0.8594594594594595
Training: Epoch[048/050]  Loss: 0.1984 Acc:97.83%
Training: Epoch[049/050]  Loss: 0.1695 Acc:98.55%
[[1314.  226.]
 [ 103.  762.]]
0.8632016632016632
Training: Epoch[050/050]  Loss: 0.1922 Acc:97.93%

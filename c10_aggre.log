| Building net
epoch:0
epoch:1
epoch:2
epoch:3
epoch:4
epoch:5
epoch:6
epoch:7
epoch:8
epoch:9
epoch:10
epoch:11
epoch:12
epoch:13
epoch:14
epoch:15
epoch:16
epoch:17
epoch:18
epoch:19
epoch:20
epoch:21
epoch:22
epoch:23
epoch:24
epoch:25
epoch:26
epoch:27
epoch:28
epoch:29
epoch:30
epoch:31
epoch:32
epoch:33
epoch:34
epoch:35
epoch:36
epoch:37
epoch:38
epoch:39
epoch:40
epoch:41
epoch:42
epoch:43
epoch:44
epoch:45
epoch:46
epoch:47
epoch:48
epoch:49
epoch:50
epoch:51
epoch:52
epoch:53
epoch:54
epoch:55
epoch:56
epoch:57
epoch:58
epoch:59
epoch:60
epoch:61
epoch:62
epoch:63
epoch:64
epoch:65
epoch:66
epoch:67
epoch:68
epoch:69
epoch:70
epoch:71
epoch:72
epoch:73
epoch:74
epoch:75
epoch:76
epoch:77
epoch:78
epoch:79
epoch:80
epoch:81
epoch:82
epoch:83
epoch:84
epoch:85
epoch:86
epoch:87
epoch:88
epoch:89
epoch:90
epoch:91
epoch:92
epoch:93
epoch:94
epoch:95
epoch:96
epoch:97
epoch:98
epoch:99
epoch:100
epoch:101
epoch:102
epoch:103
epoch:104
epoch:105
epoch:106
epoch:107
epoch:108
epoch:109
epoch:110
epoch:111
epoch:112
epoch:113
epoch:114
epoch:115
epoch:116
epoch:117
epoch:118
epoch:119
epoch:120
epoch:121
epoch:122
epoch:123
epoch:124
epoch:125
epoch:126
epoch:127
epoch:128
epoch:129
epoch:130
epoch:131
epoch:132
epoch:133
epoch:134
epoch:135
epoch:136
epoch:137
epoch:138
epoch:139
epoch:140
epoch:141
epoch:142
epoch:143
epoch:144
epoch:145
epoch:146
epoch:147
epoch:148
epoch:149
epoch:150
epoch:151
epoch:152
epoch:153
epoch:154
epoch:155
epoch:156
epoch:157
epoch:158
epoch:159
epoch:160
epoch:161
epoch:162
epoch:163
epoch:164
epoch:165
epoch:166
epoch:167
epoch:168
epoch:169
epoch:170
epoch:171
epoch:172
epoch:173
epoch:174
epoch:175
epoch:176
epoch:177
epoch:178
epoch:179
epoch:180
epoch:181
epoch:182
epoch:183
epoch:184
epoch:185
epoch:186
epoch:187
epoch:188
epoch:189
epoch:190
epoch:191
epoch:192
epoch:193
epoch:194
epoch:195
epoch:196
epoch:197
epoch:198
epoch:199
epoch:200
epoch:201
epoch:202
epoch:203
epoch:204
epoch:205
epoch:206
epoch:207
epoch:208
epoch:209
epoch:210
epoch:211
epoch:212
epoch:213
epoch:214
epoch:215
epoch:216
epoch:217
epoch:218
epoch:219
epoch:220
epoch:221
epoch:222
epoch:223
epoch:224
epoch:225
epoch:226
epoch:227
epoch:228
epoch:229
epoch:230
epoch:231
epoch:232
epoch:233
epoch:234
epoch:235
epoch:236
epoch:237
epoch:238
epoch:239
epoch:240
epoch:241
epoch:242
epoch:243
epoch:244
epoch:245
epoch:246
epoch:247
epoch:248
epoch:249
epoch:250
epoch:251
epoch:252
epoch:253
epoch:254
epoch:255
epoch:256
epoch:257
epoch:258
epoch:259
epoch:260
epoch:261
epoch:262
epoch:263
epoch:264
epoch:265
epoch:266
epoch:267
epoch:268
epoch:269
epoch:270
epoch:271
epoch:272
epoch:273
epoch:274
epoch:275
epoch:276
epoch:277
epoch:278
epoch:279
epoch:280
epoch:281
epoch:282
epoch:283
epoch:284
epoch:285
epoch:286
epoch:287
epoch:288
epoch:289
epoch:290
epoch:291
epoch:292
epoch:293
epoch:294
epoch:295
epoch:296
epoch:297
epoch:298
epoch:299
| Building net
0.2
4001
saving noisy labels to ./checkpoints/c10/20sym/aggre/saved/easy_labels.p...
epoch:0
epoch:1
epoch:2
epoch:3
epoch:4
epoch:5
epoch:6
epoch:7
epoch:8
epoch:9
epoch:10
epoch:11
epoch:12
epoch:13
epoch:14
epoch:15
epoch:16
epoch:17
epoch:18
epoch:19
epoch:20
epoch:21
epoch:22
epoch:23
epoch:24
epoch:25
epoch:26
epoch:27
epoch:28
epoch:29
epoch:30
epoch:31
epoch:32
epoch:33
epoch:34
epoch:35
epoch:36
epoch:37
epoch:38
epoch:39
epoch:40
epoch:41
epoch:42
epoch:43
epoch:44
epoch:45
epoch:46
epoch:47
epoch:48
epoch:49
epoch:50
epoch:51
epoch:52
epoch:53
epoch:54
epoch:55
epoch:56
epoch:57
epoch:58
epoch:59
epoch:60
epoch:61
epoch:62
epoch:63
epoch:64
epoch:65
epoch:66
epoch:67
epoch:68
epoch:69
epoch:70
epoch:71
epoch:72
epoch:73
epoch:74
epoch:75
epoch:76
epoch:77
epoch:78
epoch:79
epoch:80
epoch:81
epoch:82
epoch:83
epoch:84
epoch:85
epoch:86
epoch:87
epoch:88
epoch:89
epoch:90
epoch:91
epoch:92
epoch:93
epoch:94
epoch:95
epoch:96
epoch:97
epoch:98
epoch:99
epoch:100
epoch:101
epoch:102
epoch:103
epoch:104
epoch:105
epoch:106
epoch:107
epoch:108
epoch:109
epoch:110
epoch:111
epoch:112
epoch:113
epoch:114
epoch:115
epoch:116
epoch:117
epoch:118
epoch:119
epoch:120
epoch:121
epoch:122
epoch:123
epoch:124
epoch:125
epoch:126
epoch:127
epoch:128
epoch:129
epoch:130
epoch:131
epoch:132
epoch:133
epoch:134
epoch:135
epoch:136
epoch:137
epoch:138
epoch:139
epoch:140
epoch:141
epoch:142
epoch:143
epoch:144
epoch:145
epoch:146
epoch:147
epoch:148
epoch:149
epoch:150
epoch:151
epoch:152
epoch:153
epoch:154
epoch:155
epoch:156
epoch:157
epoch:158
epoch:159
epoch:160
epoch:161
epoch:162
epoch:163
epoch:164
epoch:165
epoch:166
epoch:167
epoch:168
epoch:169
epoch:170
epoch:171
epoch:172
epoch:173
epoch:174
epoch:175
epoch:176
epoch:177
epoch:178
epoch:179
epoch:180
epoch:181
epoch:182
epoch:183
epoch:184
epoch:185
epoch:186
epoch:187
epoch:188
epoch:189
epoch:190
epoch:191
epoch:192
epoch:193
epoch:194
epoch:195
epoch:196
epoch:197
epoch:198
epoch:199
epoch:200
epoch:201
epoch:202
epoch:203
epoch:204
epoch:205
epoch:206
epoch:207
epoch:208
epoch:209
epoch:210
epoch:211
epoch:212
epoch:213
epoch:214
epoch:215
epoch:216
epoch:217
epoch:218
epoch:219
epoch:220
epoch:221
epoch:222
epoch:223
epoch:224
epoch:225
epoch:226
epoch:227
epoch:228
epoch:229
epoch:230
epoch:231
epoch:232
epoch:233
epoch:234
epoch:235
epoch:236
epoch:237
epoch:238
epoch:239
epoch:240
epoch:241
epoch:242
epoch:243
epoch:244
epoch:245
epoch:246
epoch:247
epoch:248
epoch:249
epoch:250
epoch:251
epoch:252
epoch:253
epoch:254
epoch:255
epoch:256
epoch:257
epoch:258
epoch:259
epoch:260
epoch:261
epoch:262
epoch:263
epoch:264
epoch:265
epoch:266
epoch:267
epoch:268
epoch:269
epoch:270
epoch:271
epoch:272
epoch:273
epoch:274
epoch:275
epoch:276
epoch:277
epoch:278
epoch:279
epoch:280
epoch:281
epoch:282
epoch:283
epoch:284
epoch:285
epoch:286
epoch:287
epoch:288
epoch:289
epoch:290
epoch:291
epoch:292
epoch:293
epoch:294
epoch:295
epoch:296
epoch:297
epoch:298
epoch:299
training 1D-CNN classifier for class0
Training: Epoch[001/050]  Loss: 3.1270 Acc:63.27%
[[   0.  101.]
 [   0. 2556.]]
0.9619872036130975
Training: Epoch[002/050]  Loss: 2.6957 Acc:85.93%
Training: Epoch[003/050]  Loss: 2.4621 Acc:85.93%
[[   0.  101.]
 [   0. 2556.]]
0.9619872036130975
Training: Epoch[004/050]  Loss: 2.3035 Acc:85.93%
Training: Epoch[005/050]  Loss: 2.1669 Acc:85.93%
[[   0.  101.]
 [   0. 2556.]]
0.9619872036130975
Training: Epoch[006/050]  Loss: 2.0013 Acc:85.93%
Training: Epoch[007/050]  Loss: 1.7988 Acc:85.93%
[[   0.  101.]
 [   0. 2556.]]
0.9619872036130975
Training: Epoch[008/050]  Loss: 1.5400 Acc:85.93%
Training: Epoch[009/050]  Loss: 1.2662 Acc:85.93%
[[   0.  101.]
 [   0. 2556.]]
0.9619872036130975
Training: Epoch[010/050]  Loss: 0.9607 Acc:87.25%
Training: Epoch[011/050]  Loss: 0.6799 Acc:95.00%
[[   8.   93.]
 [   9. 2547.]]
0.9616108392924351
Training: Epoch[012/050]  Loss: 0.4602 Acc:97.07%
Training: Epoch[013/050]  Loss: 0.3222 Acc:98.02%
[[  39.   62.]
 [  41. 2515.]]
0.9612344749717727
Training: Epoch[014/050]  Loss: 0.2364 Acc:98.58%
Training: Epoch[015/050]  Loss: 0.1838 Acc:98.87%
[[  55.   46.]
 [  63. 2493.]]
0.9589762890477983
Training: Epoch[016/050]  Loss: 0.1492 Acc:99.53%
Training: Epoch[017/050]  Loss: 0.1262 Acc:99.53%
[[  59.   42.]
 [  80. 2476.]]
0.954083552879187
Training: Epoch[018/050]  Loss: 0.1101 Acc:99.53%
Training: Epoch[019/050]  Loss: 0.0987 Acc:99.53%
[[  61.   40.]
 [  89. 2467.]]
0.9514490026345502
Training: Epoch[020/050]  Loss: 0.0903 Acc:99.53%
Training: Epoch[021/050]  Loss: 0.0842 Acc:99.62%
[[  63.   38.]
 [  96. 2460.]]
0.9495671810312383
Training: Epoch[022/050]  Loss: 0.0798 Acc:99.62%
Training: Epoch[023/050]  Loss: 0.0767 Acc:99.62%
[[  63.   38.]
 [  97. 2459.]]
0.9491908167105758
Training: Epoch[024/050]  Loss: 0.0746 Acc:99.62%
Training: Epoch[025/050]  Loss: 0.0732 Acc:99.62%
[[  63.   38.]
 [  98. 2458.]]
0.9488144523899135
Training: Epoch[026/050]  Loss: 0.0717 Acc:99.62%
Training: Epoch[027/050]  Loss: 0.0694 Acc:99.72%
[[  63.   38.]
 [ 105. 2451.]]
0.9461799021452766
Training: Epoch[028/050]  Loss: 0.0655 Acc:99.72%
Training: Epoch[029/050]  Loss: 0.0593 Acc:99.62%
[[  69.   32.]
 [ 119. 2437.]]
0.9431689875799775
Training: Epoch[030/050]  Loss: 0.0518 Acc:99.62%
Training: Epoch[031/050]  Loss: 0.0457 Acc:99.62%
[[  69.   32.]
 [ 141. 2415.]]
0.9348889725254046
Training: Epoch[032/050]  Loss: 0.0441 Acc:99.62%
Training: Epoch[033/050]  Loss: 0.0442 Acc:99.62%
[[  69.   32.]
 [ 137. 2419.]]
0.9363944298080542
Training: Epoch[034/050]  Loss: 0.0439 Acc:99.62%
Training: Epoch[035/050]  Loss: 0.0444 Acc:99.62%
[[  69.   32.]
 [ 133. 2423.]]
0.9378998870907038
Training: Epoch[036/050]  Loss: 0.0464 Acc:99.62%
Training: Epoch[037/050]  Loss: 0.0527 Acc:99.62%
[[  63.   38.]
 [ 103. 2453.]]
0.9469326307866014
Training: Epoch[038/050]  Loss: 0.0692 Acc:99.62%
Training: Epoch[039/050]  Loss: 0.0934 Acc:99.43%
[[  59.   42.]
 [  77. 2479.]]
0.9552126458411743
Training: Epoch[040/050]  Loss: 0.1009 Acc:99.43%
Training: Epoch[041/050]  Loss: 0.0686 Acc:99.62%
[[  71.   30.]
 [ 157. 2399.]]
0.929619872036131
Training: Epoch[042/050]  Loss: 0.0378 Acc:99.72%
Training: Epoch[043/050]  Loss: 0.0397 Acc:99.72%
[[  71.   30.]
 [ 169. 2387.]]
0.9251035001881822
Training: Epoch[044/050]  Loss: 0.0375 Acc:99.72%
Training: Epoch[045/050]  Loss: 0.0453 Acc:99.62%
[[  71.   30.]
 [ 174. 2382.]]
0.9232216785848701
Training: Epoch[046/050]  Loss: 0.0379 Acc:99.72%
Training: Epoch[047/050]  Loss: 0.0541 Acc:99.72%
[[  71.   30.]
 [ 174. 2382.]]
0.9232216785848701
Training: Epoch[048/050]  Loss: 0.0374 Acc:99.72%
Training: Epoch[049/050]  Loss: 0.0515 Acc:99.72%
[[  71.   30.]
 [ 177. 2379.]]
0.922092585622883
Training: Epoch[050/050]  Loss: 0.0372 Acc:99.72%
Training: Epoch[001/050]  Loss: 4.3855 Acc:65.72%
[[   0.  101.]
 [   0. 2556.]]
0.9619872036130975
Training: Epoch[002/050]  Loss: 2.5577 Acc:85.93%
Training: Epoch[003/050]  Loss: 2.3880 Acc:85.93%
[[   0.  101.]
 [   0. 2556.]]
0.9619872036130975
Training: Epoch[004/050]  Loss: 2.2563 Acc:85.93%
Training: Epoch[005/050]  Loss: 2.0887 Acc:85.93%
[[   0.  101.]
 [   0. 2556.]]
0.9619872036130975
Training: Epoch[006/050]  Loss: 2.1716 Acc:85.93%
Training: Epoch[007/050]  Loss: 1.7580 Acc:85.93%
[[   0.  101.]
 [   0. 2556.]]
0.9619872036130975
Training: Epoch[008/050]  Loss: 1.5483 Acc:85.93%
Training: Epoch[009/050]  Loss: 1.3087 Acc:85.93%
[[   0.  101.]
 [   0. 2556.]]
0.9619872036130975
Training: Epoch[010/050]  Loss: 1.0454 Acc:87.54%
Training: Epoch[011/050]  Loss: 0.8008 Acc:94.52%
[[3.000e+00 9.800e+01]
 [2.000e+00 2.554e+03]]
0.9623635679337599
Training: Epoch[012/050]  Loss: 0.5359 Acc:96.32%
Training: Epoch[013/050]  Loss: 0.3352 Acc:97.83%
[[  22.   79.]
 [  28. 2528.]]
0.9597290176891231
Training: Epoch[014/050]  Loss: 0.2506 Acc:98.11%
Training: Epoch[015/050]  Loss: 0.2004 Acc:98.68%
[[  35.   66.]
 [  46. 2510.]]
0.957847196085811
Training: Epoch[016/050]  Loss: 0.1685 Acc:99.06%
Training: Epoch[017/050]  Loss: 0.1485 Acc:99.24%
[[  44.   57.]
 [  51. 2505.]]
0.9593526533684607
Training: Epoch[018/050]  Loss: 0.1361 Acc:99.43%
Training: Epoch[019/050]  Loss: 0.1284 Acc:99.43%
[[  46.   55.]
 [  55. 2501.]]
0.9585999247271358
Training: Epoch[020/050]  Loss: 0.1235 Acc:99.43%
Training: Epoch[021/050]  Loss: 0.1186 Acc:99.43%
[[  52.   49.]
 [  62. 2494.]]
0.9582235604064735
Training: Epoch[022/050]  Loss: 0.1119 Acc:99.53%
Training: Epoch[023/050]  Loss: 0.1026 Acc:99.53%
[[  54.   47.]
 [  73. 2483.]]
0.9548362815205118
Training: Epoch[024/050]  Loss: 0.0920 Acc:99.53%
Training: Epoch[025/050]  Loss: 0.0825 Acc:99.53%
[[  59.   42.]
 [  87. 2469.]]
0.9514490026345502
Training: Epoch[026/050]  Loss: 0.0761 Acc:99.62%
Training: Epoch[027/050]  Loss: 0.0738 Acc:99.62%
[[  59.   42.]
 [  90. 2466.]]
0.9503199096725631
Training: Epoch[028/050]  Loss: 0.0761 Acc:99.62%
Training: Epoch[029/050]  Loss: 0.0840 Acc:99.62%
[[  55.   46.]
 [  76. 2480.]]
0.954083552879187
Training: Epoch[030/050]  Loss: 0.0950 Acc:99.43%
Training: Epoch[031/050]  Loss: 0.0968 Acc:99.43%
[[  57.   44.]
 [  81. 2475.]]
0.9529544599171998
Training: Epoch[032/050]  Loss: 0.0789 Acc:99.53%
Training: Epoch[033/050]  Loss: 0.0544 Acc:99.72%
[[  66.   35.]
 [ 126. 2430.]]
0.9394053443733534
Training: Epoch[034/050]  Loss: 0.0427 Acc:99.81%
Training: Epoch[035/050]  Loss: 0.0422 Acc:99.81%
[[  66.   35.]
 [ 128. 2428.]]
0.9386526157320286
Training: Epoch[036/050]  Loss: 0.0423 Acc:99.81%
Training: Epoch[037/050]  Loss: 0.0424 Acc:99.81%
[[  66.   35.]
 [ 127. 2429.]]
0.939028980052691
Training: Epoch[038/050]  Loss: 0.0435 Acc:99.81%
Training: Epoch[039/050]  Loss: 0.0482 Acc:99.81%
[[  62.   39.]
 [ 104. 2452.]]
0.9461799021452766
Training: Epoch[040/050]  Loss: 0.0690 Acc:99.62%
Training: Epoch[041/050]  Loss: 0.1037 Acc:99.43%
[[  53.   48.]
 [  71. 2485.]]
0.9552126458411743
Training: Epoch[042/050]  Loss: 0.0955 Acc:99.43%
Training: Epoch[043/050]  Loss: 0.0553 Acc:99.72%
[[  68.   33.]
 [ 149. 2407.]]
0.931501693639443
Training: Epoch[044/050]  Loss: 0.0385 Acc:99.62%
Training: Epoch[045/050]  Loss: 0.0412 Acc:99.81%
[[  68.   33.]
 [ 148. 2408.]]
0.9318780579601054
Training: Epoch[046/050]  Loss: 0.0381 Acc:99.62%
Training: Epoch[047/050]  Loss: 0.0414 Acc:99.81%
[[  68.   33.]
 [ 144. 2412.]]
0.933383515242755
Training: Epoch[048/050]  Loss: 0.0379 Acc:99.72%
Training: Epoch[049/050]  Loss: 0.0410 Acc:99.81%
[[  68.   33.]
 [ 140. 2416.]]
0.9348889725254046
Training: Epoch[050/050]  Loss: 0.0388 Acc:99.81%
training 1D-CNN classifier for class1
Training: Epoch[001/050]  Loss: 4.5416 Acc:63.73%
[[   0.  231.]
 [   0. 2524.]]
0.9161524500907441
Training: Epoch[002/050]  Loss: 2.5179 Acc:87.48%
Training: Epoch[003/050]  Loss: 2.3057 Acc:87.48%
[[   0.  231.]
 [   0. 2524.]]
0.9161524500907441
Training: Epoch[004/050]  Loss: 2.2315 Acc:87.48%
Training: Epoch[005/050]  Loss: 2.1719 Acc:87.48%
[[   0.  231.]
 [   0. 2524.]]
0.9161524500907441
Training: Epoch[006/050]  Loss: 2.0027 Acc:87.48%
Training: Epoch[007/050]  Loss: 2.6281 Acc:87.48%
[[   0.  231.]
 [   0. 2524.]]
0.9161524500907441
Training: Epoch[008/050]  Loss: 2.4805 Acc:87.48%
Training: Epoch[009/050]  Loss: 2.0363 Acc:87.48%
[[   0.  231.]
 [   0. 2524.]]
0.9161524500907441
Training: Epoch[010/050]  Loss: 2.0950 Acc:87.48%
Training: Epoch[011/050]  Loss: 1.5641 Acc:87.48%
[[   0.  231.]
 [   0. 2524.]]
0.9161524500907441
Training: Epoch[012/050]  Loss: 1.2024 Acc:87.48%
Training: Epoch[013/050]  Loss: 0.8740 Acc:91.93%
[[1.000e+00 2.300e+02]
 [0.000e+00 2.524e+03]]
0.9165154264972777
Training: Epoch[014/050]  Loss: 0.6071 Acc:96.47%
Training: Epoch[015/050]  Loss: 0.4045 Acc:97.40%
[[  33.  198.]
 [   4. 2520.]]
0.9266787658802178
Training: Epoch[016/050]  Loss: 0.2686 Acc:98.61%
Training: Epoch[017/050]  Loss: 0.2022 Acc:98.70%
[[  66.  165.]
 [  17. 2507.]]
0.9339382940108893
Training: Epoch[018/050]  Loss: 0.1568 Acc:98.98%
Training: Epoch[019/050]  Loss: 0.1276 Acc:99.26%
[[  85.  146.]
 [  23. 2501.]]
0.9386569872958258
Training: Epoch[020/050]  Loss: 0.1070 Acc:99.26%
Training: Epoch[021/050]  Loss: 0.0931 Acc:99.26%
[[  90.  141.]
 [  28. 2496.]]
0.9386569872958258
Training: Epoch[022/050]  Loss: 0.0828 Acc:99.35%
Training: Epoch[023/050]  Loss: 0.0748 Acc:99.44%
[[  94.  137.]
 [  32. 2492.]]
0.9386569872958258
Training: Epoch[024/050]  Loss: 0.0676 Acc:99.44%
Training: Epoch[025/050]  Loss: 0.0601 Acc:99.63%
[[ 103.  128.]
 [  35. 2489.]]
0.9408348457350272
Training: Epoch[026/050]  Loss: 0.0549 Acc:99.81%
Training: Epoch[027/050]  Loss: 0.0496 Acc:99.81%
[[ 113.  118.]
 [  36. 2488.]]
0.9441016333938294
Training: Epoch[028/050]  Loss: 0.0431 Acc:99.81%
Training: Epoch[029/050]  Loss: 0.0365 Acc:99.81%
[[ 123.  108.]
 [  46. 2478.]]
0.9441016333938294
Training: Epoch[030/050]  Loss: 0.0307 Acc:99.81%
Training: Epoch[031/050]  Loss: 0.0250 Acc:99.81%
[[ 135.   96.]
 [  56. 2468.]]
0.9448275862068966
Training: Epoch[032/050]  Loss: 0.0205 Acc:99.91%
Training: Epoch[033/050]  Loss: 0.0171 Acc:99.91%
[[ 141.   90.]
 [  61. 2463.]]
0.9451905626134302
Training: Epoch[034/050]  Loss: 0.0153 Acc:99.91%
Training: Epoch[035/050]  Loss: 0.0143 Acc:99.91%
[[ 146.   85.]
 [  63. 2461.]]
0.9462794918330308
Training: Epoch[036/050]  Loss: 0.0138 Acc:99.91%
Training: Epoch[037/050]  Loss: 0.0147 Acc:99.91%
[[ 137.   94.]
 [  59. 2465.]]
0.944464609800363
Training: Epoch[038/050]  Loss: 0.0179 Acc:99.91%
Training: Epoch[039/050]  Loss: 0.0306 Acc:99.81%
[[  95.  136.]
 [  32. 2492.]]
0.9390199637023593
Training: Epoch[040/050]  Loss: 0.0980 Acc:99.07%
Training: Epoch[041/050]  Loss: 0.2203 Acc:98.05%
[[  82.  149.]
 [  22. 2502.]]
0.9379310344827586
Training: Epoch[042/050]  Loss: 0.0816 Acc:99.26%
Training: Epoch[043/050]  Loss: 0.0081 Acc:100.00%
[[ 154.   77.]
 [  86. 2438.]]
0.9408348457350272
Training: Epoch[044/050]  Loss: 0.0066 Acc:100.00%
Training: Epoch[045/050]  Loss: 0.0064 Acc:100.00%
[[ 154.   77.]
 [  85. 2439.]]
0.9411978221415608
Training: Epoch[046/050]  Loss: 0.0062 Acc:100.00%
Training: Epoch[047/050]  Loss: 0.0061 Acc:100.00%
[[ 154.   77.]
 [  86. 2438.]]
0.9408348457350272
Training: Epoch[048/050]  Loss: 0.0059 Acc:100.00%
Training: Epoch[049/050]  Loss: 0.0057 Acc:100.00%
[[ 154.   77.]
 [  86. 2438.]]
0.9408348457350272
Training: Epoch[050/050]  Loss: 0.0055 Acc:100.00%
Training: Epoch[001/050]  Loss: 2.4712 Acc:63.73%
[[   0.  231.]
 [   0. 2524.]]
0.9161524500907441
Training: Epoch[002/050]  Loss: 2.8182 Acc:87.48%
Training: Epoch[003/050]  Loss: 2.4214 Acc:87.48%
[[   0.  231.]
 [   0. 2524.]]
0.9161524500907441
Training: Epoch[004/050]  Loss: 2.2095 Acc:87.48%
Training: Epoch[005/050]  Loss: 2.0627 Acc:87.48%
[[   0.  231.]
 [   0. 2524.]]
0.9161524500907441
Training: Epoch[006/050]  Loss: 1.8993 Acc:87.48%
Training: Epoch[007/050]  Loss: 1.9682 Acc:87.48%
[[   0.  231.]
 [   0. 2524.]]
0.9161524500907441
Training: Epoch[008/050]  Loss: 1.6311 Acc:87.48%
Training: Epoch[009/050]  Loss: 1.8521 Acc:87.48%
[[   0.  231.]
 [   0. 2524.]]
0.9161524500907441
Training: Epoch[010/050]  Loss: 1.4130 Acc:87.48%
Training: Epoch[011/050]  Loss: 1.1019 Acc:87.48%
[[   0.  231.]
 [   0. 2524.]]
0.9161524500907441
Training: Epoch[012/050]  Loss: 0.9785 Acc:87.48%
Training: Epoch[013/050]  Loss: 0.6761 Acc:94.25%
[[9.000e+00 2.220e+02]
 [1.000e+00 2.523e+03]]
0.9190562613430127
Training: Epoch[014/050]  Loss: 0.4816 Acc:96.66%
Training: Epoch[015/050]  Loss: 0.3396 Acc:97.96%
[[  51.  180.]
 [   6. 2518.]]
0.932486388384755
Training: Epoch[016/050]  Loss: 0.2368 Acc:98.61%
Training: Epoch[017/050]  Loss: 0.1694 Acc:99.07%
[[  82.  149.]
 [  21. 2503.]]
0.9382940108892922
Training: Epoch[018/050]  Loss: 0.1266 Acc:99.26%
Training: Epoch[019/050]  Loss: 0.1005 Acc:99.26%
[[  92.  139.]
 [  27. 2497.]]
0.9397459165154265
Training: Epoch[020/050]  Loss: 0.0808 Acc:99.44%
Training: Epoch[021/050]  Loss: 0.0678 Acc:99.54%
[[ 101.  130.]
 [  36. 2488.]]
0.9397459165154265
Training: Epoch[022/050]  Loss: 0.0587 Acc:99.72%
Training: Epoch[023/050]  Loss: 0.0527 Acc:99.72%
[[ 106.  125.]
 [  37. 2487.]]
0.9411978221415608
Training: Epoch[024/050]  Loss: 0.0489 Acc:99.72%
Training: Epoch[025/050]  Loss: 0.0472 Acc:99.72%
[[ 108.  123.]
 [  37. 2487.]]
0.941923774954628
Training: Epoch[026/050]  Loss: 0.0473 Acc:99.72%
Training: Epoch[027/050]  Loss: 0.0489 Acc:99.72%
[[ 106.  125.]
 [  36. 2488.]]
0.9415607985480944
Training: Epoch[028/050]  Loss: 0.0502 Acc:99.72%
Training: Epoch[029/050]  Loss: 0.0469 Acc:99.72%
[[ 114.  117.]
 [  41. 2483.]]
0.9426497277676951
Training: Epoch[030/050]  Loss: 0.0359 Acc:99.81%
Training: Epoch[031/050]  Loss: 0.0198 Acc:99.81%
[[ 151.   80.]
 [  71. 2453.]]
0.9451905626134302
Training: Epoch[032/050]  Loss: 0.0106 Acc:99.91%
Training: Epoch[033/050]  Loss: 0.0092 Acc:99.91%
[[ 153.   78.]
 [  78. 2446.]]
0.9433756805807623
Training: Epoch[034/050]  Loss: 0.0087 Acc:99.91%
Training: Epoch[035/050]  Loss: 0.0083 Acc:99.91%
[[ 153.   78.]
 [  81. 2443.]]
0.9422867513611616
Training: Epoch[036/050]  Loss: 0.0078 Acc:99.91%
Training: Epoch[037/050]  Loss: 0.0074 Acc:99.91%
[[ 154.   77.]
 [  81. 2443.]]
0.9426497277676951
Training: Epoch[038/050]  Loss: 0.0071 Acc:100.00%
Training: Epoch[039/050]  Loss: 0.0067 Acc:100.00%
[[ 154.   77.]
 [  81. 2443.]]
0.9426497277676951
Training: Epoch[040/050]  Loss: 0.0062 Acc:100.00%
Training: Epoch[041/050]  Loss: 0.0056 Acc:100.00%
[[ 154.   77.]
 [  85. 2439.]]
0.9411978221415608
Training: Epoch[042/050]  Loss: 0.0050 Acc:100.00%
Training: Epoch[043/050]  Loss: 0.0048 Acc:100.00%
[[ 154.   77.]
 [  86. 2438.]]
0.9408348457350272
Training: Epoch[044/050]  Loss: 0.0044 Acc:100.00%
Training: Epoch[045/050]  Loss: 0.0043 Acc:100.00%
[[ 154.   77.]
 [  87. 2437.]]
0.9404718693284937
Training: Epoch[046/050]  Loss: 0.0039 Acc:100.00%
Training: Epoch[047/050]  Loss: 0.0050 Acc:100.00%
[[ 154.   77.]
 [  87. 2437.]]
0.9404718693284937
Training: Epoch[048/050]  Loss: 0.0037 Acc:100.00%
Training: Epoch[049/050]  Loss: 0.0035 Acc:100.00%
[[ 154.   77.]
 [  86. 2438.]]
0.9408348457350272
Training: Epoch[050/050]  Loss: 0.0036 Acc:100.00%
training 1D-CNN classifier for class2
Training: Epoch[001/050]  Loss: 0.9465 Acc:86.58%
[[   0.  279.]
 [   0. 2425.]]
0.896819526627219
Training: Epoch[002/050]  Loss: 3.5217 Acc:88.88%
Training: Epoch[003/050]  Loss: 2.3865 Acc:88.88%
[[   0.  279.]
 [   0. 2425.]]
0.896819526627219
Training: Epoch[004/050]  Loss: 2.1291 Acc:88.88%
Training: Epoch[005/050]  Loss: 2.1089 Acc:88.88%
[[   0.  279.]
 [   0. 2425.]]
0.896819526627219
Training: Epoch[006/050]  Loss: 1.9440 Acc:88.88%
Training: Epoch[007/050]  Loss: 1.8632 Acc:88.88%
[[   0.  279.]
 [   0. 2425.]]
0.896819526627219
Training: Epoch[008/050]  Loss: 1.7865 Acc:88.88%
Training: Epoch[009/050]  Loss: 1.7073 Acc:88.88%
[[   0.  279.]
 [   0. 2425.]]
0.896819526627219
Training: Epoch[010/050]  Loss: 1.6011 Acc:88.88%
Training: Epoch[011/050]  Loss: 1.4950 Acc:88.88%
[[   0.  279.]
 [   0. 2425.]]
0.896819526627219
Training: Epoch[012/050]  Loss: 1.5346 Acc:88.88%
Training: Epoch[013/050]  Loss: 1.2372 Acc:88.88%
[[   0.  279.]
 [   0. 2425.]]
0.896819526627219
Training: Epoch[014/050]  Loss: 1.1987 Acc:88.88%
Training: Epoch[015/050]  Loss: 0.9889 Acc:88.88%
[[   0.  279.]
 [   0. 2425.]]
0.896819526627219
Training: Epoch[016/050]  Loss: 0.8387 Acc:89.55%
Training: Epoch[017/050]  Loss: 0.6460 Acc:94.34%
[[   8.  271.]
 [   3. 2422.]]
0.8986686390532544
Training: Epoch[018/050]  Loss: 0.4854 Acc:96.45%
Training: Epoch[019/050]  Loss: 0.3685 Acc:97.51%
[[  51.  228.]
 [  46. 2379.]]
0.8986686390532544
Training: Epoch[020/050]  Loss: 0.2816 Acc:98.56%
Training: Epoch[021/050]  Loss: 0.2246 Acc:98.95%
[[  72.  207.]
 [  77. 2348.]]
0.8949704142011834
Training: Epoch[022/050]  Loss: 0.1862 Acc:98.95%
Training: Epoch[023/050]  Loss: 0.1613 Acc:99.04%
[[  87.  192.]
 [  94. 2331.]]
0.8942307692307693
Training: Epoch[024/050]  Loss: 0.1450 Acc:99.14%
Training: Epoch[025/050]  Loss: 0.1330 Acc:99.14%
[[  95.  184.]
 [ 105. 2320.]]
0.8931213017751479
Training: Epoch[026/050]  Loss: 0.1243 Acc:99.23%
Training: Epoch[027/050]  Loss: 0.1197 Acc:99.23%
[[  99.  180.]
 [ 109. 2316.]]
0.8931213017751479
Training: Epoch[028/050]  Loss: 0.1193 Acc:99.23%
Training: Epoch[029/050]  Loss: 0.1307 Acc:99.14%
[[  81.  198.]
 [  88. 2337.]]
0.8942307692307693
Training: Epoch[030/050]  Loss: 0.1642 Acc:98.95%
Training: Epoch[031/050]  Loss: 0.1987 Acc:98.37%
[[  73.  206.]
 [  73. 2352.]]
0.896819526627219
Training: Epoch[032/050]  Loss: 0.1600 Acc:98.95%
Training: Epoch[033/050]  Loss: 0.0877 Acc:99.33%
[[ 131.  148.]
 [ 155. 2270.]]
0.8879437869822485
Training: Epoch[034/050]  Loss: 0.0808 Acc:99.62%
Training: Epoch[035/050]  Loss: 0.1072 Acc:99.14%
[[ 135.  144.]
 [ 165. 2260.]]
0.8857248520710059
Training: Epoch[036/050]  Loss: 0.0801 Acc:99.52%
Training: Epoch[037/050]  Loss: 0.1405 Acc:98.95%
[[ 137.  142.]
 [ 166. 2259.]]
0.886094674556213
Training: Epoch[038/050]  Loss: 0.0796 Acc:99.42%
Training: Epoch[039/050]  Loss: 0.1321 Acc:98.95%
[[ 142.  137.]
 [ 178. 2247.]]
0.8835059171597633
Training: Epoch[040/050]  Loss: 0.0828 Acc:99.23%
Training: Epoch[041/050]  Loss: 0.1535 Acc:98.95%
[[ 145.  134.]
 [ 203. 2222.]]
0.8753698224852071
Training: Epoch[042/050]  Loss: 0.0895 Acc:98.95%
Training: Epoch[043/050]  Loss: 0.1594 Acc:98.95%
[[ 146.  133.]
 [ 208. 2217.]]
0.8738905325443787
Training: Epoch[044/050]  Loss: 0.0925 Acc:98.85%
Training: Epoch[045/050]  Loss: 0.1458 Acc:98.95%
[[ 146.  133.]
 [ 207. 2218.]]
0.8742603550295858
Training: Epoch[046/050]  Loss: 0.0902 Acc:98.85%
Training: Epoch[047/050]  Loss: 0.1321 Acc:98.95%
[[ 145.  134.]
 [ 205. 2220.]]
0.8746301775147929
Training: Epoch[048/050]  Loss: 0.0871 Acc:99.04%
Training: Epoch[049/050]  Loss: 0.1228 Acc:98.95%
[[ 144.  135.]
 [ 199. 2226.]]
0.8764792899408284
Training: Epoch[050/050]  Loss: 0.0848 Acc:99.23%
Training: Epoch[001/050]  Loss: 3.9560 Acc:64.33%
[[   0.  279.]
 [   0. 2425.]]
0.896819526627219
Training: Epoch[002/050]  Loss: 1.9783 Acc:88.88%
Training: Epoch[003/050]  Loss: 1.8720 Acc:88.88%
[[   0.  279.]
 [   0. 2425.]]
0.896819526627219
Training: Epoch[004/050]  Loss: 1.8092 Acc:88.88%
Training: Epoch[005/050]  Loss: 1.7389 Acc:88.88%
[[   0.  279.]
 [   0. 2425.]]
0.896819526627219
Training: Epoch[006/050]  Loss: 1.6474 Acc:88.88%
Training: Epoch[007/050]  Loss: 1.5424 Acc:88.88%
[[   0.  279.]
 [   0. 2425.]]
0.896819526627219
Training: Epoch[008/050]  Loss: 1.4348 Acc:88.88%
Training: Epoch[009/050]  Loss: 1.3110 Acc:88.88%
[[   0.  279.]
 [   0. 2425.]]
0.896819526627219
Training: Epoch[010/050]  Loss: 1.1885 Acc:88.88%
Training: Epoch[011/050]  Loss: 1.0427 Acc:88.88%
[[   0.  279.]
 [   0. 2425.]]
0.896819526627219
Training: Epoch[012/050]  Loss: 0.8898 Acc:89.55%
Training: Epoch[013/050]  Loss: 0.7255 Acc:93.19%
[[1.000e+00 2.780e+02]
 [1.000e+00 2.424e+03]]
0.896819526627219
Training: Epoch[014/050]  Loss: 0.5966 Acc:95.21%
Training: Epoch[015/050]  Loss: 0.4371 Acc:97.03%
[[  31.  248.]
 [  19. 2406.]]
0.9012573964497042
Training: Epoch[016/050]  Loss: 0.3581 Acc:97.60%
Training: Epoch[017/050]  Loss: 0.2612 Acc:98.47%
[[  58.  221.]
 [  68. 2357.]]
0.8931213017751479
Training: Epoch[018/050]  Loss: 0.1913 Acc:98.95%
Training: Epoch[019/050]  Loss: 0.1570 Acc:98.95%
[[  81.  198.]
 [  89. 2336.]]
0.8938609467455622
Training: Epoch[020/050]  Loss: 0.1406 Acc:99.04%
Training: Epoch[021/050]  Loss: 0.1353 Acc:99.04%
[[  81.  198.]
 [  89. 2336.]]
0.8938609467455622
Training: Epoch[022/050]  Loss: 0.1500 Acc:98.95%
Training: Epoch[023/050]  Loss: 0.2195 Acc:98.37%
[[  41.  238.]
 [  36. 2389.]]
0.8986686390532544
Training: Epoch[024/050]  Loss: 0.3305 Acc:97.51%
Training: Epoch[025/050]  Loss: 0.2747 Acc:97.99%
[[  81.  198.]
 [  91. 2334.]]
0.8931213017751479
Training: Epoch[026/050]  Loss: 0.1131 Acc:99.14%
Training: Epoch[027/050]  Loss: 0.0798 Acc:99.52%
[[  99.  180.]
 [ 110. 2315.]]
0.8927514792899408
Training: Epoch[028/050]  Loss: 0.0951 Acc:99.23%
Training: Epoch[029/050]  Loss: 0.0767 Acc:99.52%
[[  98.  181.]
 [ 106. 2319.]]
0.8938609467455622
Training: Epoch[030/050]  Loss: 0.1026 Acc:99.23%
Training: Epoch[031/050]  Loss: 0.0772 Acc:99.33%
[[ 122.  157.]
 [ 140. 2285.]]
0.8901627218934911
Training: Epoch[032/050]  Loss: 0.0759 Acc:99.42%
Training: Epoch[033/050]  Loss: 0.0765 Acc:99.33%
[[ 122.  157.]
 [ 141. 2284.]]
0.889792899408284
Training: Epoch[034/050]  Loss: 0.0757 Acc:99.42%
Training: Epoch[035/050]  Loss: 0.0810 Acc:99.33%
[[ 102.  177.]
 [ 108. 2317.]]
0.8946005917159763
Training: Epoch[036/050]  Loss: 0.1622 Acc:98.56%
Training: Epoch[037/050]  Loss: 0.2688 Acc:98.08%
[[  70.  209.]
 [  74. 2351.]]
0.8953402366863905
Training: Epoch[038/050]  Loss: 0.1576 Acc:98.75%
Training: Epoch[039/050]  Loss: 0.0709 Acc:99.62%
[[ 110.  169.]
 [ 120. 2305.]]
0.8931213017751479
Training: Epoch[040/050]  Loss: 0.0838 Acc:99.33%
Training: Epoch[041/050]  Loss: 0.0782 Acc:99.33%
[[  61.  218.]
 [  69. 2356.]]
0.8938609467455622
Training: Epoch[042/050]  Loss: 0.1648 Acc:98.85%
Training: Epoch[043/050]  Loss: 0.0745 Acc:99.33%
[[  77.  202.]
 [  87. 2338.]]
0.8931213017751479
Training: Epoch[044/050]  Loss: 0.1240 Acc:99.14%
Training: Epoch[045/050]  Loss: 0.0763 Acc:99.33%
[[  69.  210.]
 [  76. 2349.]]
0.8942307692307693
Training: Epoch[046/050]  Loss: 0.1416 Acc:98.95%
Training: Epoch[047/050]  Loss: 0.0770 Acc:99.33%
[[  69.  210.]
 [  77. 2348.]]
0.8938609467455622
Training: Epoch[048/050]  Loss: 0.1392 Acc:98.95%
Training: Epoch[049/050]  Loss: 0.0790 Acc:99.33%
[[  67.  212.]
 [  71. 2354.]]
0.8953402366863905
Training: Epoch[050/050]  Loss: 0.1416 Acc:98.95%
training 1D-CNN classifier for class3
Training: Epoch[001/050]  Loss: 3.4431 Acc:65.66%
[[   0.  188.]
 [   0. 2122.]]
0.9186147186147187
Training: Epoch[002/050]  Loss: 2.3544 Acc:82.34%
Training: Epoch[003/050]  Loss: 2.4121 Acc:82.34%
[[   0.  188.]
 [   0. 2122.]]
0.9186147186147187
Training: Epoch[004/050]  Loss: 2.2068 Acc:82.34%
Training: Epoch[005/050]  Loss: 2.1207 Acc:82.34%
[[   0.  188.]
 [   0. 2122.]]
0.9186147186147187
Training: Epoch[006/050]  Loss: 2.0449 Acc:82.34%
Training: Epoch[007/050]  Loss: 2.0219 Acc:82.34%
[[   0.  188.]
 [   0. 2122.]]
0.9186147186147187
Training: Epoch[008/050]  Loss: 1.9475 Acc:82.34%
Training: Epoch[009/050]  Loss: 1.8569 Acc:82.34%
[[   0.  188.]
 [   0. 2122.]]
0.9186147186147187
Training: Epoch[010/050]  Loss: 2.6046 Acc:82.34%
Training: Epoch[011/050]  Loss: 1.9831 Acc:82.34%
[[   0.  188.]
 [   0. 2122.]]
0.9186147186147187
Training: Epoch[012/050]  Loss: 1.7426 Acc:82.34%
Training: Epoch[013/050]  Loss: 1.6544 Acc:82.34%
[[   0.  188.]
 [   0. 2122.]]
0.9186147186147187
Training: Epoch[014/050]  Loss: 1.5347 Acc:82.34%
Training: Epoch[015/050]  Loss: 1.4211 Acc:82.34%
[[   0.  188.]
 [   0. 2122.]]
0.9186147186147187
Training: Epoch[016/050]  Loss: 1.4484 Acc:82.34%
Training: Epoch[017/050]  Loss: 1.2048 Acc:82.34%
[[   0.  188.]
 [   0. 2122.]]
0.9186147186147187
Training: Epoch[018/050]  Loss: 1.0852 Acc:85.92%
Training: Epoch[019/050]  Loss: 0.9559 Acc:89.92%
[[   0.  188.]
 [   0. 2122.]]
0.9186147186147187
Training: Epoch[020/050]  Loss: 0.8501 Acc:93.07%
Training: Epoch[021/050]  Loss: 0.7556 Acc:93.82%
[[   0.  188.]
 [   5. 2117.]]
0.9164502164502164
Training: Epoch[022/050]  Loss: 0.6276 Acc:95.02%
Training: Epoch[023/050]  Loss: 0.5191 Acc:95.88%
[[   4.  184.]
 [  23. 2099.]]
0.9103896103896104
Training: Epoch[024/050]  Loss: 0.4377 Acc:96.64%
Training: Epoch[025/050]  Loss: 0.3705 Acc:96.97%
[[  21.  167.]
 [  43. 2079.]]
0.9090909090909091
Training: Epoch[026/050]  Loss: 0.3163 Acc:97.18%
Training: Epoch[027/050]  Loss: 0.2756 Acc:97.83%
[[  39.  149.]
 [  88. 2034.]]
0.8974025974025974
Training: Epoch[028/050]  Loss: 0.2461 Acc:98.37%
Training: Epoch[029/050]  Loss: 0.2253 Acc:98.37%
[[  48.  140.]
 [ 125. 1997.]]
0.8852813852813853
Training: Epoch[030/050]  Loss: 0.2104 Acc:98.59%
Training: Epoch[031/050]  Loss: 0.2005 Acc:98.81%
[[  58.  130.]
 [ 147. 1975.]]
0.8800865800865801
Training: Epoch[032/050]  Loss: 0.1947 Acc:98.81%
Training: Epoch[033/050]  Loss: 0.1942 Acc:98.81%
[[  57.  131.]
 [ 145. 1977.]]
0.8805194805194805
Training: Epoch[034/050]  Loss: 0.2043 Acc:98.81%
Training: Epoch[035/050]  Loss: 0.2522 Acc:98.27%
[[  20.  168.]
 [  45. 2077.]]
0.9077922077922078
Training: Epoch[036/050]  Loss: 0.3754 Acc:96.75%
Training: Epoch[037/050]  Loss: 0.4431 Acc:96.42%
[[  19.  169.]
 [  43. 2079.]]
0.9082251082251083
Training: Epoch[038/050]  Loss: 0.3080 Acc:97.29%
Training: Epoch[039/050]  Loss: 0.1712 Acc:98.81%
[[  81.  107.]
 [ 256. 1866.]]
0.8428571428571429
Training: Epoch[040/050]  Loss: 0.1638 Acc:98.70%
Training: Epoch[041/050]  Loss: 0.1679 Acc:98.81%
[[  80.  108.]
 [ 246. 1876.]]
0.8467532467532467
Training: Epoch[042/050]  Loss: 0.1625 Acc:98.70%
Training: Epoch[043/050]  Loss: 0.1668 Acc:98.70%
[[  79.  109.]
 [ 239. 1883.]]
0.8493506493506493
Training: Epoch[044/050]  Loss: 0.1613 Acc:98.59%
Training: Epoch[045/050]  Loss: 0.1670 Acc:98.70%
[[  74.  114.]
 [ 226. 1896.]]
0.8528138528138528
Training: Epoch[046/050]  Loss: 0.1598 Acc:98.70%
Training: Epoch[047/050]  Loss: 0.1627 Acc:98.70%
[[  74.  114.]
 [ 209. 1913.]]
0.8601731601731601
Training: Epoch[048/050]  Loss: 0.1595 Acc:98.70%
Training: Epoch[049/050]  Loss: 0.1590 Acc:98.70%
[[  70.  118.]
 [ 199. 1923.]]
0.8627705627705627
Training: Epoch[050/050]  Loss: 0.1612 Acc:98.70%
Training: Epoch[001/050]  Loss: 5.5741 Acc:62.19%
[[   0.  188.]
 [   0. 2122.]]
0.9186147186147187
Training: Epoch[002/050]  Loss: 2.6474 Acc:82.34%
Training: Epoch[003/050]  Loss: 2.4464 Acc:82.34%
[[   0.  188.]
 [   0. 2122.]]
0.9186147186147187
Training: Epoch[004/050]  Loss: 2.3717 Acc:82.34%
Training: Epoch[005/050]  Loss: 2.2204 Acc:82.34%
[[   0.  188.]
 [   0. 2122.]]
0.9186147186147187
Training: Epoch[006/050]  Loss: 2.1383 Acc:82.34%
Training: Epoch[007/050]  Loss: 2.0849 Acc:82.34%
[[   0.  188.]
 [   0. 2122.]]
0.9186147186147187
Training: Epoch[008/050]  Loss: 1.9783 Acc:82.34%
Training: Epoch[009/050]  Loss: 1.9303 Acc:82.34%
[[   0.  188.]
 [   0. 2122.]]
0.9186147186147187
Training: Epoch[010/050]  Loss: 1.8326 Acc:82.34%
Training: Epoch[011/050]  Loss: 1.6386 Acc:82.34%
[[   0.  188.]
 [   0. 2122.]]
0.9186147186147187
Training: Epoch[012/050]  Loss: 1.4662 Acc:82.34%
Training: Epoch[013/050]  Loss: 1.2957 Acc:82.34%
[[   0.  188.]
 [   0. 2122.]]
0.9186147186147187
Training: Epoch[014/050]  Loss: 1.0955 Acc:87.65%
Training: Epoch[015/050]  Loss: 0.8903 Acc:92.63%
[[0.000e+00 1.880e+02]
 [1.000e+00 2.121e+03]]
0.9181818181818182
Training: Epoch[016/050]  Loss: 0.7274 Acc:94.58%
Training: Epoch[017/050]  Loss: 0.5752 Acc:95.34%
[[   3.  185.]
 [  14. 2108.]]
0.9138528138528138
Training: Epoch[018/050]  Loss: 0.4748 Acc:96.42%
Training: Epoch[019/050]  Loss: 0.3843 Acc:96.86%
[[  19.  169.]
 [  49. 2073.]]
0.9056277056277057
Training: Epoch[020/050]  Loss: 0.3210 Acc:97.29%
Training: Epoch[021/050]  Loss: 0.2813 Acc:97.62%
[[  38.  150.]
 [  88. 2034.]]
0.896969696969697
Training: Epoch[022/050]  Loss: 0.2558 Acc:98.37%
Training: Epoch[023/050]  Loss: 0.2380 Acc:98.37%
[[  46.  142.]
 [ 114. 2008.]]
0.8891774891774892
Training: Epoch[024/050]  Loss: 0.2268 Acc:98.59%
Training: Epoch[025/050]  Loss: 0.2210 Acc:98.59%
[[  48.  140.]
 [ 123. 1999.]]
0.8861471861471861
Training: Epoch[026/050]  Loss: 0.2213 Acc:98.59%
Training: Epoch[027/050]  Loss: 0.2294 Acc:98.59%
[[  42.  146.]
 [ 106. 2016.]]
0.8909090909090909
Training: Epoch[028/050]  Loss: 0.2504 Acc:98.48%
Training: Epoch[029/050]  Loss: 0.2839 Acc:98.27%
[[  31.  157.]
 [  67. 2055.]]
0.9030303030303031
Training: Epoch[030/050]  Loss: 0.3002 Acc:98.05%
Training: Epoch[031/050]  Loss: 0.2587 Acc:98.37%
[[  59.  129.]
 [ 142. 1980.]]
0.8826839826839827
Training: Epoch[032/050]  Loss: 0.1876 Acc:98.92%
Training: Epoch[033/050]  Loss: 0.1631 Acc:98.59%
[[  68.  120.]
 [ 190. 1932.]]
0.8658008658008658
Training: Epoch[034/050]  Loss: 0.1652 Acc:98.92%
Training: Epoch[035/050]  Loss: 0.1609 Acc:98.59%
[[  68.  120.]
 [ 193. 1929.]]
0.8645021645021645
Training: Epoch[036/050]  Loss: 0.1622 Acc:98.81%
Training: Epoch[037/050]  Loss: 0.1595 Acc:98.81%
[[  67.  121.]
 [ 192. 1930.]]
0.8645021645021645
Training: Epoch[038/050]  Loss: 0.1599 Acc:98.81%
Training: Epoch[039/050]  Loss: 0.1618 Acc:98.92%
[[  62.  126.]
 [ 166. 1956.]]
0.8735930735930736
Training: Epoch[040/050]  Loss: 0.1780 Acc:98.92%
Training: Epoch[041/050]  Loss: 0.2608 Acc:98.48%
[[  26.  162.]
 [  52. 2070.]]
0.9073593073593074
Training: Epoch[042/050]  Loss: 0.3259 Acc:97.72%
Training: Epoch[043/050]  Loss: 0.2459 Acc:98.27%
[[  66.  122.]
 [ 178. 1944.]]
0.8701298701298701
Training: Epoch[044/050]  Loss: 0.1586 Acc:98.92%
Training: Epoch[045/050]  Loss: 0.1544 Acc:98.70%
[[  64.  124.]
 [ 176. 1946.]]
0.8701298701298701
Training: Epoch[046/050]  Loss: 0.1566 Acc:98.92%
Training: Epoch[047/050]  Loss: 0.1510 Acc:98.70%
[[  64.  124.]
 [ 177. 1945.]]
0.8696969696969697
Training: Epoch[048/050]  Loss: 0.1541 Acc:98.92%
Training: Epoch[049/050]  Loss: 0.1481 Acc:98.92%
[[  65.  123.]
 [ 179. 1943.]]
0.8692640692640693
Training: Epoch[050/050]  Loss: 0.1505 Acc:98.81%
training 1D-CNN classifier for class4
Training: Epoch[001/050]  Loss: 4.0825 Acc:49.28%
[[   0.   28.]
 [   0. 2147.]]
0.9871264367816092
Training: Epoch[002/050]  Loss: 2.6913 Acc:77.29%
Training: Epoch[003/050]  Loss: 2.7759 Acc:77.29%
[[   0.   28.]
 [   0. 2147.]]
0.9871264367816092
Training: Epoch[004/050]  Loss: 2.6022 Acc:77.29%
Training: Epoch[005/050]  Loss: 2.5315 Acc:77.29%
[[   0.   28.]
 [   0. 2147.]]
0.9871264367816092
Training: Epoch[006/050]  Loss: 2.3862 Acc:77.29%
Training: Epoch[007/050]  Loss: 2.3001 Acc:77.29%
[[   0.   28.]
 [   0. 2147.]]
0.9871264367816092
Training: Epoch[008/050]  Loss: 2.0969 Acc:77.29%
Training: Epoch[009/050]  Loss: 2.1618 Acc:77.29%
[[   0.   28.]
 [   0. 2147.]]
0.9871264367816092
Training: Epoch[010/050]  Loss: 1.7089 Acc:77.29%
Training: Epoch[011/050]  Loss: 1.5840 Acc:77.29%
[[   0.   28.]
 [   0. 2147.]]
0.9871264367816092
Training: Epoch[012/050]  Loss: 1.3303 Acc:77.18%
Training: Epoch[013/050]  Loss: 1.1373 Acc:85.23%
[[   0.   28.]
 [   0. 2147.]]
0.9871264367816092
Training: Epoch[014/050]  Loss: 0.8987 Acc:91.73%
Training: Epoch[015/050]  Loss: 0.6968 Acc:95.15%
[[   0.   28.]
 [   0. 2147.]]
0.9871264367816092
Training: Epoch[016/050]  Loss: 0.5210 Acc:96.58%
Training: Epoch[017/050]  Loss: 0.3911 Acc:97.68%
[[   0.   28.]
 [   0. 2147.]]
0.9871264367816092
Training: Epoch[018/050]  Loss: 0.2998 Acc:98.13%
Training: Epoch[019/050]  Loss: 0.2387 Acc:98.79%
[[   0.   28.]
 [   0. 2147.]]
0.9871264367816092
Training: Epoch[020/050]  Loss: 0.1984 Acc:98.90%
Training: Epoch[021/050]  Loss: 0.1712 Acc:98.90%
[[   0.   28.]
 [   0. 2147.]]
0.9871264367816092
Training: Epoch[022/050]  Loss: 0.1520 Acc:98.79%
Training: Epoch[023/050]  Loss: 0.1381 Acc:98.90%
[[   0.   28.]
 [   0. 2147.]]
0.9871264367816092
Training: Epoch[024/050]  Loss: 0.1277 Acc:99.23%
Training: Epoch[025/050]  Loss: 0.1198 Acc:99.23%
[[   0.   28.]
 [   0. 2147.]]
0.9871264367816092
Training: Epoch[026/050]  Loss: 0.1138 Acc:99.23%
Training: Epoch[027/050]  Loss: 0.1093 Acc:99.23%
[[   0.   28.]
 [   0. 2147.]]
0.9871264367816092
Training: Epoch[028/050]  Loss: 0.1062 Acc:99.23%
Training: Epoch[029/050]  Loss: 0.1048 Acc:99.23%
[[   0.   28.]
 [   0. 2147.]]
0.9871264367816092
Training: Epoch[030/050]  Loss: 0.1056 Acc:99.23%
Training: Epoch[031/050]  Loss: 0.1113 Acc:99.23%
[[   0.   28.]
 [   0. 2147.]]
0.9871264367816092
Training: Epoch[032/050]  Loss: 0.1287 Acc:98.68%
Training: Epoch[033/050]  Loss: 0.1652 Acc:98.68%
[[   0.   28.]
 [   0. 2147.]]
0.9871264367816092
Training: Epoch[034/050]  Loss: 0.1838 Acc:98.35%
Training: Epoch[035/050]  Loss: 0.1226 Acc:98.90%
[[0.000e+00 2.800e+01]
 [1.000e+00 2.146e+03]]
0.9866666666666667
Training: Epoch[036/050]  Loss: 0.0819 Acc:99.23%
Training: Epoch[037/050]  Loss: 0.0810 Acc:99.12%
[[0.000e+00 2.800e+01]
 [1.000e+00 2.146e+03]]
0.9866666666666667
Training: Epoch[038/050]  Loss: 0.0810 Acc:99.23%
Training: Epoch[039/050]  Loss: 0.0804 Acc:99.12%
[[0.000e+00 2.800e+01]
 [1.000e+00 2.146e+03]]
0.9866666666666667
Training: Epoch[040/050]  Loss: 0.0804 Acc:99.23%
Training: Epoch[041/050]  Loss: 0.0798 Acc:99.12%
[[0.000e+00 2.800e+01]
 [1.000e+00 2.146e+03]]
0.9866666666666667
Training: Epoch[042/050]  Loss: 0.0799 Acc:99.23%
Training: Epoch[043/050]  Loss: 0.0792 Acc:99.12%
[[0.000e+00 2.800e+01]
 [1.000e+00 2.146e+03]]
0.9866666666666667
Training: Epoch[044/050]  Loss: 0.0795 Acc:99.23%
Training: Epoch[045/050]  Loss: 0.0786 Acc:99.12%
[[0.000e+00 2.800e+01]
 [2.000e+00 2.145e+03]]
0.9862068965517241
Training: Epoch[046/050]  Loss: 0.0791 Acc:99.23%
Training: Epoch[047/050]  Loss: 0.0779 Acc:99.23%
[[0.000e+00 2.800e+01]
 [2.000e+00 2.145e+03]]
0.9862068965517241
Training: Epoch[048/050]  Loss: 0.0784 Acc:99.23%
Training: Epoch[049/050]  Loss: 0.0779 Acc:99.23%
[[0.000e+00 2.800e+01]
 [2.000e+00 2.145e+03]]
0.9862068965517241
Training: Epoch[050/050]  Loss: 0.0779 Acc:99.23%
Training: Epoch[001/050]  Loss: 5.5706 Acc:66.59%
[[   0.   28.]
 [   0. 2147.]]
0.9871264367816092
Training: Epoch[002/050]  Loss: 3.6862 Acc:49.28%
Training: Epoch[003/050]  Loss: 2.7966 Acc:77.18%
[[   0.   28.]
 [   0. 2147.]]
0.9871264367816092
Training: Epoch[004/050]  Loss: 2.4059 Acc:77.29%
Training: Epoch[005/050]  Loss: 2.8186 Acc:76.74%
[[   0.   28.]
 [   0. 2147.]]
0.9871264367816092
Training: Epoch[006/050]  Loss: 2.1655 Acc:77.07%
Training: Epoch[007/050]  Loss: 1.6295 Acc:77.07%
[[   0.   28.]
 [   0. 2147.]]
0.9871264367816092
Training: Epoch[008/050]  Loss: 1.5839 Acc:75.52%
Training: Epoch[009/050]  Loss: 1.1054 Acc:87.98%
[[   0.   28.]
 [   0. 2147.]]
0.9871264367816092
Training: Epoch[010/050]  Loss: 0.8563 Acc:93.05%
Training: Epoch[011/050]  Loss: 0.6444 Acc:95.37%
[[   0.   28.]
 [   0. 2147.]]
0.9871264367816092
Training: Epoch[012/050]  Loss: 0.5378 Acc:96.25%
Training: Epoch[013/050]  Loss: 0.3609 Acc:97.57%
[[   0.   28.]
 [   0. 2147.]]
0.9871264367816092
Training: Epoch[014/050]  Loss: 0.2550 Acc:98.35%
Training: Epoch[015/050]  Loss: 0.2055 Acc:98.79%
[[   0.   28.]
 [   0. 2147.]]
0.9871264367816092
Training: Epoch[016/050]  Loss: 0.1752 Acc:99.01%
Training: Epoch[017/050]  Loss: 0.1545 Acc:99.23%
[[   0.   28.]
 [   0. 2147.]]
0.9871264367816092
Training: Epoch[018/050]  Loss: 0.1403 Acc:99.23%
Training: Epoch[019/050]  Loss: 0.1306 Acc:99.23%
[[   0.   28.]
 [   0. 2147.]]
0.9871264367816092
Training: Epoch[020/050]  Loss: 0.1242 Acc:99.23%
Training: Epoch[021/050]  Loss: 0.1209 Acc:99.23%
[[   0.   28.]
 [   0. 2147.]]
0.9871264367816092
Training: Epoch[022/050]  Loss: 0.1216 Acc:99.23%
Training: Epoch[023/050]  Loss: 0.1301 Acc:99.12%
[[   0.   28.]
 [   0. 2147.]]
0.9871264367816092
Training: Epoch[024/050]  Loss: 0.1582 Acc:98.68%
Training: Epoch[025/050]  Loss: 0.2264 Acc:97.91%
[[   0.   28.]
 [   0. 2147.]]
0.9871264367816092
Training: Epoch[026/050]  Loss: 0.3006 Acc:97.13%
Training: Epoch[027/050]  Loss: 0.2160 Acc:97.68%
[[   0.   28.]
 [   0. 2147.]]
0.9871264367816092
Training: Epoch[028/050]  Loss: 0.0854 Acc:99.12%
Training: Epoch[029/050]  Loss: 0.0803 Acc:99.23%
[[0.000e+00 2.800e+01]
 [2.000e+00 2.145e+03]]
0.9862068965517241
Training: Epoch[030/050]  Loss: 0.0803 Acc:99.23%
Training: Epoch[031/050]  Loss: 0.0798 Acc:99.23%
[[0.000e+00 2.800e+01]
 [2.000e+00 2.145e+03]]
0.9862068965517241
Training: Epoch[032/050]  Loss: 0.0796 Acc:99.23%
Training: Epoch[033/050]  Loss: 0.0792 Acc:99.23%
[[0.000e+00 2.800e+01]
 [2.000e+00 2.145e+03]]
0.9862068965517241
Training: Epoch[034/050]  Loss: 0.0790 Acc:99.23%
Training: Epoch[035/050]  Loss: 0.0787 Acc:99.23%
[[0.000e+00 2.800e+01]
 [2.000e+00 2.145e+03]]
0.9862068965517241
Training: Epoch[036/050]  Loss: 0.0784 Acc:99.23%
Training: Epoch[037/050]  Loss: 0.0781 Acc:99.23%
[[0.000e+00 2.800e+01]
 [2.000e+00 2.145e+03]]
0.9862068965517241
Training: Epoch[038/050]  Loss: 0.0778 Acc:99.23%
Training: Epoch[039/050]  Loss: 0.0775 Acc:99.23%
[[0.000e+00 2.800e+01]
 [2.000e+00 2.145e+03]]
0.9862068965517241
Training: Epoch[040/050]  Loss: 0.0773 Acc:99.23%
Training: Epoch[041/050]  Loss: 0.0771 Acc:99.23%
[[0.000e+00 2.800e+01]
 [2.000e+00 2.145e+03]]
0.9862068965517241
Training: Epoch[042/050]  Loss: 0.0771 Acc:99.23%
Training: Epoch[043/050]  Loss: 0.0771 Acc:99.23%
[[0.000e+00 2.800e+01]
 [2.000e+00 2.145e+03]]
0.9862068965517241
Training: Epoch[044/050]  Loss: 0.0769 Acc:99.23%
Training: Epoch[045/050]  Loss: 0.0764 Acc:99.23%
[[0.000e+00 2.800e+01]
 [2.000e+00 2.145e+03]]
0.9862068965517241
Training: Epoch[046/050]  Loss: 0.0761 Acc:99.23%
Training: Epoch[047/050]  Loss: 0.0763 Acc:99.23%
[[0.000e+00 2.800e+01]
 [2.000e+00 2.145e+03]]
0.9862068965517241
Training: Epoch[048/050]  Loss: 0.0763 Acc:99.23%
Training: Epoch[049/050]  Loss: 0.0755 Acc:99.23%
[[0.000e+00 2.800e+01]
 [2.000e+00 2.145e+03]]
0.9862068965517241
Training: Epoch[050/050]  Loss: 0.0755 Acc:99.23%
training 1D-CNN classifier for class5
Training: Epoch[001/050]  Loss: 3.5352 Acc:55.39%
[[   0.  187.]
 [   0. 2294.]]
0.924627166465135
Training: Epoch[002/050]  Loss: 2.8436 Acc:80.12%
Training: Epoch[003/050]  Loss: 2.8876 Acc:80.12%
[[   0.  187.]
 [   0. 2294.]]
0.924627166465135
Training: Epoch[004/050]  Loss: 2.6972 Acc:80.12%
Training: Epoch[005/050]  Loss: 2.6128 Acc:80.12%
[[   0.  187.]
 [   0. 2294.]]
0.924627166465135
Training: Epoch[006/050]  Loss: 2.5439 Acc:80.12%
Training: Epoch[007/050]  Loss: 2.4536 Acc:80.12%
[[   0.  187.]
 [   0. 2294.]]
0.924627166465135
Training: Epoch[008/050]  Loss: 2.3838 Acc:80.12%
Training: Epoch[009/050]  Loss: 2.3400 Acc:80.12%
[[   0.  187.]
 [   0. 2294.]]
0.924627166465135
Training: Epoch[010/050]  Loss: 2.1795 Acc:80.12%
Training: Epoch[011/050]  Loss: 2.0530 Acc:80.12%
[[   0.  187.]
 [   0. 2294.]]
0.924627166465135
Training: Epoch[012/050]  Loss: 2.1705 Acc:79.92%
Training: Epoch[013/050]  Loss: 1.8622 Acc:79.03%
[[   0.  187.]
 [   0. 2294.]]
0.924627166465135
Training: Epoch[014/050]  Loss: 1.3176 Acc:81.70%
Training: Epoch[015/050]  Loss: 1.0866 Acc:90.60%
[[   0.  187.]
 [   0. 2294.]]
0.924627166465135
Training: Epoch[016/050]  Loss: 0.8546 Acc:93.77%
Training: Epoch[017/050]  Loss: 0.6092 Acc:95.94%
[[2.000e+00 1.850e+02]
 [6.000e+00 2.288e+03]]
0.9230149133413946
Training: Epoch[018/050]  Loss: 0.4428 Acc:97.33%
Training: Epoch[019/050]  Loss: 0.3294 Acc:98.22%
[[  17.  170.]
 [  24. 2270.]]
0.9218057234985892
Training: Epoch[020/050]  Loss: 0.2637 Acc:98.52%
Training: Epoch[021/050]  Loss: 0.2249 Acc:98.42%
[[  31.  156.]
 [  41. 2253.]]
0.920596533655784
Training: Epoch[022/050]  Loss: 0.2033 Acc:98.52%
Training: Epoch[023/050]  Loss: 0.1881 Acc:98.52%
[[  36.  151.]
 [  50. 2244.]]
0.9189842805320435
Training: Epoch[024/050]  Loss: 0.1788 Acc:98.52%
Training: Epoch[025/050]  Loss: 0.1731 Acc:98.62%
[[  39.  148.]
 [  53. 2241.]]
0.9189842805320435
Training: Epoch[026/050]  Loss: 0.1709 Acc:98.62%
Training: Epoch[027/050]  Loss: 0.1710 Acc:98.62%
[[  36.  151.]
 [  51. 2243.]]
0.9185812172511084
Training: Epoch[028/050]  Loss: 0.1796 Acc:98.52%
Training: Epoch[029/050]  Loss: 0.2172 Acc:98.32%
[[  13.  174.]
 [  16. 2278.]]
0.9234179766223297
Training: Epoch[030/050]  Loss: 0.3000 Acc:97.82%
Training: Epoch[031/050]  Loss: 0.3226 Acc:97.82%
[[  20.  167.]
 [  29. 2265.]]
0.9209995969367191
Training: Epoch[032/050]  Loss: 0.2081 Acc:98.52%
Training: Epoch[033/050]  Loss: 0.1495 Acc:98.81%
[[  54.  133.]
 [  83. 2211.]]
0.9129383313180169
Training: Epoch[034/050]  Loss: 0.1489 Acc:98.81%
Training: Epoch[035/050]  Loss: 0.1484 Acc:98.81%
[[  54.  133.]
 [  82. 2212.]]
0.913341394598952
Training: Epoch[036/050]  Loss: 0.1479 Acc:98.81%
Training: Epoch[037/050]  Loss: 0.1473 Acc:98.81%
[[  54.  133.]
 [  82. 2212.]]
0.913341394598952
Training: Epoch[038/050]  Loss: 0.1468 Acc:98.81%
Training: Epoch[039/050]  Loss: 0.1461 Acc:98.81%
[[  54.  133.]
 [  81. 2213.]]
0.9137444578798871
Training: Epoch[040/050]  Loss: 0.1455 Acc:98.81%
Training: Epoch[041/050]  Loss: 0.1449 Acc:98.81%
[[  52.  135.]
 [  78. 2216.]]
0.9141475211608222
Training: Epoch[042/050]  Loss: 0.1450 Acc:98.81%
Training: Epoch[043/050]  Loss: 0.1600 Acc:99.01%
[[  23.  164.]
 [  31. 2263.]]
0.9214026602176542
Training: Epoch[044/050]  Loss: 0.2384 Acc:98.22%
Training: Epoch[045/050]  Loss: 0.2616 Acc:98.02%
[[  25.  162.]
 [  37. 2257.]]
0.9197904070939138
Training: Epoch[046/050]  Loss: 0.1841 Acc:98.52%
Training: Epoch[047/050]  Loss: 0.1449 Acc:98.71%
[[  54.  133.]
 [  86. 2208.]]
0.9117291414752116
Training: Epoch[048/050]  Loss: 0.1427 Acc:98.81%
Training: Epoch[049/050]  Loss: 0.1420 Acc:98.71%
[[  54.  133.]
 [  82. 2212.]]
0.913341394598952
Training: Epoch[050/050]  Loss: 0.1404 Acc:98.81%
Training: Epoch[001/050]  Loss: 4.6458 Acc:55.09%
[[   0.  187.]
 [   0. 2294.]]
0.924627166465135
Training: Epoch[002/050]  Loss: 2.9960 Acc:80.12%
Training: Epoch[003/050]  Loss: 2.9041 Acc:80.12%
[[   0.  187.]
 [   0. 2294.]]
0.924627166465135
Training: Epoch[004/050]  Loss: 2.7171 Acc:80.12%
Training: Epoch[005/050]  Loss: 2.4980 Acc:80.12%
[[   0.  187.]
 [   0. 2294.]]
0.924627166465135
Training: Epoch[006/050]  Loss: 2.5612 Acc:80.12%
Training: Epoch[007/050]  Loss: 2.3719 Acc:80.12%
[[   0.  187.]
 [   0. 2294.]]
0.924627166465135
Training: Epoch[008/050]  Loss: 2.3997 Acc:80.12%
Training: Epoch[009/050]  Loss: 2.1693 Acc:80.12%
[[   0.  187.]
 [   0. 2294.]]
0.924627166465135
Training: Epoch[010/050]  Loss: 2.2379 Acc:80.12%
Training: Epoch[011/050]  Loss: 1.8463 Acc:80.02%
[[   0.  187.]
 [   0. 2294.]]
0.924627166465135
Training: Epoch[012/050]  Loss: 1.6878 Acc:78.93%
Training: Epoch[013/050]  Loss: 1.3893 Acc:82.99%
[[   0.  187.]
 [   0. 2294.]]
0.924627166465135
Training: Epoch[014/050]  Loss: 1.0606 Acc:91.20%
Training: Epoch[015/050]  Loss: 0.7596 Acc:94.86%
[[1.000e+00 1.860e+02]
 [3.000e+00 2.291e+03]]
0.9238210399032648
Training: Epoch[016/050]  Loss: 0.5236 Acc:97.03%
Training: Epoch[017/050]  Loss: 0.3867 Acc:97.73%
[[   6.  181.]
 [  11. 2283.]]
0.9226118500604595
Training: Epoch[018/050]  Loss: 0.3038 Acc:98.12%
Training: Epoch[019/050]  Loss: 0.2577 Acc:98.32%
[[  16.  171.]
 [  23. 2271.]]
0.9218057234985892
Training: Epoch[020/050]  Loss: 0.2278 Acc:98.42%
Training: Epoch[021/050]  Loss: 0.2087 Acc:98.52%
[[  24.  163.]
 [  35. 2259.]]
0.9201934703748489
Training: Epoch[022/050]  Loss: 0.1963 Acc:98.62%
Training: Epoch[023/050]  Loss: 0.1885 Acc:98.52%
[[  28.  159.]
 [  42. 2252.]]
0.9189842805320435
Training: Epoch[024/050]  Loss: 0.1839 Acc:98.52%
Training: Epoch[025/050]  Loss: 0.1830 Acc:98.52%
[[  28.  159.]
 [  41. 2253.]]
0.9193873438129786
Training: Epoch[026/050]  Loss: 0.1866 Acc:98.52%
Training: Epoch[027/050]  Loss: 0.1985 Acc:98.62%
[[  18.  169.]
 [  28. 2266.]]
0.920596533655784
Training: Epoch[028/050]  Loss: 0.2262 Acc:98.22%
Training: Epoch[029/050]  Loss: 0.2685 Acc:97.92%
[[  10.  177.]
 [  14. 2280.]]
0.9230149133413946
Training: Epoch[030/050]  Loss: 0.2871 Acc:97.92%
Training: Epoch[031/050]  Loss: 0.2326 Acc:98.12%
[[  32.  155.]
 [  52. 2242.]]
0.9165659008464329
Training: Epoch[032/050]  Loss: 0.1604 Acc:98.62%
Training: Epoch[033/050]  Loss: 0.1507 Acc:98.62%
[[  44.  143.]
 [  69. 2225.]]
0.9145505844417574
Training: Epoch[034/050]  Loss: 0.1507 Acc:98.81%
Training: Epoch[035/050]  Loss: 0.1494 Acc:98.71%
[[  47.  140.]
 [  72. 2222.]]
0.9145505844417574
Training: Epoch[036/050]  Loss: 0.1493 Acc:98.81%
Training: Epoch[037/050]  Loss: 0.1486 Acc:98.81%
[[  48.  139.]
 [  73. 2221.]]
0.9145505844417574
Training: Epoch[038/050]  Loss: 0.1483 Acc:98.81%
Training: Epoch[039/050]  Loss: 0.1479 Acc:98.81%
[[  48.  139.]
 [  71. 2223.]]
0.9153567110036276
Training: Epoch[040/050]  Loss: 0.1477 Acc:98.81%
Training: Epoch[041/050]  Loss: 0.1489 Acc:98.81%
[[  40.  147.]
 [  58. 2236.]]
0.9173720274083031
Training: Epoch[042/050]  Loss: 0.1658 Acc:98.81%
Training: Epoch[043/050]  Loss: 0.2415 Acc:98.12%
[[  10.  177.]
 [  14. 2280.]]
0.9230149133413946
Training: Epoch[044/050]  Loss: 0.2847 Acc:97.82%
Training: Epoch[045/050]  Loss: 0.2227 Acc:98.42%
[[  44.  143.]
 [  62. 2232.]]
0.9173720274083031
Training: Epoch[046/050]  Loss: 0.1491 Acc:99.01%
Training: Epoch[047/050]  Loss: 0.1500 Acc:98.62%
[[  45.  142.]
 [  67. 2227.]]
0.9157597742845627
Training: Epoch[048/050]  Loss: 0.1466 Acc:98.91%
Training: Epoch[049/050]  Loss: 0.1466 Acc:98.62%
[[  47.  140.]
 [  68. 2226.]]
0.9161628375654978
Training: Epoch[050/050]  Loss: 0.1448 Acc:98.91%
training 1D-CNN classifier for class6
Training: Epoch[001/050]  Loss: 4.1911 Acc:53.35%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[002/050]  Loss: 2.9503 Acc:80.89%
Training: Epoch[003/050]  Loss: 2.3623 Acc:80.89%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[004/050]  Loss: 2.2058 Acc:80.89%
Training: Epoch[005/050]  Loss: 2.0860 Acc:80.89%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[006/050]  Loss: 1.9204 Acc:80.89%
Training: Epoch[007/050]  Loss: 1.8245 Acc:80.89%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[008/050]  Loss: 1.5575 Acc:80.89%
Training: Epoch[009/050]  Loss: 1.3277 Acc:80.89%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[010/050]  Loss: 1.0881 Acc:88.55%
Training: Epoch[011/050]  Loss: 0.8399 Acc:93.20%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[012/050]  Loss: 0.6181 Acc:95.25%
Training: Epoch[013/050]  Loss: 0.4621 Acc:96.87%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[014/050]  Loss: 0.3570 Acc:97.41%
Training: Epoch[015/050]  Loss: 0.2904 Acc:98.06%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[016/050]  Loss: 0.2491 Acc:98.06%
Training: Epoch[017/050]  Loss: 0.2218 Acc:98.27%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[018/050]  Loss: 0.2031 Acc:98.49%
Training: Epoch[019/050]  Loss: 0.1908 Acc:98.49%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[020/050]  Loss: 0.1830 Acc:98.60%
Training: Epoch[021/050]  Loss: 0.1777 Acc:98.70%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[022/050]  Loss: 0.1729 Acc:98.70%
Training: Epoch[023/050]  Loss: 0.1693 Acc:98.70%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[024/050]  Loss: 0.1651 Acc:98.70%
Training: Epoch[025/050]  Loss: 0.1594 Acc:98.81%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[026/050]  Loss: 0.1540 Acc:98.81%
Training: Epoch[027/050]  Loss: 0.1485 Acc:98.81%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[028/050]  Loss: 0.1425 Acc:98.81%
Training: Epoch[029/050]  Loss: 0.1366 Acc:98.92%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[030/050]  Loss: 0.1318 Acc:98.92%
Training: Epoch[031/050]  Loss: 0.1281 Acc:99.14%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[032/050]  Loss: 0.1259 Acc:99.24%
Training: Epoch[033/050]  Loss: 0.1247 Acc:99.24%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[034/050]  Loss: 0.1242 Acc:99.24%
Training: Epoch[035/050]  Loss: 0.1243 Acc:99.24%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[036/050]  Loss: 0.1239 Acc:99.24%
Training: Epoch[037/050]  Loss: 0.1216 Acc:99.24%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[038/050]  Loss: 0.1170 Acc:99.35%
Training: Epoch[039/050]  Loss: 0.1097 Acc:99.35%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[040/050]  Loss: 0.1014 Acc:99.35%
Training: Epoch[041/050]  Loss: 0.0942 Acc:99.35%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[042/050]  Loss: 0.0902 Acc:99.46%
Training: Epoch[043/050]  Loss: 0.0898 Acc:99.46%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[044/050]  Loss: 0.0914 Acc:99.46%
Training: Epoch[045/050]  Loss: 0.0961 Acc:99.35%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[046/050]  Loss: 0.1110 Acc:99.35%
Training: Epoch[047/050]  Loss: 0.1464 Acc:98.92%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[048/050]  Loss: 0.1717 Acc:98.60%
Training: Epoch[049/050]  Loss: 0.1419 Acc:98.81%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[050/050]  Loss: 0.0948 Acc:99.35%
Training: Epoch[001/050]  Loss: 3.5723 Acc:61.88%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[002/050]  Loss: 2.4081 Acc:80.89%
Training: Epoch[003/050]  Loss: 2.4117 Acc:80.89%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[004/050]  Loss: 2.3440 Acc:80.89%
Training: Epoch[005/050]  Loss: 2.1641 Acc:80.89%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[006/050]  Loss: 1.9964 Acc:80.89%
Training: Epoch[007/050]  Loss: 1.8022 Acc:80.89%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[008/050]  Loss: 1.5609 Acc:80.89%
Training: Epoch[009/050]  Loss: 1.2896 Acc:81.10%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[010/050]  Loss: 0.9824 Acc:91.90%
Training: Epoch[011/050]  Loss: 0.7409 Acc:94.28%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[012/050]  Loss: 0.5242 Acc:96.65%
Training: Epoch[013/050]  Loss: 0.4189 Acc:97.08%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[014/050]  Loss: 0.3426 Acc:97.95%
Training: Epoch[015/050]  Loss: 0.2892 Acc:97.95%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[016/050]  Loss: 0.2526 Acc:98.16%
Training: Epoch[017/050]  Loss: 0.2271 Acc:98.16%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[018/050]  Loss: 0.2084 Acc:98.27%
Training: Epoch[019/050]  Loss: 0.1943 Acc:98.27%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[020/050]  Loss: 0.1834 Acc:98.70%
Training: Epoch[021/050]  Loss: 0.1749 Acc:98.70%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[022/050]  Loss: 0.1679 Acc:98.70%
Training: Epoch[023/050]  Loss: 0.1619 Acc:98.81%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[024/050]  Loss: 0.1565 Acc:98.81%
Training: Epoch[025/050]  Loss: 0.1516 Acc:98.81%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[026/050]  Loss: 0.1466 Acc:98.81%
Training: Epoch[027/050]  Loss: 0.1422 Acc:98.81%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[028/050]  Loss: 0.1374 Acc:98.81%
Training: Epoch[029/050]  Loss: 0.1330 Acc:98.92%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[030/050]  Loss: 0.1288 Acc:98.92%
Training: Epoch[031/050]  Loss: 0.1247 Acc:98.92%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[032/050]  Loss: 0.1207 Acc:99.03%
Training: Epoch[033/050]  Loss: 0.1171 Acc:99.03%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[034/050]  Loss: 0.1137 Acc:99.14%
Training: Epoch[035/050]  Loss: 0.1107 Acc:99.24%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[036/050]  Loss: 0.1081 Acc:99.24%
Training: Epoch[037/050]  Loss: 0.1061 Acc:99.24%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[038/050]  Loss: 0.1044 Acc:99.24%
Training: Epoch[039/050]  Loss: 0.1033 Acc:99.24%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[040/050]  Loss: 0.1026 Acc:99.24%
Training: Epoch[041/050]  Loss: 0.1024 Acc:99.24%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[042/050]  Loss: 0.1025 Acc:99.24%
Training: Epoch[043/050]  Loss: 0.1026 Acc:99.24%
[[   0.   12.]
 [   0. 2296.]]
0.9948006932409013
Training: Epoch[044/050]  Loss: 0.1022 Acc:99.24%
Training: Epoch[045/050]  Loss: 0.1003 Acc:99.24%
[[0.000e+00 1.200e+01]
 [1.000e+00 2.295e+03]]
0.994367417677643
Training: Epoch[046/050]  Loss: 0.0965 Acc:99.35%
Training: Epoch[047/050]  Loss: 0.0907 Acc:99.46%
[[0.000e+00 1.200e+01]
 [1.000e+00 2.295e+03]]
0.994367417677643
Training: Epoch[048/050]  Loss: 0.0847 Acc:99.46%
Training: Epoch[049/050]  Loss: 0.0820 Acc:99.46%
[[0.000e+00 1.200e+01]
 [1.000e+00 2.295e+03]]
0.994367417677643
Training: Epoch[050/050]  Loss: 0.0827 Acc:99.46%
training 1D-CNN classifier for class7
Training: Epoch[001/050]  Loss: 4.0878 Acc:66.96%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[002/050]  Loss: 2.3375 Acc:83.78%
Training: Epoch[003/050]  Loss: 2.2824 Acc:83.78%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[004/050]  Loss: 2.1472 Acc:83.78%
Training: Epoch[005/050]  Loss: 2.0531 Acc:83.78%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[006/050]  Loss: 1.9354 Acc:83.78%
Training: Epoch[007/050]  Loss: 1.7896 Acc:83.78%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[008/050]  Loss: 1.5947 Acc:83.78%
Training: Epoch[009/050]  Loss: 1.3646 Acc:83.78%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[010/050]  Loss: 1.1098 Acc:84.37%
Training: Epoch[011/050]  Loss: 0.8335 Acc:93.47%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[012/050]  Loss: 0.6096 Acc:95.65%
Training: Epoch[013/050]  Loss: 0.4056 Acc:97.73%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[014/050]  Loss: 0.2806 Acc:98.42%
Training: Epoch[015/050]  Loss: 0.2095 Acc:99.01%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[016/050]  Loss: 0.1653 Acc:99.11%
Training: Epoch[017/050]  Loss: 0.1370 Acc:99.21%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[018/050]  Loss: 0.1180 Acc:99.41%
Training: Epoch[019/050]  Loss: 0.1048 Acc:99.41%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[020/050]  Loss: 0.0954 Acc:99.41%
Training: Epoch[021/050]  Loss: 0.0888 Acc:99.41%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[022/050]  Loss: 0.0842 Acc:99.51%
Training: Epoch[023/050]  Loss: 0.0816 Acc:99.51%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[024/050]  Loss: 0.0816 Acc:99.51%
Training: Epoch[025/050]  Loss: 0.0865 Acc:99.51%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[026/050]  Loss: 0.1051 Acc:99.31%
Training: Epoch[027/050]  Loss: 0.1529 Acc:99.01%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[028/050]  Loss: 0.1855 Acc:98.81%
Training: Epoch[029/050]  Loss: 0.1005 Acc:99.21%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[030/050]  Loss: 0.0586 Acc:99.41%
Training: Epoch[031/050]  Loss: 0.0631 Acc:99.51%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[032/050]  Loss: 0.0575 Acc:99.41%
Training: Epoch[033/050]  Loss: 0.0608 Acc:99.51%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[034/050]  Loss: 0.0564 Acc:99.41%
Training: Epoch[035/050]  Loss: 0.0615 Acc:99.51%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[036/050]  Loss: 0.0554 Acc:99.41%
Training: Epoch[037/050]  Loss: 0.0654 Acc:99.51%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[038/050]  Loss: 0.0568 Acc:99.51%
Training: Epoch[039/050]  Loss: 0.0548 Acc:99.41%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[040/050]  Loss: 0.0583 Acc:99.60%
Training: Epoch[041/050]  Loss: 0.0582 Acc:99.51%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[042/050]  Loss: 0.0638 Acc:99.60%
Training: Epoch[043/050]  Loss: 0.1125 Acc:99.21%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[044/050]  Loss: 0.1916 Acc:98.81%
Training: Epoch[045/050]  Loss: 0.1019 Acc:99.21%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[046/050]  Loss: 0.0597 Acc:99.51%
Training: Epoch[047/050]  Loss: 0.0688 Acc:99.51%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[048/050]  Loss: 0.0575 Acc:99.51%
Training: Epoch[049/050]  Loss: 0.0657 Acc:99.51%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[050/050]  Loss: 0.0564 Acc:99.51%
Training: Epoch[001/050]  Loss: 3.8934 Acc:58.46%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[002/050]  Loss: 2.6844 Acc:83.78%
Training: Epoch[003/050]  Loss: 2.2803 Acc:83.78%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[004/050]  Loss: 2.0573 Acc:83.78%
Training: Epoch[005/050]  Loss: 1.8733 Acc:83.78%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[006/050]  Loss: 1.6476 Acc:83.78%
Training: Epoch[007/050]  Loss: 1.3517 Acc:83.78%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[008/050]  Loss: 0.9886 Acc:86.94%
Training: Epoch[009/050]  Loss: 0.6189 Acc:95.55%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[010/050]  Loss: 0.3245 Acc:98.42%
Training: Epoch[011/050]  Loss: 0.1860 Acc:99.11%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[012/050]  Loss: 0.1326 Acc:99.41%
Training: Epoch[013/050]  Loss: 0.1078 Acc:99.41%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[014/050]  Loss: 0.0937 Acc:99.51%
Training: Epoch[015/050]  Loss: 0.0846 Acc:99.41%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[016/050]  Loss: 0.0784 Acc:99.41%
Training: Epoch[017/050]  Loss: 0.0739 Acc:99.41%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[018/050]  Loss: 0.0706 Acc:99.41%
Training: Epoch[019/050]  Loss: 0.0680 Acc:99.41%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[020/050]  Loss: 0.0661 Acc:99.41%
Training: Epoch[021/050]  Loss: 0.0648 Acc:99.41%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[022/050]  Loss: 0.0642 Acc:99.41%
Training: Epoch[023/050]  Loss: 0.0651 Acc:99.41%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[024/050]  Loss: 0.0717 Acc:99.51%
Training: Epoch[025/050]  Loss: 0.1065 Acc:99.31%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[026/050]  Loss: 0.1512 Acc:98.81%
Training: Epoch[027/050]  Loss: 0.0769 Acc:99.41%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[028/050]  Loss: 0.0614 Acc:99.51%
Training: Epoch[029/050]  Loss: 0.0741 Acc:99.41%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[030/050]  Loss: 0.0604 Acc:99.51%
Training: Epoch[031/050]  Loss: 0.0763 Acc:99.41%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[032/050]  Loss: 0.0595 Acc:99.51%
Training: Epoch[033/050]  Loss: 0.0786 Acc:99.41%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[034/050]  Loss: 0.0582 Acc:99.51%
Training: Epoch[035/050]  Loss: 0.0769 Acc:99.41%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[036/050]  Loss: 0.0572 Acc:99.51%
Training: Epoch[037/050]  Loss: 0.0755 Acc:99.41%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[038/050]  Loss: 0.0563 Acc:99.51%
Training: Epoch[039/050]  Loss: 0.0735 Acc:99.51%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[040/050]  Loss: 0.0555 Acc:99.51%
Training: Epoch[041/050]  Loss: 0.0726 Acc:99.51%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[042/050]  Loss: 0.0547 Acc:99.51%
Training: Epoch[043/050]  Loss: 0.0703 Acc:99.51%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[044/050]  Loss: 0.0540 Acc:99.51%
Training: Epoch[045/050]  Loss: 0.0696 Acc:99.51%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[046/050]  Loss: 0.0532 Acc:99.51%
Training: Epoch[047/050]  Loss: 0.0674 Acc:99.51%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[048/050]  Loss: 0.0526 Acc:99.51%
Training: Epoch[049/050]  Loss: 0.0674 Acc:99.51%
[[   0.   30.]
 [   0. 2471.]]
0.9880047980807677
Training: Epoch[050/050]  Loss: 0.0519 Acc:99.51%
training 1D-CNN classifier for class8
Training: Epoch[001/050]  Loss: 3.6084 Acc:60.30%
[[   0.   52.]
 [   0. 2622.]]
0.9805534779356769
Training: Epoch[002/050]  Loss: 2.8396 Acc:84.27%
Training: Epoch[003/050]  Loss: 2.5779 Acc:84.27%
[[   0.   52.]
 [   0. 2622.]]
0.9805534779356769
Training: Epoch[004/050]  Loss: 2.2987 Acc:84.27%
Training: Epoch[005/050]  Loss: 2.6019 Acc:84.27%
[[   0.   52.]
 [   0. 2622.]]
0.9805534779356769
Training: Epoch[006/050]  Loss: 2.7096 Acc:84.27%
Training: Epoch[007/050]  Loss: 2.0149 Acc:84.27%
[[   0.   52.]
 [   0. 2622.]]
0.9805534779356769
Training: Epoch[008/050]  Loss: 1.4997 Acc:84.27%
Training: Epoch[009/050]  Loss: 1.1114 Acc:84.27%
[[   0.   52.]
 [   0. 2622.]]
0.9805534779356769
Training: Epoch[010/050]  Loss: 0.7500 Acc:94.76%
Training: Epoch[011/050]  Loss: 0.4813 Acc:97.94%
[[   0.   52.]
 [   0. 2622.]]
0.9805534779356769
Training: Epoch[012/050]  Loss: 0.3015 Acc:98.60%
Training: Epoch[013/050]  Loss: 0.2043 Acc:99.06%
[[   0.   52.]
 [   0. 2622.]]
0.9805534779356769
Training: Epoch[014/050]  Loss: 0.1525 Acc:99.34%
Training: Epoch[015/050]  Loss: 0.1223 Acc:99.34%
[[   0.   52.]
 [   0. 2622.]]
0.9805534779356769
Training: Epoch[016/050]  Loss: 0.1032 Acc:99.34%
Training: Epoch[017/050]  Loss: 0.0903 Acc:99.53%
[[   0.   52.]
 [   0. 2622.]]
0.9805534779356769
Training: Epoch[018/050]  Loss: 0.0815 Acc:99.53%
Training: Epoch[019/050]  Loss: 0.0755 Acc:99.63%
[[   0.   52.]
 [   0. 2622.]]
0.9805534779356769
Training: Epoch[020/050]  Loss: 0.0718 Acc:99.63%
Training: Epoch[021/050]  Loss: 0.0707 Acc:99.63%
[[   0.   52.]
 [   0. 2622.]]
0.9805534779356769
Training: Epoch[022/050]  Loss: 0.0732 Acc:99.63%
Training: Epoch[023/050]  Loss: 0.0797 Acc:99.63%
[[   0.   52.]
 [   0. 2622.]]
0.9805534779356769
Training: Epoch[024/050]  Loss: 0.0885 Acc:99.53%
Training: Epoch[025/050]  Loss: 0.0907 Acc:99.44%
[[   0.   52.]
 [   0. 2622.]]
0.9805534779356769
Training: Epoch[026/050]  Loss: 0.0759 Acc:99.63%
Training: Epoch[027/050]  Loss: 0.0492 Acc:99.72%
[[2.000e+00 5.000e+01]
 [0.000e+00 2.622e+03]]
0.981301421091997
Training: Epoch[028/050]  Loss: 0.0373 Acc:99.72%
Training: Epoch[029/050]  Loss: 0.0400 Acc:99.72%
[[2.000e+00 5.000e+01]
 [0.000e+00 2.622e+03]]
0.981301421091997
Training: Epoch[030/050]  Loss: 0.0359 Acc:99.72%
Training: Epoch[031/050]  Loss: 0.0385 Acc:99.72%
[[2.000e+00 5.000e+01]
 [0.000e+00 2.622e+03]]
0.981301421091997
Training: Epoch[032/050]  Loss: 0.0362 Acc:99.72%
Training: Epoch[033/050]  Loss: 0.0364 Acc:99.72%
[[2.000e+00 5.000e+01]
 [0.000e+00 2.622e+03]]
0.981301421091997
Training: Epoch[034/050]  Loss: 0.0399 Acc:99.81%
Training: Epoch[035/050]  Loss: 0.0673 Acc:99.63%
[[   0.   52.]
 [   0. 2622.]]
0.9805534779356769
Training: Epoch[036/050]  Loss: 0.1212 Acc:99.34%
Training: Epoch[037/050]  Loss: 0.1034 Acc:99.34%
[[2.000e+00 5.000e+01]
 [0.000e+00 2.622e+03]]
0.981301421091997
Training: Epoch[038/050]  Loss: 0.0366 Acc:99.81%
Training: Epoch[039/050]  Loss: 0.0327 Acc:99.63%
[[1.000e+00 5.100e+01]
 [0.000e+00 2.622e+03]]
0.9809274495138369
Training: Epoch[040/050]  Loss: 0.0381 Acc:99.81%
Training: Epoch[041/050]  Loss: 0.0320 Acc:99.53%
[[1.000e+00 5.100e+01]
 [0.000e+00 2.622e+03]]
0.9809274495138369
Training: Epoch[042/050]  Loss: 0.0414 Acc:99.72%
Training: Epoch[043/050]  Loss: 0.0305 Acc:99.63%
[[1.000e+00 5.100e+01]
 [0.000e+00 2.622e+03]]
0.9809274495138369
Training: Epoch[044/050]  Loss: 0.0407 Acc:99.81%
Training: Epoch[045/050]  Loss: 0.0292 Acc:99.72%
[[2.000e+00 5.000e+01]
 [1.000e+00 2.621e+03]]
0.9809274495138369
Training: Epoch[046/050]  Loss: 0.0346 Acc:99.81%
Training: Epoch[047/050]  Loss: 0.0285 Acc:99.81%
[[2.000e+00 5.000e+01]
 [1.000e+00 2.621e+03]]
0.9809274495138369
Training: Epoch[048/050]  Loss: 0.0328 Acc:99.81%
Training: Epoch[049/050]  Loss: 0.0284 Acc:99.81%
[[2.000e+00 5.000e+01]
 [1.000e+00 2.621e+03]]
0.9809274495138369
Training: Epoch[050/050]  Loss: 0.0283 Acc:99.81%
Training: Epoch[001/050]  Loss: 2.8946 Acc:60.30%
[[   0.   52.]
 [   0. 2622.]]
0.9805534779356769
Training: Epoch[002/050]  Loss: 3.1864 Acc:84.27%
Training: Epoch[003/050]  Loss: 2.7927 Acc:84.27%
[[   0.   52.]
 [   0. 2622.]]
0.9805534779356769
Training: Epoch[004/050]  Loss: 2.5409 Acc:84.27%
Training: Epoch[005/050]  Loss: 2.3480 Acc:84.27%
[[   0.   52.]
 [   0. 2622.]]
0.9805534779356769
Training: Epoch[006/050]  Loss: 2.1572 Acc:84.27%
Training: Epoch[007/050]  Loss: 1.9873 Acc:84.27%
[[   0.   52.]
 [   0. 2622.]]
0.9805534779356769
Training: Epoch[008/050]  Loss: 1.8458 Acc:84.27%
Training: Epoch[009/050]  Loss: 1.5479 Acc:84.27%
[[   0.   52.]
 [   0. 2622.]]
0.9805534779356769
Training: Epoch[010/050]  Loss: 1.3085 Acc:84.27%
Training: Epoch[011/050]  Loss: 0.9937 Acc:84.27%
[[   0.   52.]
 [   0. 2622.]]
0.9805534779356769
Training: Epoch[012/050]  Loss: 0.7065 Acc:94.66%
Training: Epoch[013/050]  Loss: 0.4789 Acc:97.94%
[[   0.   52.]
 [   0. 2622.]]
0.9805534779356769
Training: Epoch[014/050]  Loss: 0.2845 Acc:98.60%
Training: Epoch[015/050]  Loss: 0.1885 Acc:99.06%
[[   0.   52.]
 [   0. 2622.]]
0.9805534779356769
Training: Epoch[016/050]  Loss: 0.1371 Acc:99.25%
Training: Epoch[017/050]  Loss: 0.1075 Acc:99.44%
[[   0.   52.]
 [   0. 2622.]]
0.9805534779356769
Training: Epoch[018/050]  Loss: 0.0895 Acc:99.53%
Training: Epoch[019/050]  Loss: 0.0775 Acc:99.63%
[[1.000e+00 5.100e+01]
 [0.000e+00 2.622e+03]]
0.9809274495138369
Training: Epoch[020/050]  Loss: 0.0693 Acc:99.63%
Training: Epoch[021/050]  Loss: 0.0637 Acc:99.63%
[[1.000e+00 5.100e+01]
 [0.000e+00 2.622e+03]]
0.9809274495138369
Training: Epoch[022/050]  Loss: 0.0598 Acc:99.63%
Training: Epoch[023/050]  Loss: 0.0576 Acc:99.63%
[[1.000e+00 5.100e+01]
 [0.000e+00 2.622e+03]]
0.9809274495138369
Training: Epoch[024/050]  Loss: 0.0565 Acc:99.63%
Training: Epoch[025/050]  Loss: 0.0560 Acc:99.63%
[[1.000e+00 5.100e+01]
 [0.000e+00 2.622e+03]]
0.9809274495138369
Training: Epoch[026/050]  Loss: 0.0564 Acc:99.63%
Training: Epoch[027/050]  Loss: 0.0563 Acc:99.63%
[[1.000e+00 5.100e+01]
 [0.000e+00 2.622e+03]]
0.9809274495138369
Training: Epoch[028/050]  Loss: 0.0550 Acc:99.63%
Training: Epoch[029/050]  Loss: 0.0519 Acc:99.63%
[[1.000e+00 5.100e+01]
 [0.000e+00 2.622e+03]]
0.9809274495138369
Training: Epoch[030/050]  Loss: 0.0462 Acc:99.72%
Training: Epoch[031/050]  Loss: 0.0393 Acc:99.81%
[[2.000e+00 5.000e+01]
 [1.000e+00 2.621e+03]]
0.9809274495138369
Training: Epoch[032/050]  Loss: 0.0339 Acc:99.81%
Training: Epoch[033/050]  Loss: 0.0324 Acc:99.81%
[[3.000e+00 4.900e+01]
 [1.000e+00 2.621e+03]]
0.981301421091997
Training: Epoch[034/050]  Loss: 0.0328 Acc:99.81%
Training: Epoch[035/050]  Loss: 0.0328 Acc:99.81%
[[2.000e+00 5.000e+01]
 [1.000e+00 2.621e+03]]
0.9809274495138369
Training: Epoch[036/050]  Loss: 0.0336 Acc:99.81%
Training: Epoch[037/050]  Loss: 0.0366 Acc:99.81%
[[2.000e+00 5.000e+01]
 [0.000e+00 2.622e+03]]
0.981301421091997
Training: Epoch[038/050]  Loss: 0.0483 Acc:99.81%
Training: Epoch[039/050]  Loss: 0.0727 Acc:99.53%
[[1.000e+00 5.100e+01]
 [0.000e+00 2.622e+03]]
0.9809274495138369
Training: Epoch[040/050]  Loss: 0.0841 Acc:99.53%
Training: Epoch[041/050]  Loss: 0.0573 Acc:99.63%
[[3.00e+00 4.90e+01]
 [2.00e+00 2.62e+03]]
0.9809274495138369
Training: Epoch[042/050]  Loss: 0.0300 Acc:99.81%
Training: Epoch[043/050]  Loss: 0.0297 Acc:99.81%
[[   3.   49.]
 [   3. 2619.]]
0.9805534779356769
Training: Epoch[044/050]  Loss: 0.0293 Acc:99.81%
Training: Epoch[045/050]  Loss: 0.0292 Acc:99.81%
[[   3.   49.]
 [   3. 2619.]]
0.9805534779356769
Training: Epoch[046/050]  Loss: 0.0288 Acc:99.81%
Training: Epoch[047/050]  Loss: 0.0290 Acc:99.81%
[[   3.   49.]
 [   4. 2618.]]
0.9801795063575168
Training: Epoch[048/050]  Loss: 0.0282 Acc:99.72%
Training: Epoch[049/050]  Loss: 0.0294 Acc:99.81%
[[   3.   49.]
 [   4. 2618.]]
0.9801795063575168
Training: Epoch[050/050]  Loss: 0.0277 Acc:99.72%
training 1D-CNN classifier for class9
Training: Epoch[001/050]  Loss: 3.0364 Acc:57.27%
[[   0.   31.]
 [   0. 2404.]]
0.9872689938398357
Training: Epoch[002/050]  Loss: 2.2405 Acc:83.50%
Training: Epoch[003/050]  Loss: 2.1972 Acc:83.50%
[[   0.   31.]
 [   0. 2404.]]
0.9872689938398357
Training: Epoch[004/050]  Loss: 2.0812 Acc:83.50%
Training: Epoch[005/050]  Loss: 1.8316 Acc:83.50%
[[   0.   31.]
 [   0. 2404.]]
0.9872689938398357
Training: Epoch[006/050]  Loss: 1.7633 Acc:83.50%
Training: Epoch[007/050]  Loss: 1.3350 Acc:83.50%
[[   0.   31.]
 [   0. 2404.]]
0.9872689938398357
Training: Epoch[008/050]  Loss: 0.9481 Acc:87.19%
Training: Epoch[009/050]  Loss: 0.5717 Acc:97.03%
[[   0.   31.]
 [   0. 2404.]]
0.9872689938398357
Training: Epoch[010/050]  Loss: 0.2730 Acc:99.08%
Training: Epoch[011/050]  Loss: 0.1427 Acc:99.69%
[[   0.   31.]
 [   0. 2404.]]
0.9872689938398357
Training: Epoch[012/050]  Loss: 0.0917 Acc:99.90%
Training: Epoch[013/050]  Loss: 0.0660 Acc:99.90%
[[   0.   31.]
 [   0. 2404.]]
0.9872689938398357
Training: Epoch[014/050]  Loss: 0.0507 Acc:99.90%
Training: Epoch[015/050]  Loss: 0.0406 Acc:100.00%
[[1.000e+00 3.000e+01]
 [1.000e+00 2.403e+03]]
0.9872689938398357
Training: Epoch[016/050]  Loss: 0.0334 Acc:100.00%
Training: Epoch[017/050]  Loss: 0.0280 Acc:100.00%
[[1.000e+00 3.000e+01]
 [1.000e+00 2.403e+03]]
0.9872689938398357
Training: Epoch[018/050]  Loss: 0.0238 Acc:100.00%
Training: Epoch[019/050]  Loss: 0.0205 Acc:100.00%
[[1.000e+00 3.000e+01]
 [1.000e+00 2.403e+03]]
0.9872689938398357
Training: Epoch[020/050]  Loss: 0.0177 Acc:100.00%
Training: Epoch[021/050]  Loss: 0.0154 Acc:100.00%
[[1.000e+00 3.000e+01]
 [1.000e+00 2.403e+03]]
0.9872689938398357
Training: Epoch[022/050]  Loss: 0.0134 Acc:100.00%
Training: Epoch[023/050]  Loss: 0.0118 Acc:100.00%
[[1.000e+00 3.000e+01]
 [2.000e+00 2.402e+03]]
0.9868583162217659
Training: Epoch[024/050]  Loss: 0.0103 Acc:100.00%
Training: Epoch[025/050]  Loss: 0.0091 Acc:100.00%
[[1.000e+00 3.000e+01]
 [2.000e+00 2.402e+03]]
0.9868583162217659
Training: Epoch[026/050]  Loss: 0.0080 Acc:100.00%
Training: Epoch[027/050]  Loss: 0.0071 Acc:100.00%
[[1.000e+00 3.000e+01]
 [2.000e+00 2.402e+03]]
0.9868583162217659
Training: Epoch[028/050]  Loss: 0.0063 Acc:100.00%
Training: Epoch[029/050]  Loss: 0.0056 Acc:100.00%
[[1.000e+00 3.000e+01]
 [2.000e+00 2.402e+03]]
0.9868583162217659
Training: Epoch[030/050]  Loss: 0.0051 Acc:100.00%
Training: Epoch[031/050]  Loss: 0.0045 Acc:100.00%
[[1.000e+00 3.000e+01]
 [2.000e+00 2.402e+03]]
0.9868583162217659
Training: Epoch[032/050]  Loss: 0.0041 Acc:100.00%
Training: Epoch[033/050]  Loss: 0.0037 Acc:100.00%
[[1.000e+00 3.000e+01]
 [2.000e+00 2.402e+03]]
0.9868583162217659
Training: Epoch[034/050]  Loss: 0.0034 Acc:100.00%
Training: Epoch[035/050]  Loss: 0.0030 Acc:100.00%
[[1.000e+00 3.000e+01]
 [2.000e+00 2.402e+03]]
0.9868583162217659
Training: Epoch[036/050]  Loss: 0.0031 Acc:100.00%
Training: Epoch[037/050]  Loss: 0.0026 Acc:100.00%
[[   0.   31.]
 [   0. 2404.]]
0.9872689938398357
Training: Epoch[038/050]  Loss: 0.0053 Acc:100.00%
Training: Epoch[039/050]  Loss: 0.0033 Acc:100.00%
[[   0.   31.]
 [   0. 2404.]]
0.9872689938398357
Training: Epoch[040/050]  Loss: 0.0080 Acc:100.00%
Training: Epoch[041/050]  Loss: 0.0038 Acc:100.00%
[[   0.   31.]
 [   0. 2404.]]
0.9872689938398357
Training: Epoch[042/050]  Loss: 0.0056 Acc:100.00%
Training: Epoch[043/050]  Loss: 0.0033 Acc:100.00%
[[   0.   31.]
 [   0. 2404.]]
0.9872689938398357
Training: Epoch[044/050]  Loss: 0.0033 Acc:100.00%
Training: Epoch[045/050]  Loss: 0.0019 Acc:100.00%
[[1.000e+00 3.000e+01]
 [2.000e+00 2.402e+03]]
0.9868583162217659
Training: Epoch[046/050]  Loss: 0.0017 Acc:100.00%
Training: Epoch[047/050]  Loss: 0.0013 Acc:100.00%
[[1.000e+00 3.000e+01]
 [2.000e+00 2.402e+03]]
0.9868583162217659
Training: Epoch[048/050]  Loss: 0.0013 Acc:100.00%
Training: Epoch[049/050]  Loss: 0.0012 Acc:100.00%
[[1.000e+00 3.000e+01]
 [3.000e+00 2.401e+03]]
0.9864476386036961
Training: Epoch[050/050]  Loss: 0.0012 Acc:100.00%
Training: Epoch[001/050]  Loss: 3.9159 Acc:64.04%
[[   0.   31.]
 [   0. 2404.]]
0.9872689938398357
Training: Epoch[002/050]  Loss: 2.6511 Acc:83.50%
Training: Epoch[003/050]  Loss: 2.4186 Acc:83.50%
[[   0.   31.]
 [   0. 2404.]]
0.9872689938398357
Training: Epoch[004/050]  Loss: 2.1209 Acc:83.50%
Training: Epoch[005/050]  Loss: 2.0034 Acc:83.50%
[[   0.   31.]
 [   0. 2404.]]
0.9872689938398357
Training: Epoch[006/050]  Loss: 1.8244 Acc:83.50%
Training: Epoch[007/050]  Loss: 1.6989 Acc:83.50%
[[   0.   31.]
 [   0. 2404.]]
0.9872689938398357
Training: Epoch[008/050]  Loss: 1.3877 Acc:83.50%
Training: Epoch[009/050]  Loss: 1.0647 Acc:83.40%
[[   0.   31.]
 [   0. 2404.]]
0.9872689938398357
Training: Epoch[010/050]  Loss: 0.6720 Acc:94.57%
Training: Epoch[011/050]  Loss: 0.2669 Acc:98.77%
[[   0.   31.]
 [   0. 2404.]]
0.9872689938398357
Training: Epoch[012/050]  Loss: 0.1241 Acc:99.80%
Training: Epoch[013/050]  Loss: 0.0786 Acc:99.90%
[[   0.   31.]
 [   0. 2404.]]
0.9872689938398357
Training: Epoch[014/050]  Loss: 0.0564 Acc:99.90%
Training: Epoch[015/050]  Loss: 0.0436 Acc:99.90%
[[1.000e+00 3.000e+01]
 [0.000e+00 2.404e+03]]
0.9876796714579056
Training: Epoch[016/050]  Loss: 0.0350 Acc:99.90%
Training: Epoch[017/050]  Loss: 0.0289 Acc:99.90%
[[1.000e+00 3.000e+01]
 [1.000e+00 2.403e+03]]
0.9872689938398357
Training: Epoch[018/050]  Loss: 0.0243 Acc:100.00%
Training: Epoch[019/050]  Loss: 0.0206 Acc:100.00%
[[1.000e+00 3.000e+01]
 [1.000e+00 2.403e+03]]
0.9872689938398357
Training: Epoch[020/050]  Loss: 0.0177 Acc:100.00%
Training: Epoch[021/050]  Loss: 0.0152 Acc:100.00%
[[1.000e+00 3.000e+01]
 [2.000e+00 2.402e+03]]
0.9868583162217659
Training: Epoch[022/050]  Loss: 0.0132 Acc:100.00%
Training: Epoch[023/050]  Loss: 0.0114 Acc:100.00%
[[1.000e+00 3.000e+01]
 [2.000e+00 2.402e+03]]
0.9868583162217659
Training: Epoch[024/050]  Loss: 0.0099 Acc:100.00%
Training: Epoch[025/050]  Loss: 0.0087 Acc:100.00%
[[1.000e+00 3.000e+01]
 [3.000e+00 2.401e+03]]
0.9864476386036961
Training: Epoch[026/050]  Loss: 0.0076 Acc:100.00%
Training: Epoch[027/050]  Loss: 0.0066 Acc:100.00%
[[1.000e+00 3.000e+01]
 [3.000e+00 2.401e+03]]
0.9864476386036961
Training: Epoch[028/050]  Loss: 0.0058 Acc:100.00%
Training: Epoch[029/050]  Loss: 0.0051 Acc:100.00%
[[1.000e+00 3.000e+01]
 [3.000e+00 2.401e+03]]
0.9864476386036961
Training: Epoch[030/050]  Loss: 0.0045 Acc:100.00%
Training: Epoch[031/050]  Loss: 0.0041 Acc:100.00%
[[1.000e+00 3.000e+01]
 [3.000e+00 2.401e+03]]
0.9864476386036961
Training: Epoch[032/050]  Loss: 0.0036 Acc:100.00%
Training: Epoch[033/050]  Loss: 0.0033 Acc:100.00%
[[1.000e+00 3.000e+01]
 [3.000e+00 2.401e+03]]
0.9864476386036961
Training: Epoch[034/050]  Loss: 0.0029 Acc:100.00%
Training: Epoch[035/050]  Loss: 0.0028 Acc:100.00%
[[1.000e+00 3.000e+01]
 [5.000e+00 2.399e+03]]
0.9856262833675564
Training: Epoch[036/050]  Loss: 0.0024 Acc:100.00%
Training: Epoch[037/050]  Loss: 0.0030 Acc:100.00%
[[   3.   28.]
 [   7. 2397.]]
0.9856262833675564
Training: Epoch[038/050]  Loss: 0.0024 Acc:100.00%
Training: Epoch[039/050]  Loss: 0.0077 Acc:100.00%
[[1.000e+00 3.000e+01]
 [3.000e+00 2.401e+03]]
0.9864476386036961
Training: Epoch[040/050]  Loss: 0.0018 Acc:100.00%
Training: Epoch[041/050]  Loss: 0.0016 Acc:100.00%
[[1.000e+00 3.000e+01]
 [2.000e+00 2.402e+03]]
0.9868583162217659
Training: Epoch[042/050]  Loss: 0.0019 Acc:100.00%
Training: Epoch[043/050]  Loss: 0.0017 Acc:100.00%
[[0.000e+00 3.100e+01]
 [1.000e+00 2.403e+03]]
0.9868583162217659
Training: Epoch[044/050]  Loss: 0.0023 Acc:100.00%
Training: Epoch[045/050]  Loss: 0.0020 Acc:100.00%
[[   0.   31.]
 [   0. 2404.]]
0.9872689938398357
Training: Epoch[046/050]  Loss: 0.0033 Acc:100.00%
Training: Epoch[047/050]  Loss: 0.0023 Acc:100.00%
[[   0.   31.]
 [   0. 2404.]]
0.9872689938398357
Training: Epoch[048/050]  Loss: 0.0038 Acc:100.00%
Training: Epoch[049/050]  Loss: 0.0024 Acc:100.00%
[[   0.   31.]
 [   0. 2404.]]
0.9872689938398357
Training: Epoch[050/050]  Loss: 0.0030 Acc:100.00%
